apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "app.fullname" . }}
  labels:
    {{- include "app.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "app.selectorLabels" . | nindent 6 }}
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "app.labels" . | nindent 8 }}
        {{- include "app.apoloPodLabels" . | nindent 8 }}
    spec:
      {{- with .Values.image.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if .Values.modelDownload.initEnabled }}
      initContainers:
        - name: download-model
          image: "{{ .Values.modelDownload.image.repository }}:{{ .Values.modelDownload.image.tag }}"
          volumeMounts:
            {{- if .Values.cache.enabled }}
            - name: huggingface-cache
              mountPath: /root/.cache/huggingface
            {{- end }}
            {{- range .Values.volumes }}
            - name: {{ .pvcName }}
              mountPath: {{ .mountPath }}
              readOnly: {{ .mountReadOnly }}
              subPath: {{ .volumeSubPath }}
            {{- end }}
          env:
          {{- range $k, $v := .Values.env }}
            - name: {{ $k }}
              value: {{ $v | quote }}
          {{- end }}
          command: ["huggingface-cli"]
          args:
            - download
            - --resume-download
            - --repo-type=model
            {{- if or .Values.llm.modelRevision .Values.model.modelRevision }}
            - --revision={{ or .Values.llm.modelRevision .Values.model.modelRevision }}
            {{- end }}
            - "{{ or .Values.llm.modelHFName .Values.model.modelHFName }}"
      {{- end }}
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          livenessProbe:
            initialDelaySeconds: {{ .Values.healthChecksDelay }}
            httpGet:
              path: /health
              port: http
          readinessProbe:
            initialDelaySeconds: {{ .Values.healthChecksDelay }}
            httpGet:
              path: /health
              port: http
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
            {{- if .Values.cache.enabled }}
            - name: huggingface-cache
              mountPath: /root/.cache/huggingface
            {{- end }}
            {{- range .Values.PVCVolumes }}
            - name: {{ .pvcName }}
              mountPath: {{ .mountPath }}
              readOnly: {{ .mountReadOnly }}
              subPath: {{ .volumeSubPath }}
            {{- end }}
          env:
          {{- range $k, $v := .Values.env }}
            - name: {{ $k }}
              value: {{ $v | quote }}
          {{- end }}
            - name: HIP_VISIBLE_DEVICES
              value: "0"
          command:
            - /bin/bash
            - -c
            - |
              set -e
              echo "Running ROCm SMI to check GPU status..."
              rocminfo && rocm-smi
              
              echo "Running PyTorch GPU test..."
              python3 -c '
              import torch
              print("CUDA available:", torch.cuda.is_available())
              if torch.cuda.is_available():
                  x = torch.tensor([1.0, 2.0, 3.0]).cuda()
                  y = torch.tensor([4.0, 5.0, 6.0]).cuda()
                  print("x + y:", x + y)
              '
          
              echo "Starting vLLM server..."
              python3 -m vllm.entrypoints.openai.api_server --host=0.0.0.0 --port=8000 --model=lmsys/vicuna-7b-v1.3 --tokenizer= --dtype=half
          args:
            - --host=0.0.0.0
            - --port={{ .Values.service.port }}
            - --model={{ or .Values.llm.modelHFName .Values.model.modelHFName }}
            {{- if or .Values.llm.modelRevision .Values.model.modelRevision }}
            - --code-revision={{ or .Values.llm.modelRevision .Values.model.modelRevision }}
            {{- end }}
            - --tokenizer={{ .Values.llm.tokenizerHFName }}
            {{- if or .Values.llm.tokenizerRevision .Values.model.tokenizerRevision }}
            - --tokenizer-revision={{ or .Values.llm.tokenizerRevision .Values.model.tokenizerRevision }}
            - --kv-cache-dtype=fp8
            {{- end }}
          {{- with .Values.serverExtraArgs -}}
          {{- toYaml . | nindent 12 }}
          {{- end }}
      {{- if or .Values.volumes .Values.cache.enabled }}
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
      {{- if .Values.cache.enabled }}
        - name: huggingface-cache
          emptyDir:
            sizeLimit: {{ .Values.cache.sizeLimit }}
      {{- end }}
      {{- range .Values.PVCVolumes }}
        - name: {{ .pvcName }}
          persistentVolumeClaim:
            claimName: {{ .pvcName }}
      {{- end }}
      {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        - key: "amd.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
        - key: "platform.neuromation.io/job"
          operator: "Exists"
          effect: "NoSchedule"
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if .Values.priorityClassName }}
      priorityClassName: {{ .Values.priorityClassName }}
      {{- end }}
