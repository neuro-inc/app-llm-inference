nameOverride: ""
fullnameOverride: ""

# Deployment
llm:
  modelHFName: ""
  modelRevision: ""
  tokenizerHFName: ""
  tokenizerRevision: ""

model:
  modelHFName: ""
  modelRevision: ""
  tokenizerHFName: ""
  tokenizerRevision: ""

serverExtraArgs: []

replicaCount: 1

# Old default image
image:
  repository: vllm/vllm-openai
  pullPolicy: IfNotPresent
  tag: v0.10.1.1
  imagePullSecrets: []

resources: {}

probe:
  # One‑off grace period on cold start – pulls weights / builds kernels
  startup:
    periodSeconds: 10 # probe every 10 s
    failureThreshold: 60 # 10 min grace (60×10 s)
    timeoutSeconds: 2

  # Steady‑state readiness gate (service/ingress)
  readiness:
    periodSeconds: 10
    failureThreshold: 3 # ≈30 s to mark NotReady
    timeoutSeconds: 2

  # Steady‑state health check (deadlock / OOM detection)
  liveness:
    periodSeconds: 30
    failureThreshold: 3 # ≈90 s to restart
    timeoutSeconds: 5

cache:
  enabled: true
  sizeLimit: 100Gi

env:
  HUGGING_FACE_HUB_TOKEN: ""

podAnnotations: {}
podExtraLabels: {}

nodeSelector: {}

tolerations:
  - key: platform.neuromation.io/job
    operator: Exists
    effect: NoSchedule

affinity: {}

priorityClassName: ""

# Model download hook
modelDownload:
  # hook is WIP, use initEnabled for now
  hookEnabled: false
  initEnabled: true
  image:
    repository: ghcr.io/neuro-inc/hf-downloader
    tag: v25.7.0
    pullPolicy: IfNotPresent
  retries: 3
  resources: {}

# Service
service:
  port: 8000

# Ingress
ingress:
  enabled: false
  clusterName: ""

preset_name: ""

########################################
# (NEW) GPU provider logic
########################################
gpuProvider: "nvidia"

########################################
# (NEW) AMD image & env
########################################
amdImage:
  repository: rocm/vllm-ci
  tag: "93cd0c2ba1c7ab80df94f79d557abb2774897d2a"
  pullPolicy: IfNotPresent

envAmd:
  HIP_VISIBLE_DEVICES: "0"
  TORCH_USE_HIP_DSA: "1"
  HSA_FORCE_FINE_GRAIN_PCIE: "1"
  HSA_ENABLE_SDMA: "1"
  ROCM_DISABLE_CU_MASK: "0"
  VLLM_WORKER_MULTIPROC_METHOD: "spawn"
  ROCR_VISIBLE_DEVICES: "0"
  NCCL_P2P_DISABLE: "0"
  VLLM_USE_TRITON_FLASH_ATTN: "0"

########################################
# (NEW) NVIDIA image & env
########################################
nvidiaImage:
  repository: vllm/vllm-openai
  tag: "v0.9.1"
  pullPolicy: IfNotPresent

envNvidia:
  NVIDIA_VISIBLE_DEVICES: "all"
  NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
  VLLM_USE_TRITON_FLASH_ATTN: "1"

volumes: []

autoscaling:
  enabled: false
  externalKedaHttpProxyService: keda-add-ons-http-interceptor-proxy.platform.svc.cluster.local
  replicas:
    min: 0
    max: 1
  scaledownPeriod: 300
  requestRate:
    granularity: 1m
    targetValue: 2
    window: 1m
