nameOverride: ""
fullnameOverride: ""

# Deployment
llm:
  modelHFName: ""
  modelRevision: ""
  tokenizerHFName: ""
  tokenizerRevision: ""

model:
  modelHFName: ""
  modelRevision: ""
  tokenizerHFName: ""
  tokenizerRevision: ""

serverExtraArgs: []

replicaCount: 1

image:
  repository: rocm/vllm-ci
  pullPolicy: IfNotPresent
  tag: 16366ee8bbdc30aad9776b74121cfc4d8f8c897d
  imagePullSecrets: []

resources: {}

healthChecksDelay: 60

volumesManage: false
PVCVolumes: []

  # - pvcName: huggingface-cache
  #   autocreate: false
  #   accessMode: ReadWriteOnce
  #   storageClassName: ""
  #   storage: 300Gi
  #   mountPath: /root/.cache/huggingface
  #   mountReadOnly: false
  #   volumeSubPath: "llm-cache"

cache:
  enabled: true
  sizeLimit: 100Gi

env:
  HUGGING_FACE_HUB_TOKEN: ""

  # AMD-specific vars (with example defaults):
  VLLM_USE_TRITON_FLASH_ATTN: "0"
  TORCH_USE_HIP_DSA: "1"
  HSA_FORCE_FINE_GRAIN_PCIE: "1"
  HSA_ENABLE_SDMA: "1"
  ROCM_DISABLE_CU_MASK: "0"
  VLLM_WORKER_MULTIPROC_METHOD: "spawn"
  HIP_VISIBLE_DEVICES: "0,1"
  ROCR_VISIBLE_DEVICES: "0,1"

  # p2p disable (default 0 => p2p is enabled by default)
  NCCL_P2P_DISABLE: "0"

podAnnotations: {}

nodeSelector: {}

tolerations:
  - key: platform.neuromation.io/job
    operator: Exists
    effect: NoSchedule
  - key: amd.com/gpu
    operator: Exists
    effect: NoSchedule

affinity: {}

priorityClassName: ""

# Model download hook
modelDownload:
  # hook is WIP, use initEnabled for now
  hookEnabled: false
  initEnabled: true
  image:
    repository: huggingface/downloader
    tag: 0.17.3
    pullPolicy: IfNotPresent
  resources: {}

# Service
service:
  port: 8000

# Ingress
ingress:
  enabled: false
  clusterName: ""

preset_name: ""
