nameOverride: ""
fullnameOverride: ""

# Deployment
llm:
  modelHFName: ""
  modelRevision: ""
  tokenizerHFName: ""
  tokenizerRevision: ""

model:
  modelHFName: ""
  modelRevision: ""
  tokenizerHFName: ""
  tokenizerRevision: ""

serverExtraArgs: []

replicaCount: 1

# Old default image
image:
  repository: vllm/vllm-openai
  pullPolicy: IfNotPresent
  tag: v0.11.0
  imagePullSecrets: []

resources: {}

probe:
  # One‑off grace period on cold start – pulls weights / builds kernels
  startup:
    periodSeconds: 10 # probe every 10 s
    failureThreshold: 60 # 10 min grace (60×10 s)
    timeoutSeconds: 2

  # Steady‑state readiness gate (service/ingress)
  readiness:
    periodSeconds: 10
    failureThreshold: 36 # ≈360 s to mark NotReady
    timeoutSeconds: 2

  # Steady‑state health check (deadlock / OOM detection)
  liveness:
    periodSeconds: 30
    failureThreshold: 9 # ≈270 s to restart
    timeoutSeconds: 5

cache:
  enabled: true
  sizeLimit: 100Gi

env:
  HUGGING_FACE_HUB_TOKEN: ""

podAnnotations: {}
podExtraLabels: {}

nodeSelector: {}

tolerations:
  - key: platform.neuromation.io/job
    operator: Exists
    effect: NoSchedule

affinity: {}

priorityClassName: ""

# Model download hook
modelDownload:
  # hook is WIP, use initEnabled for now
  hookEnabled: false
  initEnabled: true
  image:
    repository: ghcr.io/neuro-inc/hf-downloader
    tag: v25.7.0
    pullPolicy: IfNotPresent
  retries: 3
  resources: {}

# Service
service:
  port: 8000

# Ingress
ingress:
  enabled: false
  clusterName: ""

preset_name: ""

########################################
# (NEW) GPU provider logic
########################################
gpuProvider: "nvidia"

########################################
# (NEW) AMD image & env
########################################
amdImage:
  repository: rocm/vllm-ci
  tag: "93cd0c2ba1c7ab80df94f79d557abb2774897d2a"
  pullPolicy: IfNotPresent

envAmd:
  TORCH_USE_HIP_DSA: "1"
  HSA_FORCE_FINE_GRAIN_PCIE: "1"

########################################
# (NEW) NVIDIA image & env
########################################
nvidiaImage:
  repository: vllm/vllm-openai
  tag: "v0.11.0"
  pullPolicy: IfNotPresent

envNvidia: {}

volumes: []

autoscaling:
  enabled: false
  externalKedaHttpProxyService: keda-add-ons-http-interceptor-proxy.platform.svc.cluster.local
  replicas:
    min: 0
    max: 1
  scaledownPeriod: 300
  requestRate:
    granularity: 1m
    targetValue: 2
    window: 1m
