kind: batch
title: "vLLM environment variable optimization test (AMD)"

defaults:
  preset: mi210x2
  max_parallel: 3
  fail_fast: false
  workdir: /app
  cache: 
    strategy: none
  volumes:
    - storage:${{ flow.project_id }}/vllm-tests:/app/vllm-tests

params:
  experiment_name:
    default: "vllm_optimization_test"
    descr: "Name of the experiment or run."

volumes:
  vllm_tests:
    remote: storage:${{ flow.project_id }}/vllm-tests
    mount: /app/vllm-tests

images:
  vllm_test_env:
    ref: "rocm/vllm-ci:16366ee8bbdc30aad9776b74121cfc4d8f8c897d"

tasks:
  - strategy:
      matrix:
        model: [
          # "stabilityai/stablelm-tuned-alpha-7b",
          # "internlm/internlm-chat-7b"
          # "Qwen/Qwen-7B-Chat",
          # "bigcode/starcoder",
          # "mosaicml/mpt-7b",
          # "EleutherAI/gpt-j-6b",
          # "EleutherAI/gpt-neox-20b",
          # "gpt2",
          # "meta-llama/Llama-2-7b-hf",
          # "tiiuae/falcon-7b-instruct",
          # "tiiuae/Falcon3-7B-Base",
          # "mistralai/Mistral-7B-v0.1"
          # "lmsys/vicuna-7b-v1.3",
          # "meta-llama/Llama-3.3-70B-Instruct",
          # "Qwen/Qwen2-7B-Instruct",
          # "openbmb/MiniCPM-o-2_6",
          # "microsoft/phi-4",
          # "NovaSky-AI/Sky-T1-32B-Preview",
          "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4",
          "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
         
          "Qwen/QwQ-32B-preview",
          # "AIDC-AI/aMarco-o1",
          # "meta-llama/Llama-3.3-70B-Instruct",
          # "Daemontatox/RA_Reasoner",
         
          # "deepseek-ai/DeepSeek-R1",
          # "deepseek-ai/DeepSeek-V3",
          # "MiniMaxAI/MiniMax-Text-01",
          # "internlm/internlm3-8b-instruct",
          # "kyutai/helium-1-preview-2b",
          # "meta-llama/Llama-3.1-8B-Instruct",
          # "bartowski/Sky-T1-32B-Preview-GGUF"
          # "Qwen/QwQ-32B-Preview",
          # "driaforall/Dria-Agent-a-3B",
          # "meta-llama/Llama-3.2-1B",
          # "Qwen/Qwen2-VL-7B-Instruct",
          # "meta-llama/Llama-3.2-11B-Vision-Instruct",
          # "meta-llama/Llama-3.2-3B-Instruct",
          # "meta-llama/Llama-3.1-8B",
          # "Qwen/QVQ-72B-Preview",
          # "mistralai/Mistral-7B-Instruct-v0.3",
          # "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
          # "Qwen/Qwen2.5-72B-Instruct",
          # "meta-llama/Meta-Llama-3-8B-Instruct",
          # "Qwen/Qwen2.5-7B-Instruct"
        ]
        # model specific
        vllm_use_triton_flash_attn: [ "0" ]  # Use CK flash attention for better SWA support
        
        # amd specific
        torch_use_hip_dsa:          [ "1" ]
        hsa_force_fine_grain_pcie:  [ "1" ]
        hsa_enable_sdma:            [ "1" ]
        rocm_disable_cu_mask:       [ "0" ]
        vllm_worker_multiproc_method: [ "spawn" ]
        hip_visible_devices:        [ "0,1" ]
        rocr_visible_devices:       [ "0,1" ]

    id: "amd_optimization_test_${{ replace(replace(replace(replace(str(matrix.model + '_flash_' + matrix.vllm_use_triton_flash_attn + '_hipDsa_' + matrix.torch_use_hip_dsa + '_finePCIE_' + matrix.hsa_force_fine_grain_pcie + '_sdma_' + matrix.hsa_enable_sdma + '_cuMask_' + matrix.rocm_disable_cu_mask + '_multiproc_' + matrix.vllm_worker_multiproc_method + '_hip_' + matrix.hip_visible_devices + '_rocr_' + matrix.rocr_visible_devices), '/', '_'), '-', '_'), '.', '_'), ',', '_') }}"

    title: >
      Optimization test model=${{ matrix.model }},
      flashAttn=${{ matrix.vllm_use_triton_flash_attn }},
      hipDsa=${{ matrix.torch_use_hip_dsa }},
      finePCIE=${{ matrix.hsa_force_fine_grain_pcie }},
      sdma=${{ matrix.hsa_enable_sdma }},
      cuMask=${{ matrix.rocm_disable_cu_mask }},
      multiproc=${{ matrix.vllm_worker_multiproc_method }},
      hip=${{ matrix.hip_visible_devices }},
      rocr=${{ matrix.rocr_visible_devices }}

    image: ${{ images.vllm_test_env.ref }}

    env:
      VLLM_USE_TRITON_FLASH_ATTN:  ${{ matrix.vllm_use_triton_flash_attn }}
      TORCH_USE_HIP_DSA:           ${{ matrix.torch_use_hip_dsa }}
      HSA_FORCE_FINE_GRAIN_PCIE:   ${{ matrix.hsa_force_fine_grain_pcie }}
      HSA_ENABLE_SDMA:             ${{ matrix.hsa_enable_sdma }}
      ROCM_DISABLE_CU_MASK:        ${{ matrix.rocm_disable_cu_mask }}
      VLLM_WORKER_MULTIPROC_METHOD: ${{ matrix.vllm_worker_multiproc_method }}
      HIP_VISIBLE_DEVICES:        ${{ matrix.hip_visible_devices }}
      ROCR_VISIBLE_DEVICES:       ${{ matrix.rocr_visible_devices }}
      TARGET_MODEL: ${{ matrix.model }}
      EXPERIMENT_NAME: ${{ params.experiment_name }}
      HUGGING_FACE_HUB_TOKEN: secret:HF_TOKEN

    bash: |
      set -euxo pipefail
      
      export VLLM_LOGGING_LEVEL=DEBUG
      export NCCL_DEBUG=TRACE
      export VLLM_TRACE_FUNCTION=1

      rocminfo
      rocm-smi

      apt-get update && apt-get install -y wget
      wget https://raw.githubusercontent.com/vllm-project/vllm/main/collect_env.py
      
      echo "======================Start Collecting Environment========================="
      python collect_env.py
      echo "======================End Collecting Environment========================="

      echo "==============================================="
      echo "Starting AMD optimization test with environment vars:"
      env | grep -E '^VLLM|^HIP|^TORCH|^HSA|^ROCM|^TARGET_MODEL|^EXPERIMENT_NAME|^NCCL|^HIP_VISIBLE_DEVICES|^ROCR_VISIBLE_DEVICES|^VLLM_WORKER_MULTIPROC_METHOD|' 
      echo "==============================================="

      echo "Running vLLM with model=${TARGET_MODEL}"
      vllm serve "${TARGET_MODEL}" \
        --host=0.0.0.0 \
        --port=8000 \
        --max-model-len=2048 \
        --enforce-eager \
        --dtype=half \
        --tensor-parallel-size 2 \
        --trust-remote-code \
        --gpu-memory-utilization 0.9 \
        > vllm_out.log 2>&1 &

      VLLM_PID=$!

      SECS=7200
      for i in $(seq 1 $SECS); do
        if ! kill -0 "$VLLM_PID" 2>/dev/null; then
          echo "vLLM has crashed/exited unexpectedly. Marking as FAIL."
          cat vllm_out.log
          exit 1
        fi

        if grep -Eq "Started server process|Uvicorn running on " vllm_out.log; then
          echo "vLLM started successfully. Marking as SUCCESS."
          cat vllm_out.log
          kill "$VLLM_PID" || true
          exit 0
        fi

        sleep 1
      done

      echo "Timed out waiting for vLLM to start. Marking as FAIL."
      kill "$VLLM_PID" || true
      cat vllm_out.log
      exit 1
