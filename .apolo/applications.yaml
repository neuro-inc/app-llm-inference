- app_type: llm-inference
  name: llm-inference
  title: vLLM
  install_type: workflow
  helm_path: charts/llm-inference-app
  app_package_name: apolo_apps_llm_inference
  inputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/VLLMInferenceInputs.json
    types_name: VLLMInferenceInputs
    processor: VLLMInferenceInputsProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  outputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/VLLMInferenceOutputs.json
    types_name: VLLMInferenceOutputs
    processor: VLLMInferenceOutputsProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  short_description: Deploy scalable applications
  description: |
    vLLM is a fast and easy-to-use library for LLM inference and serving.

    Originally developed in the Sky Computing Lab at UC Berkeley, vLLM has evolved
    into a community-driven project with contributions from both academia and industry.
  pub_date: "2025-01-01T00:00:00+00:00"
  logo: https://storage.googleapis.com/development-421920-assets/app-logos/vllm.svg
  changelog:
    file: ./CHANGELOG.md
  tags:
    - "LLM"
    - "Inference"
    - "MLOps"
    - "Hugging Face"
    - "Embeddings"
    - "vLLM"
    - "RAG"
    - "Embeddings"
  assets:
    - type: image
      url: https://storage.googleapis.com/development-421920-assets/app-logos/vllm-banner.png
    - type: video
      url: https://www.youtube.com/watch?v=Ju2FrqIrdx0
  urls:
    - name: vLLM installation guide
      type: documentation
      url: https://docs.apolo.us/index/apolo-console/apps/installable-apps/available-apps/llm-inference
    - name: Hugging Face Models
      type: external
      url: https://huggingface.co/models
    - name: Apolo CLI
      type: documentation
      url: https://docs.apolo.us/index/apolo-concepts-cli/installing
    - name: GitHub Repository
      url: https://github.com/neuro-inc/app-llm-inference
      type: external
    - name: Getting Started
      url: https://docs.vllm.ai/en/latest/
      type: documentation
- app_type: llama4
  name: llama4-inference
  app_package_name: apolo_apps_llm_inference
  title: LLAMA4
  install_type: workflow
  helm_path: charts/llm-inference-app
  inputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/LLama4Inputs.json
    types_name: LLama4Inputs
    processor: Llama4InferenceValueProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  outputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/VLLMInferenceOutputs.json
    types_name: VLLMInferenceOutputs
    processor: VLLMInferenceOutputsProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  short_description: Open, powerful, and efficient language models from Meta
  description: |
    LLaMA 4 is Meta's latest open-weight large language model series,
    offering improved reasoning, efficiency, and multilingual capabilities.

    Built with a Mixture of Experts (MoE) architecture, LLaMA 4 balances
    performance and scalability, and is available in various configurations
    optimized for both research and production use.
  pub_date: "2025-07-28T00:00:00+00:00"
  logo: https://storage.googleapis.com/development-421920-assets/app-logos/llama4.png
  changelog:
    file: ./CHANGELOG.md
  tags:
    - "LLM"
    - "Inference"
    - "MLOps"
    - "Hugging Face"
    - "Embeddings"
    - "vLLM"
    - "RAG"
    - "Embeddings"
    - "Meta"
    - "LLaMA"
    - "Bundle"
  assets:
    - type: image
      url: https://storage.googleapis.com/development-421920-assets/app-logos/llama4-banner.svg
  urls:
    - name: Llama4 installation guide
      type: documentation
      url: https://docs.apolo.us/index/apolo-console/apps/installable-apps/available-apps/llama4
    - name: Hugging Face Meta Models
      type: external
      url: https://huggingface.co/meta-llama
    - name: Apolo CLI
      type: documentation
      url: https://docs.apolo.us/index/apolo-concepts-cli/installing
    - name: GitHub Repository
      type: external
      url: https://github.com/neuro-inc/app-llm-inference
    - name: Getting Started
      type: documentation
      url: https://docs.vllm.ai/en/latest/
- app_type: deepseek
  name: deepseek-inference
  title: DeepSeek
  app_package_name: apolo_apps_llm_inference
  install_type: workflow
  helm_path: charts/llm-inference-app
  inputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/DeepSeekInputs.json
    types_name: DeepSeekInputs
    processor: DeepSeekInferenceValueProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  outputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/VLLMInferenceOutputs.json
    types_name: VLLMInferenceOutputs
    processor: VLLMInferenceOutputsProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  short_description: Open-source, high-performance language models from DeepSeek-AI
  description: |
    DeepSeek is a family of powerful open-source language models developed by DeepSeek-AI,
    focused on high-performance reasoning, comprehension, and text generation.

    The DeepSeek-LLM series delivers competitive performance across a wide range of
    language understanding and generation tasks, making it suitable for both research
    and production environments.
  pub_date: "2025-07-31T00:00:00+00:00"
  logo: https://storage.googleapis.com/development-421920-assets/app-logos/deepseek-r1.png
  changelog:
    file: ./CHANGELOG.md
  tags:
    - "LLM"
    - "Inference"
    - "MLOps"
    - "Hugging Face"
    - "Embeddings"
    - "vLLM"
    - "RAG"
    - "Embeddings"
    - "DeepSeek"
    - "Bundle"
  assets:
    - type: image
      url: https://storage.googleapis.com/development-421920-assets/app-logos/deepseek-r1-banner.svg
  urls:
    - name: Deepseek installation guide
      type: documentation
      url: https://docs.apolo.us/index/apolo-console/apps/installable-apps/available-apps/deepseekr1
    - name: Hugging Face Meta Models
      type: external
      url: https://huggingface.co/deepseek-ai
    - name: Apolo CLI
      type: documentation
      url: https://docs.apolo.us/index/apolo-concepts-cli/installing
    - name: GitHub Repository
      type: external
      url: https://github.com/neuro-inc/app-llm-inference
    - name: Getting Started
      type: documentation
      url: https://docs.vllm.ai/en/latest/
- app_type: mistral
  name: mistral-inference
  app_package_name: apolo_apps_llm_inference
  title: Mistral
  install_type: workflow
  helm_path: charts/llm-inference-app
  inputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/MistralInputs.json
    types_name: MistralInputs
    processor: MistralInferenceValueProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  outputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/VLLMInferenceOutputs.json
    types_name: VLLMInferenceOutputs
    processor: VLLMInferenceOutputsProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  short_description: Fast, open-weight language models optimized for real-world use
  description: |
    Mistral is a suite of open-weight language models designed for efficient and scalable deployment,
    offering strong performance in reasoning, summarization, and instruction following.

    Developed by Mistral AI, these models include dense and Mixture of Experts (MoE) architectures,
    making them suitable for both research and production across diverse workloads.
  pub_date: "2025-07-31T00:00:00+00:00"
  logo: https://storage.googleapis.com/development-421920-assets/app-logos/mistral.png
  changelog:
    file: ./CHANGELOG.md
  tags:
    - "LLM"
    - "Inference"
    - "MLOps"
    - "Hugging Face"
    - "Embeddings"
    - "vLLM"
    - "RAG"
    - "Embeddings"
    - "Mistral"
    - "Bundle"
  assets:
    - type: image
      url: https://storage.googleapis.com/development-421920-assets/app-logos/mistral-banner.svg
  urls:
    - name: Mistral installation guide
      type: documentation
      url: https://docs.apolo.us/index/apolo-console/apps/installable-apps/available-apps/mistral
    - name: Hugging Face Meta Models
      type: external
      url: https://huggingface.co/deepseek-ai
    - name: Apolo CLI
      type: documentation
      url: https://docs.apolo.us/index/apolo-concepts-cli/installing
    - name: GitHub Repository
      type: external
      url: https://github.com/neuro-inc/app-llm-inference
    - name: Getting Started
      type: documentation
      url: https://docs.vllm.ai/en/latest/
- app_type: gpt-oss
  name: gpt-inference
  title: OpenAI GPT OSS
  install_type: workflow
  app_package_name: apolo_apps_llm_inference
  helm_path: charts/llm-inference-app
  inputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/GptOssInputs.json
    types_name: GptOssInputs
    processor: GPTOSSInferenceValueProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  outputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/VLLMInferenceOutputs.json
    types_name: VLLMInferenceOutputs
    processor: VLLMInferenceOutputsProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  short_description: High-performance, open-source GPT models for versatile NLP tasks developed by OpenAI
  description: |
    GPTOSS is an open-source suite of powerful language models designed for high-performance text generation,
    reasoning, and natural language understanding. Developed by OpenAI and fine-tuned for general-purpose use,
    GPTOSS models are optimized for efficient inference and robust output quality.

    This family of models supports a wide range of NLP applications—from chatbots and assistants
    to code generation and summarization—making GPTOSS a versatile choice for both research and production environments.
  pub_date: "2025-07-31T00:00:00+00:00"
  logo: https://storage.googleapis.com/development-421920-assets/app-logos/gpt-oss.png
  changelog:
    file: ./CHANGELOG.md
  tags:
    - "LLM"
    - "Inference"
    - "MLOps"
    - "Hugging Face"
    - "Embeddings"
    - "vLLM"
    - "RAG"
    - "Embeddings"
    - "OpenAI"
    - "GPT"
    - "GptOSS"
    - "Bundle"
  assets:
    - type: image
      url: https://storage.googleapis.com/development-421920-assets/app-logos/gpt-oss-banner.svg
  urls:
    - name: Gpt-oss installation guide
      type: documentation
      url: https://docs.apolo.us/index/apolo-console/apps/installable-apps/available-apps/gpt-oss
    - name: Hugging Face OpenAI Models
      type: external
      url: https://huggingface.co/openai
    - name: Apolo CLI
      type: documentation
      url: https://docs.apolo.us/index/apolo-concepts-cli/installing
    - name: GitHub Repository
      type: external
      url: https://github.com/neuro-inc/app-llm-inference
    - name: Getting Started
      type: documentation
      url: https://docs.vllm.ai/en/latest/
- app_type: kimi2
  name: kimi2-inference
  title: Kimi K2
  install_type: workflow
  app_package_name: apolo_apps_llm_inference
  helm_path: charts/llm-inference-app
  inputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/Kimi2Inputs.json
    types_name: Kimi2Inputs
    processor: Kimi2InferenceValueProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  outputs:
    schema_path: .apolo/src/apolo_apps_llm_inference/schemas/VLLMInferenceOutputs.json
    types_name: VLLMInferenceOutputs
    processor: VLLMInferenceOutputsProcessor
    image: ghcr.io/neuro-inc/app-llm-inference
  short_description: State-of-the-art mixture-of-experts language model from Moonshot AI
  description: |
    Kimi K2 is a state-of-the-art mixture-of-experts language model from Moonshot AI
    with 1 trillion total parameters and 32 billion activated parameters per inference.

    Featuring 384 specialized experts with dynamic routing, 61 layers, and 128K token context,
    Kimi K2 excels at agentic reasoning, code generation, and general-purpose AI tasks.
  pub_date: "2025-12-26T00:00:00+00:00"
  logo: "https://storage.googleapis.com/development-421920-assets/app-logos/kimi.png"
  changelog:
    file: ./CHANGELOG.md
  tags:
    - "LLM"
    - "Inference"
    - "MLOps"
    - "Hugging Face"
    - "vLLM"
    - "RAG"
    - "Moonshot AI"
    - "Kimi"
    - "MoE"
    - "Bundle"
  assets: 
    - type: image
      url: https://storage.googleapis.com/development-421920-assets/app-logos/kimi-banner.png
  urls:
    - name: Kimi K2 installation guide
      type: documentation
      url: https://docs.apolo.us/index/apolo-console/apps/installable-apps/available-apps/kimi2
    - name: Hugging Face Moonshot AI Models
      type: external
      url: https://huggingface.co/moonshotai
    - name: Apolo CLI
      type: documentation
      url: https://docs.apolo.us/index/apolo-concepts-cli/installing
    - name: GitHub Repository
      type: external
      url: https://github.com/neuro-inc/app-llm-inference
    - name: Getting Started
      type: documentation
      url: https://docs.vllm.ai/en/latest/
