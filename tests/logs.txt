The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
0it [00:00, ?it/s]0it [00:00, ?it/s]
INFO 01-31 12:42:33 api_server.py:219] vLLM API server version 0.5.3.post1
INFO 01-31 12:42:33 api_server.py:220] args: Namespace(host='0.0.0.0', port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision='main', tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='half', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2048, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
WARNING 01-31 12:42:33 config.py:1425] Casting torch.bfloat16 to torch.float16.
INFO 01-31 12:42:33 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 01-31 12:42:35 model_runner.py:680] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
INFO 01-31 12:42:36 weight_utils.py:223] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:02<00:15,  2.15s/it]
Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:04<00:14,  2.44s/it]
Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:07<00:12,  2.53s/it]
Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:10<00:10,  2.59s/it]
Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:12<00:07,  2.62s/it]
Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:15<00:05,  2.66s/it]
Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:16<00:02,  2.27s/it]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:19<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:19<00:00,  2.43s/it]

INFO 01-31 12:42:56 model_runner.py:692] Loading model weights took 61.0607 GB
INFO 01-31 12:42:57 gpu_executor.py:102] # GPU blocks: 2194, # CPU blocks: 1024
INFO 01-31 12:43:00 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-31 12:43:00 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-31 12:43:13 model_runner.py:1181] Graph capturing finished in 13 secs.
WARNING 01-31 12:43:13 serving_embedding.py:170] embedding_mode is False. Embedding API will not work.
INFO 01-31 12:43:13 api_server.py:292] Available routes are:
INFO 01-31 12:43:13 api_server.py:297] Route: /openapi.json, Methods: HEAD, GET
INFO 01-31 12:43:13 api_server.py:297] Route: /docs, Methods: HEAD, GET
INFO 01-31 12:43:13 api_server.py:297] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 01-31 12:43:13 api_server.py:297] Route: /redoc, Methods: HEAD, GET
INFO 01-31 12:43:13 api_server.py:297] Route: /health, Methods: GET
INFO 01-31 12:43:13 api_server.py:297] Route: /tokenize, Methods: POST
INFO 01-31 12:43:13 api_server.py:297] Route: /detokenize, Methods: POST
INFO 01-31 12:43:13 api_server.py:297] Route: /v1/models, Methods: GET
INFO 01-31 12:43:13 api_server.py:297] Route: /version, Methods: GET
INFO 01-31 12:43:13 api_server.py:297] Route: /v1/chat/completions, Methods: POST
INFO 01-31 12:43:13 api_server.py:297] Route: /v1/completions, Methods: POST
INFO 01-31 12:43:13 api_server.py:297] Route: /v1/embeddings, Methods: POST
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO 01-31 12:43:23 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:43:33 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:43:43 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:43:53 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:44:03 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:44:13 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:44:23 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:44:33 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:44:43 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:44:53 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:45:03 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:45:13 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:45:23 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:45:33 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:45:43 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:45:53 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:46:03 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:46:13 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:46:23 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:46:33 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:46:43 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:46:53 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:47:03 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:47:13 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:47:23 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-31 12:47:33 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:37182 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:37184 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /v1/models HTTP/1.1" 200 OK
INFO 01-31 12:47:43 logger.py:36] Received request cmpl-bac97e5c20d84eafafe8ffbb05b240e9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:43 async_llm_engine.py:173] Added request cmpl-bac97e5c20d84eafafe8ffbb05b240e9-0.
INFO 01-31 12:47:43 logger.py:36] Received request cmpl-604bc8d2893b4811a342ae7bac5919d7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:43 async_llm_engine.py:173] Added request cmpl-604bc8d2893b4811a342ae7bac5919d7-0.
INFO 01-31 12:47:43 logger.py:36] Received request cmpl-7113ea462f8f480f9776847eb801da92-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:43 async_llm_engine.py:173] Added request cmpl-7113ea462f8f480f9776847eb801da92-0.
INFO 01-31 12:47:43 logger.py:36] Received request cmpl-156c95fa6ebd4c28abef4565c6a89b4b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:43 async_llm_engine.py:173] Added request cmpl-156c95fa6ebd4c28abef4565c6a89b4b-0.
INFO 01-31 12:47:43 logger.py:36] Received request cmpl-548f4bf876e14671ae77e55b523ea919-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:43 async_llm_engine.py:173] Added request cmpl-548f4bf876e14671ae77e55b523ea919-0.
INFO 01-31 12:47:43 logger.py:36] Received request cmpl-ef7c61f682064eab82f4ca19964d4b6b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:43 async_llm_engine.py:173] Added request cmpl-ef7c61f682064eab82f4ca19964d4b6b-0.
INFO 01-31 12:47:43 logger.py:36] Received request cmpl-dac05a698d4b443d860bec195fc5c567-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:43 async_llm_engine.py:173] Added request cmpl-dac05a698d4b443d860bec195fc5c567-0.
INFO 01-31 12:47:43 logger.py:36] Received request cmpl-8fb50806e34b4fd89675170987ecc6fb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:43 async_llm_engine.py:173] Added request cmpl-8fb50806e34b4fd89675170987ecc6fb-0.
INFO 01-31 12:47:43 logger.py:36] Received request cmpl-8a66afbc9dc449209d5c72145d8024ac-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:43 async_llm_engine.py:173] Added request cmpl-8a66afbc9dc449209d5c72145d8024ac-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:47:43 logger.py:36] Received request cmpl-c11b65d2ae5f486aa51e50c581fd5d95-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:43 async_llm_engine.py:173] Added request cmpl-c11b65d2ae5f486aa51e50c581fd5d95-0.
INFO 01-31 12:47:43 metrics.py:396] Avg prompt throughput: 0.9 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:51628 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:51640 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:47:45 async_llm_engine.py:140] Finished request cmpl-bac97e5c20d84eafafe8ffbb05b240e9-0.
INFO 01-31 12:47:45 async_llm_engine.py:140] Finished request cmpl-604bc8d2893b4811a342ae7bac5919d7-0.
INFO 01-31 12:47:45 async_llm_engine.py:140] Finished request cmpl-7113ea462f8f480f9776847eb801da92-0.
INFO 01-31 12:47:45 async_llm_engine.py:140] Finished request cmpl-156c95fa6ebd4c28abef4565c6a89b4b-0.
INFO 01-31 12:47:45 async_llm_engine.py:140] Finished request cmpl-548f4bf876e14671ae77e55b523ea919-0.
INFO 01-31 12:47:45 async_llm_engine.py:140] Finished request cmpl-ef7c61f682064eab82f4ca19964d4b6b-0.
INFO 01-31 12:47:45 async_llm_engine.py:140] Finished request cmpl-dac05a698d4b443d860bec195fc5c567-0.
INFO 01-31 12:47:45 async_llm_engine.py:140] Finished request cmpl-8fb50806e34b4fd89675170987ecc6fb-0.
INFO 01-31 12:47:45 async_llm_engine.py:140] Finished request cmpl-8a66afbc9dc449209d5c72145d8024ac-0.
INFO 01-31 12:47:45 async_llm_engine.py:140] Finished request cmpl-c11b65d2ae5f486aa51e50c581fd5d95-0.
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:47:46 logger.py:36] Received request cmpl-246f2ebff8814ef7a326d1717ba680c4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:46 async_llm_engine.py:173] Added request cmpl-246f2ebff8814ef7a326d1717ba680c4-0.
INFO 01-31 12:47:46 logger.py:36] Received request cmpl-12fb7ed26bcc4dc49498130ff1959575-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:46 async_llm_engine.py:173] Added request cmpl-12fb7ed26bcc4dc49498130ff1959575-0.
INFO 01-31 12:47:46 logger.py:36] Received request cmpl-56e33cea45f34603804b1220b5aec3f2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:46 async_llm_engine.py:173] Added request cmpl-56e33cea45f34603804b1220b5aec3f2-0.
INFO 01-31 12:47:46 logger.py:36] Received request cmpl-6023678c90be4f36be03e7bd834a094f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:46 logger.py:36] Received request cmpl-e8436c91960143c98ab0deda6b7067bb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:46 logger.py:36] Received request cmpl-f0ce18c9b3b94d2097b090820b3a24dd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:46 async_llm_engine.py:173] Added request cmpl-6023678c90be4f36be03e7bd834a094f-0.
INFO 01-31 12:47:46 async_llm_engine.py:173] Added request cmpl-e8436c91960143c98ab0deda6b7067bb-0.
INFO 01-31 12:47:46 async_llm_engine.py:173] Added request cmpl-f0ce18c9b3b94d2097b090820b3a24dd-0.
INFO 01-31 12:47:46 logger.py:36] Received request cmpl-fedd111823514cf8834edcc773eac3d9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:46 logger.py:36] Received request cmpl-5083988d64e5475089b405c9e88120eb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:46 logger.py:36] Received request cmpl-bd798393b1494493bfed6b7a3d1af32d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:46 async_llm_engine.py:173] Added request cmpl-fedd111823514cf8834edcc773eac3d9-0.
INFO 01-31 12:47:46 async_llm_engine.py:173] Added request cmpl-5083988d64e5475089b405c9e88120eb-0.
INFO 01-31 12:47:46 async_llm_engine.py:173] Added request cmpl-bd798393b1494493bfed6b7a3d1af32d-0.
INFO 01-31 12:47:46 logger.py:36] Received request cmpl-cb0b92409e2a484b9c804f844309548a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:46 async_llm_engine.py:173] Added request cmpl-cb0b92409e2a484b9c804f844309548a-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:47:48 metrics.py:396] Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 218.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:47:48 async_llm_engine.py:140] Finished request cmpl-246f2ebff8814ef7a326d1717ba680c4-0.
INFO 01-31 12:47:48 async_llm_engine.py:140] Finished request cmpl-12fb7ed26bcc4dc49498130ff1959575-0.
INFO 01-31 12:47:48 async_llm_engine.py:140] Finished request cmpl-56e33cea45f34603804b1220b5aec3f2-0.
INFO 01-31 12:47:48 async_llm_engine.py:140] Finished request cmpl-6023678c90be4f36be03e7bd834a094f-0.
INFO 01-31 12:47:48 async_llm_engine.py:140] Finished request cmpl-e8436c91960143c98ab0deda6b7067bb-0.
INFO 01-31 12:47:48 async_llm_engine.py:140] Finished request cmpl-f0ce18c9b3b94d2097b090820b3a24dd-0.
INFO 01-31 12:47:48 async_llm_engine.py:140] Finished request cmpl-fedd111823514cf8834edcc773eac3d9-0.
INFO 01-31 12:47:48 async_llm_engine.py:140] Finished request cmpl-5083988d64e5475089b405c9e88120eb-0.
INFO 01-31 12:47:48 async_llm_engine.py:140] Finished request cmpl-bd798393b1494493bfed6b7a3d1af32d-0.
INFO 01-31 12:47:48 async_llm_engine.py:140] Finished request cmpl-cb0b92409e2a484b9c804f844309548a-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:47:49 logger.py:36] Received request cmpl-b7e0bf208af742c7950f1738c1e976ef-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:49 async_llm_engine.py:173] Added request cmpl-b7e0bf208af742c7950f1738c1e976ef-0.
INFO 01-31 12:47:49 logger.py:36] Received request cmpl-0a55f971f3bb446ab5d9e0232c7fdd0c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:49 logger.py:36] Received request cmpl-8b87ff5d67b24e068cce0e291b4df674-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:49 async_llm_engine.py:173] Added request cmpl-0a55f971f3bb446ab5d9e0232c7fdd0c-0.
INFO 01-31 12:47:49 async_llm_engine.py:173] Added request cmpl-8b87ff5d67b24e068cce0e291b4df674-0.
INFO 01-31 12:47:49 logger.py:36] Received request cmpl-71dfdc706cbb4bb88441518fdd98e403-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:49 async_llm_engine.py:173] Added request cmpl-71dfdc706cbb4bb88441518fdd98e403-0.
INFO 01-31 12:47:49 logger.py:36] Received request cmpl-6065ac9aec5d4d16b2a48137c7e8272b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:49 async_llm_engine.py:173] Added request cmpl-6065ac9aec5d4d16b2a48137c7e8272b-0.
INFO 01-31 12:47:49 logger.py:36] Received request cmpl-6f66640805b94ca984c889ef687bb17d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:49 async_llm_engine.py:173] Added request cmpl-6f66640805b94ca984c889ef687bb17d-0.
INFO 01-31 12:47:49 logger.py:36] Received request cmpl-89ba318d79ec492e90acccd83c4cc4dd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:49 async_llm_engine.py:173] Added request cmpl-89ba318d79ec492e90acccd83c4cc4dd-0.
INFO 01-31 12:47:49 logger.py:36] Received request cmpl-218578056ba24207827b9ea62f6c33fb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:49 async_llm_engine.py:173] Added request cmpl-218578056ba24207827b9ea62f6c33fb-0.
INFO 01-31 12:47:49 logger.py:36] Received request cmpl-6f9693427a5448f0a06dc12694c6bdd6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:49 async_llm_engine.py:173] Added request cmpl-6f9693427a5448f0a06dc12694c6bdd6-0.
INFO 01-31 12:47:49 logger.py:36] Received request cmpl-28570a53e4454d8da9aa08425491a60a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:49 async_llm_engine.py:173] Added request cmpl-28570a53e4454d8da9aa08425491a60a-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:47:51 async_llm_engine.py:140] Finished request cmpl-b7e0bf208af742c7950f1738c1e976ef-0.
INFO 01-31 12:47:51 async_llm_engine.py:140] Finished request cmpl-0a55f971f3bb446ab5d9e0232c7fdd0c-0.
INFO 01-31 12:47:51 async_llm_engine.py:140] Finished request cmpl-8b87ff5d67b24e068cce0e291b4df674-0.
INFO 01-31 12:47:51 async_llm_engine.py:140] Finished request cmpl-71dfdc706cbb4bb88441518fdd98e403-0.
INFO 01-31 12:47:51 async_llm_engine.py:140] Finished request cmpl-6065ac9aec5d4d16b2a48137c7e8272b-0.
INFO 01-31 12:47:51 async_llm_engine.py:140] Finished request cmpl-6f66640805b94ca984c889ef687bb17d-0.
INFO 01-31 12:47:51 async_llm_engine.py:140] Finished request cmpl-89ba318d79ec492e90acccd83c4cc4dd-0.
INFO 01-31 12:47:51 async_llm_engine.py:140] Finished request cmpl-218578056ba24207827b9ea62f6c33fb-0.
INFO 01-31 12:47:51 async_llm_engine.py:140] Finished request cmpl-6f9693427a5448f0a06dc12694c6bdd6-0.
INFO 01-31 12:47:51 async_llm_engine.py:140] Finished request cmpl-28570a53e4454d8da9aa08425491a60a-0.
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:47:52 logger.py:36] Received request cmpl-cf66daa44aaf4cfe8dddc3f8b11ee843-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:52 logger.py:36] Received request cmpl-fec4cd68c341435aa72c400c7170fe68-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:52 async_llm_engine.py:173] Added request cmpl-cf66daa44aaf4cfe8dddc3f8b11ee843-0.
INFO 01-31 12:47:52 async_llm_engine.py:173] Added request cmpl-fec4cd68c341435aa72c400c7170fe68-0.
INFO 01-31 12:47:52 logger.py:36] Received request cmpl-0c4ee5cb54d04be59287fea0ffd55123-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:52 async_llm_engine.py:173] Added request cmpl-0c4ee5cb54d04be59287fea0ffd55123-0.
INFO 01-31 12:47:52 logger.py:36] Received request cmpl-444c99205c7d4bbca9d84c762fb3d809-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:52 logger.py:36] Received request cmpl-ad3f0faed9fd4f0c8cceb67a7857ff2c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:52 async_llm_engine.py:173] Added request cmpl-444c99205c7d4bbca9d84c762fb3d809-0.
INFO 01-31 12:47:52 async_llm_engine.py:173] Added request cmpl-ad3f0faed9fd4f0c8cceb67a7857ff2c-0.
INFO 01-31 12:47:52 logger.py:36] Received request cmpl-3b4d5d97abb041daac8ff00ebd7d6e62-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:52 async_llm_engine.py:173] Added request cmpl-3b4d5d97abb041daac8ff00ebd7d6e62-0.
INFO 01-31 12:47:52 logger.py:36] Received request cmpl-75d3c69a9f08406a8dfd4ee7a13bc336-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:52 async_llm_engine.py:173] Added request cmpl-75d3c69a9f08406a8dfd4ee7a13bc336-0.
INFO 01-31 12:47:52 logger.py:36] Received request cmpl-02a3cd854c184081a2baea2489619b62-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:52 async_llm_engine.py:173] Added request cmpl-02a3cd854c184081a2baea2489619b62-0.
INFO 01-31 12:47:52 logger.py:36] Received request cmpl-8e1aa8be57e14d5ab4b6c1aba4d9a0c4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:52 async_llm_engine.py:173] Added request cmpl-8e1aa8be57e14d5ab4b6c1aba4d9a0c4-0.
INFO 01-31 12:47:52 logger.py:36] Received request cmpl-06ae82f7448d4007a22046b89992950e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:52 async_llm_engine.py:173] Added request cmpl-06ae82f7448d4007a22046b89992950e-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:47:53 metrics.py:396] Avg prompt throughput: 35.8 tokens/s, Avg generation throughput: 211.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:47:54 async_llm_engine.py:140] Finished request cmpl-cf66daa44aaf4cfe8dddc3f8b11ee843-0.
INFO 01-31 12:47:54 async_llm_engine.py:140] Finished request cmpl-fec4cd68c341435aa72c400c7170fe68-0.
INFO 01-31 12:47:54 async_llm_engine.py:140] Finished request cmpl-0c4ee5cb54d04be59287fea0ffd55123-0.
INFO 01-31 12:47:54 async_llm_engine.py:140] Finished request cmpl-444c99205c7d4bbca9d84c762fb3d809-0.
INFO 01-31 12:47:54 async_llm_engine.py:140] Finished request cmpl-ad3f0faed9fd4f0c8cceb67a7857ff2c-0.
INFO 01-31 12:47:54 async_llm_engine.py:140] Finished request cmpl-3b4d5d97abb041daac8ff00ebd7d6e62-0.
INFO 01-31 12:47:54 async_llm_engine.py:140] Finished request cmpl-75d3c69a9f08406a8dfd4ee7a13bc336-0.
INFO 01-31 12:47:54 async_llm_engine.py:140] Finished request cmpl-02a3cd854c184081a2baea2489619b62-0.
INFO 01-31 12:47:54 async_llm_engine.py:140] Finished request cmpl-8e1aa8be57e14d5ab4b6c1aba4d9a0c4-0.
INFO 01-31 12:47:54 async_llm_engine.py:140] Finished request cmpl-06ae82f7448d4007a22046b89992950e-0.
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:47:55 logger.py:36] Received request cmpl-8468e80f5874423c93c9d7b9485b0052-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:55 async_llm_engine.py:173] Added request cmpl-8468e80f5874423c93c9d7b9485b0052-0.
INFO 01-31 12:47:55 logger.py:36] Received request cmpl-a5e8224f245d42858f6b0d8d67954289-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:55 async_llm_engine.py:173] Added request cmpl-a5e8224f245d42858f6b0d8d67954289-0.
INFO 01-31 12:47:55 logger.py:36] Received request cmpl-c098f98db6eb4c1eac19dc194a55ca32-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:55 async_llm_engine.py:173] Added request cmpl-c098f98db6eb4c1eac19dc194a55ca32-0.
INFO 01-31 12:47:55 logger.py:36] Received request cmpl-bf4a1e66e139486e9c0e16791f7a25fb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:55 async_llm_engine.py:173] Added request cmpl-bf4a1e66e139486e9c0e16791f7a25fb-0.
INFO 01-31 12:47:55 logger.py:36] Received request cmpl-8d9c12cd986d4b98af5406e00701578f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:55 async_llm_engine.py:173] Added request cmpl-8d9c12cd986d4b98af5406e00701578f-0.
INFO 01-31 12:47:55 logger.py:36] Received request cmpl-b9b54c7ede3f49feab47f4040e7f72a8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:55 async_llm_engine.py:173] Added request cmpl-b9b54c7ede3f49feab47f4040e7f72a8-0.
INFO 01-31 12:47:55 logger.py:36] Received request cmpl-068de37550634602a180f03f44e0a6b4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:55 async_llm_engine.py:173] Added request cmpl-068de37550634602a180f03f44e0a6b4-0.
INFO 01-31 12:47:55 logger.py:36] Received request cmpl-bf6568b97fc64b739ce385f52b42bdb0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:55 async_llm_engine.py:173] Added request cmpl-bf6568b97fc64b739ce385f52b42bdb0-0.
INFO 01-31 12:47:55 logger.py:36] Received request cmpl-31b89e7bb9074ce0b37b2519a0bb20e0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:55 async_llm_engine.py:173] Added request cmpl-31b89e7bb9074ce0b37b2519a0bb20e0-0.
INFO 01-31 12:47:55 logger.py:36] Received request cmpl-d1b8eeb15402432293cdeb20ac457376-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:55 async_llm_engine.py:173] Added request cmpl-d1b8eeb15402432293cdeb20ac457376-0.
INFO:     89.105.200.105:58860 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:58874 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:47:57 async_llm_engine.py:140] Finished request cmpl-8468e80f5874423c93c9d7b9485b0052-0.
INFO 01-31 12:47:57 async_llm_engine.py:140] Finished request cmpl-a5e8224f245d42858f6b0d8d67954289-0.
INFO 01-31 12:47:57 async_llm_engine.py:140] Finished request cmpl-c098f98db6eb4c1eac19dc194a55ca32-0.
INFO 01-31 12:47:57 async_llm_engine.py:140] Finished request cmpl-bf4a1e66e139486e9c0e16791f7a25fb-0.
INFO 01-31 12:47:57 async_llm_engine.py:140] Finished request cmpl-8d9c12cd986d4b98af5406e00701578f-0.
INFO 01-31 12:47:57 async_llm_engine.py:140] Finished request cmpl-b9b54c7ede3f49feab47f4040e7f72a8-0.
INFO 01-31 12:47:57 async_llm_engine.py:140] Finished request cmpl-068de37550634602a180f03f44e0a6b4-0.
INFO 01-31 12:47:57 async_llm_engine.py:140] Finished request cmpl-bf6568b97fc64b739ce385f52b42bdb0-0.
INFO 01-31 12:47:57 async_llm_engine.py:140] Finished request cmpl-31b89e7bb9074ce0b37b2519a0bb20e0-0.
INFO 01-31 12:47:57 async_llm_engine.py:140] Finished request cmpl-d1b8eeb15402432293cdeb20ac457376-0.
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:47:58 logger.py:36] Received request cmpl-4af3f15136674658adb560a526185753-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:58 async_llm_engine.py:173] Added request cmpl-4af3f15136674658adb560a526185753-0.
INFO 01-31 12:47:58 logger.py:36] Received request cmpl-e6b57fba2d194909bfa16eaace617f3a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:58 logger.py:36] Received request cmpl-d6634294a17748c1bedfcdd08fa18cff-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:58 logger.py:36] Received request cmpl-94f79e63b1ee4d85a8c7618e5b01def4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:58 async_llm_engine.py:173] Added request cmpl-e6b57fba2d194909bfa16eaace617f3a-0.
INFO 01-31 12:47:58 async_llm_engine.py:173] Added request cmpl-d6634294a17748c1bedfcdd08fa18cff-0.
INFO 01-31 12:47:58 async_llm_engine.py:173] Added request cmpl-94f79e63b1ee4d85a8c7618e5b01def4-0.
INFO 01-31 12:47:58 logger.py:36] Received request cmpl-ad1256d299384b42999773093f40182b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:58 logger.py:36] Received request cmpl-5e21c893bad145118f8cce070240b40e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:58 async_llm_engine.py:173] Added request cmpl-ad1256d299384b42999773093f40182b-0.
INFO 01-31 12:47:58 async_llm_engine.py:173] Added request cmpl-5e21c893bad145118f8cce070240b40e-0.
INFO 01-31 12:47:58 logger.py:36] Received request cmpl-c2611c32fba5417ea83d018742656056-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:58 async_llm_engine.py:173] Added request cmpl-c2611c32fba5417ea83d018742656056-0.
INFO 01-31 12:47:58 logger.py:36] Received request cmpl-a09376b1bbf94c2f9c28471395f55c77-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:58 logger.py:36] Received request cmpl-5ee615171f424b97ac61b7bce920f1a5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:58 async_llm_engine.py:173] Added request cmpl-a09376b1bbf94c2f9c28471395f55c77-0.
INFO 01-31 12:47:58 async_llm_engine.py:173] Added request cmpl-5ee615171f424b97ac61b7bce920f1a5-0.
INFO 01-31 12:47:58 logger.py:36] Received request cmpl-09d9e56d35eb4ada9aae6e56a0285827-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:47:58 async_llm_engine.py:173] Added request cmpl-09d9e56d35eb4ada9aae6e56a0285827-0.
INFO 01-31 12:47:58 metrics.py:396] Avg prompt throughput: 35.9 tokens/s, Avg generation throughput: 209.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:00 async_llm_engine.py:140] Finished request cmpl-4af3f15136674658adb560a526185753-0.
INFO 01-31 12:48:00 async_llm_engine.py:140] Finished request cmpl-e6b57fba2d194909bfa16eaace617f3a-0.
INFO 01-31 12:48:00 async_llm_engine.py:140] Finished request cmpl-d6634294a17748c1bedfcdd08fa18cff-0.
INFO 01-31 12:48:00 async_llm_engine.py:140] Finished request cmpl-94f79e63b1ee4d85a8c7618e5b01def4-0.
INFO 01-31 12:48:00 async_llm_engine.py:140] Finished request cmpl-ad1256d299384b42999773093f40182b-0.
INFO 01-31 12:48:00 async_llm_engine.py:140] Finished request cmpl-5e21c893bad145118f8cce070240b40e-0.
INFO 01-31 12:48:00 async_llm_engine.py:140] Finished request cmpl-c2611c32fba5417ea83d018742656056-0.
INFO 01-31 12:48:00 async_llm_engine.py:140] Finished request cmpl-a09376b1bbf94c2f9c28471395f55c77-0.
INFO 01-31 12:48:00 async_llm_engine.py:140] Finished request cmpl-5ee615171f424b97ac61b7bce920f1a5-0.
INFO 01-31 12:48:00 async_llm_engine.py:140] Finished request cmpl-09d9e56d35eb4ada9aae6e56a0285827-0.
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:01 logger.py:36] Received request cmpl-10a1c491efc7428b972d9497d25c5b2c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:01 async_llm_engine.py:173] Added request cmpl-10a1c491efc7428b972d9497d25c5b2c-0.
INFO 01-31 12:48:01 logger.py:36] Received request cmpl-6979e879274748d9bb6489b9fa83d793-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:01 async_llm_engine.py:173] Added request cmpl-6979e879274748d9bb6489b9fa83d793-0.
INFO 01-31 12:48:01 logger.py:36] Received request cmpl-29a7ba6016ea4ba1b8819d8417b313a8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:01 async_llm_engine.py:173] Added request cmpl-29a7ba6016ea4ba1b8819d8417b313a8-0.
INFO 01-31 12:48:01 logger.py:36] Received request cmpl-5c80fdb328a74dfb9fb8a2f9731d89a8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:01 async_llm_engine.py:173] Added request cmpl-5c80fdb328a74dfb9fb8a2f9731d89a8-0.
INFO 01-31 12:48:01 logger.py:36] Received request cmpl-cc11129825fa43eaa2526673d8f90f1b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:01 async_llm_engine.py:173] Added request cmpl-cc11129825fa43eaa2526673d8f90f1b-0.
INFO 01-31 12:48:01 logger.py:36] Received request cmpl-a42e13470f8143bf9ecdf9ce10f9b82c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:01 async_llm_engine.py:173] Added request cmpl-a42e13470f8143bf9ecdf9ce10f9b82c-0.
INFO 01-31 12:48:01 logger.py:36] Received request cmpl-8362bbcfc1f24efaa4af6e287c3b3356-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:01 async_llm_engine.py:173] Added request cmpl-8362bbcfc1f24efaa4af6e287c3b3356-0.
INFO 01-31 12:48:01 logger.py:36] Received request cmpl-4fa3f90f1b484a0b81a45c17e595697c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:01 logger.py:36] Received request cmpl-b2c480c01bd5499ab763c76feb13cb84-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:01 logger.py:36] Received request cmpl-3179619fb3ec4b3798d523f0b83b7df8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:01 async_llm_engine.py:173] Added request cmpl-4fa3f90f1b484a0b81a45c17e595697c-0.
INFO 01-31 12:48:01 async_llm_engine.py:173] Added request cmpl-b2c480c01bd5499ab763c76feb13cb84-0.
INFO 01-31 12:48:01 async_llm_engine.py:173] Added request cmpl-3179619fb3ec4b3798d523f0b83b7df8-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:03 metrics.py:396] Avg prompt throughput: 17.9 tokens/s, Avg generation throughput: 217.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:04 async_llm_engine.py:140] Finished request cmpl-10a1c491efc7428b972d9497d25c5b2c-0.
INFO 01-31 12:48:04 async_llm_engine.py:140] Finished request cmpl-6979e879274748d9bb6489b9fa83d793-0.
INFO 01-31 12:48:04 async_llm_engine.py:140] Finished request cmpl-29a7ba6016ea4ba1b8819d8417b313a8-0.
INFO 01-31 12:48:04 async_llm_engine.py:140] Finished request cmpl-5c80fdb328a74dfb9fb8a2f9731d89a8-0.
INFO 01-31 12:48:04 async_llm_engine.py:140] Finished request cmpl-cc11129825fa43eaa2526673d8f90f1b-0.
INFO 01-31 12:48:04 async_llm_engine.py:140] Finished request cmpl-a42e13470f8143bf9ecdf9ce10f9b82c-0.
INFO 01-31 12:48:04 async_llm_engine.py:140] Finished request cmpl-8362bbcfc1f24efaa4af6e287c3b3356-0.
INFO 01-31 12:48:04 async_llm_engine.py:140] Finished request cmpl-4fa3f90f1b484a0b81a45c17e595697c-0.
INFO 01-31 12:48:04 async_llm_engine.py:140] Finished request cmpl-b2c480c01bd5499ab763c76feb13cb84-0.
INFO 01-31 12:48:04 async_llm_engine.py:140] Finished request cmpl-3179619fb3ec4b3798d523f0b83b7df8-0.
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:04 logger.py:36] Received request cmpl-07095717e8814f8b84947ee660563a3c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:04 async_llm_engine.py:173] Added request cmpl-07095717e8814f8b84947ee660563a3c-0.
INFO 01-31 12:48:04 logger.py:36] Received request cmpl-bf736deb937a4eca80441622a40f0612-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:04 async_llm_engine.py:173] Added request cmpl-bf736deb937a4eca80441622a40f0612-0.
INFO 01-31 12:48:04 logger.py:36] Received request cmpl-2de47207b94d44f8bd47dddc8604961a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:04 async_llm_engine.py:173] Added request cmpl-2de47207b94d44f8bd47dddc8604961a-0.
INFO 01-31 12:48:04 logger.py:36] Received request cmpl-822715640ad44b4ea315a44235928445-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:04 async_llm_engine.py:173] Added request cmpl-822715640ad44b4ea315a44235928445-0.
INFO 01-31 12:48:04 logger.py:36] Received request cmpl-2070509f5e4846e29c0ab8cfece5c824-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:04 async_llm_engine.py:173] Added request cmpl-2070509f5e4846e29c0ab8cfece5c824-0.
INFO 01-31 12:48:04 logger.py:36] Received request cmpl-4038bd1d80ba435db38778517a20634e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:04 async_llm_engine.py:173] Added request cmpl-4038bd1d80ba435db38778517a20634e-0.
INFO 01-31 12:48:04 logger.py:36] Received request cmpl-fcdb308d95d348eb84b1aeffbcc90e1f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:04 async_llm_engine.py:173] Added request cmpl-fcdb308d95d348eb84b1aeffbcc90e1f-0.
INFO 01-31 12:48:04 logger.py:36] Received request cmpl-1f152076708a4f10a675b1fac28184fd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:04 async_llm_engine.py:173] Added request cmpl-1f152076708a4f10a675b1fac28184fd-0.
INFO 01-31 12:48:04 logger.py:36] Received request cmpl-4df391ee902d4a27afa8827e6f2083b3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:04 async_llm_engine.py:173] Added request cmpl-4df391ee902d4a27afa8827e6f2083b3-0.
INFO 01-31 12:48:04 logger.py:36] Received request cmpl-09b040af61ad4355be56debe333752c2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:04 async_llm_engine.py:173] Added request cmpl-09b040af61ad4355be56debe333752c2-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:36626 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:36622 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:07 async_llm_engine.py:140] Finished request cmpl-07095717e8814f8b84947ee660563a3c-0.
INFO 01-31 12:48:07 async_llm_engine.py:140] Finished request cmpl-bf736deb937a4eca80441622a40f0612-0.
INFO 01-31 12:48:07 async_llm_engine.py:140] Finished request cmpl-2de47207b94d44f8bd47dddc8604961a-0.
INFO 01-31 12:48:07 async_llm_engine.py:140] Finished request cmpl-822715640ad44b4ea315a44235928445-0.
INFO 01-31 12:48:07 async_llm_engine.py:140] Finished request cmpl-2070509f5e4846e29c0ab8cfece5c824-0.
INFO 01-31 12:48:07 async_llm_engine.py:140] Finished request cmpl-4038bd1d80ba435db38778517a20634e-0.
INFO 01-31 12:48:07 async_llm_engine.py:140] Finished request cmpl-fcdb308d95d348eb84b1aeffbcc90e1f-0.
INFO 01-31 12:48:07 async_llm_engine.py:140] Finished request cmpl-1f152076708a4f10a675b1fac28184fd-0.
INFO 01-31 12:48:07 async_llm_engine.py:140] Finished request cmpl-4df391ee902d4a27afa8827e6f2083b3-0.
INFO 01-31 12:48:07 async_llm_engine.py:140] Finished request cmpl-09b040af61ad4355be56debe333752c2-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:07 logger.py:36] Received request cmpl-61d0de9e96504052a4dd2fbc64c68030-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:07 logger.py:36] Received request cmpl-4b3b4f9ffe9a4fcb88fc5597e77682d9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:07 async_llm_engine.py:173] Added request cmpl-61d0de9e96504052a4dd2fbc64c68030-0.
INFO 01-31 12:48:07 async_llm_engine.py:173] Added request cmpl-4b3b4f9ffe9a4fcb88fc5597e77682d9-0.
INFO 01-31 12:48:07 logger.py:36] Received request cmpl-ca9863e6213849058c10731e8c09cb1a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:07 async_llm_engine.py:173] Added request cmpl-ca9863e6213849058c10731e8c09cb1a-0.
INFO 01-31 12:48:07 logger.py:36] Received request cmpl-2800d5dea6cc499ea83e6c2841127d4c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:07 logger.py:36] Received request cmpl-a71f1ef55fc0434282b0157e20ca0665-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:07 async_llm_engine.py:173] Added request cmpl-2800d5dea6cc499ea83e6c2841127d4c-0.
INFO 01-31 12:48:07 async_llm_engine.py:173] Added request cmpl-a71f1ef55fc0434282b0157e20ca0665-0.
INFO 01-31 12:48:07 logger.py:36] Received request cmpl-651dd9a71b5b424381936fcf368e763c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:07 async_llm_engine.py:173] Added request cmpl-651dd9a71b5b424381936fcf368e763c-0.
INFO 01-31 12:48:07 logger.py:36] Received request cmpl-5a869a3829c1406da9d24981bf2d2595-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:07 async_llm_engine.py:173] Added request cmpl-5a869a3829c1406da9d24981bf2d2595-0.
INFO 01-31 12:48:07 logger.py:36] Received request cmpl-1df6fac6fe69417ab91881298ff2c9b4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:07 async_llm_engine.py:173] Added request cmpl-1df6fac6fe69417ab91881298ff2c9b4-0.
INFO 01-31 12:48:07 logger.py:36] Received request cmpl-56a4e7e5afca45ea9cb90a8769594f33-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:07 logger.py:36] Received request cmpl-15c830f04d094b4e8b93a8bb8be0c382-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:07 async_llm_engine.py:173] Added request cmpl-56a4e7e5afca45ea9cb90a8769594f33-0.
INFO 01-31 12:48:07 async_llm_engine.py:173] Added request cmpl-15c830f04d094b4e8b93a8bb8be0c382-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:08 metrics.py:396] Avg prompt throughput: 35.9 tokens/s, Avg generation throughput: 209.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:10 async_llm_engine.py:140] Finished request cmpl-61d0de9e96504052a4dd2fbc64c68030-0.
INFO 01-31 12:48:10 async_llm_engine.py:140] Finished request cmpl-4b3b4f9ffe9a4fcb88fc5597e77682d9-0.
INFO 01-31 12:48:10 async_llm_engine.py:140] Finished request cmpl-ca9863e6213849058c10731e8c09cb1a-0.
INFO 01-31 12:48:10 async_llm_engine.py:140] Finished request cmpl-2800d5dea6cc499ea83e6c2841127d4c-0.
INFO 01-31 12:48:10 async_llm_engine.py:140] Finished request cmpl-a71f1ef55fc0434282b0157e20ca0665-0.
INFO 01-31 12:48:10 async_llm_engine.py:140] Finished request cmpl-651dd9a71b5b424381936fcf368e763c-0.
INFO 01-31 12:48:10 async_llm_engine.py:140] Finished request cmpl-5a869a3829c1406da9d24981bf2d2595-0.
INFO 01-31 12:48:10 async_llm_engine.py:140] Finished request cmpl-1df6fac6fe69417ab91881298ff2c9b4-0.
INFO 01-31 12:48:10 async_llm_engine.py:140] Finished request cmpl-56a4e7e5afca45ea9cb90a8769594f33-0.
INFO 01-31 12:48:10 async_llm_engine.py:140] Finished request cmpl-15c830f04d094b4e8b93a8bb8be0c382-0.
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:10 logger.py:36] Received request cmpl-a4b8983f694b4615a426197396fb3444-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:10 async_llm_engine.py:173] Added request cmpl-a4b8983f694b4615a426197396fb3444-0.
INFO 01-31 12:48:10 logger.py:36] Received request cmpl-34506c1163554839874ddb6d9020c956-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:10 async_llm_engine.py:173] Added request cmpl-34506c1163554839874ddb6d9020c956-0.
INFO 01-31 12:48:10 logger.py:36] Received request cmpl-7206a49fe802419386b3d86c9ff7ab4c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:10 logger.py:36] Received request cmpl-298a1bf8fe3440edaec23c65510e62c0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:10 async_llm_engine.py:173] Added request cmpl-7206a49fe802419386b3d86c9ff7ab4c-0.
INFO 01-31 12:48:10 async_llm_engine.py:173] Added request cmpl-298a1bf8fe3440edaec23c65510e62c0-0.
INFO 01-31 12:48:10 logger.py:36] Received request cmpl-9a8bd03795e0459e920d807445b0acdd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:10 async_llm_engine.py:173] Added request cmpl-9a8bd03795e0459e920d807445b0acdd-0.
INFO 01-31 12:48:10 logger.py:36] Received request cmpl-d0c273eabfd84da58577c5b3ffd09df5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:10 logger.py:36] Received request cmpl-7a4569b0ba53461d8d0f83d73f030ac6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:10 logger.py:36] Received request cmpl-83568760b19746c99323640a484ca002-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:10 logger.py:36] Received request cmpl-06df3f5600984211825543855b3f26bb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:10 async_llm_engine.py:173] Added request cmpl-d0c273eabfd84da58577c5b3ffd09df5-0.
INFO 01-31 12:48:10 async_llm_engine.py:173] Added request cmpl-7a4569b0ba53461d8d0f83d73f030ac6-0.
INFO 01-31 12:48:10 async_llm_engine.py:173] Added request cmpl-83568760b19746c99323640a484ca002-0.
INFO 01-31 12:48:10 async_llm_engine.py:173] Added request cmpl-06df3f5600984211825543855b3f26bb-0.
INFO 01-31 12:48:10 logger.py:36] Received request cmpl-9e9168baaca3490bbd7061d67ea2df13-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:10 async_llm_engine.py:173] Added request cmpl-9e9168baaca3490bbd7061d67ea2df13-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:13 async_llm_engine.py:140] Finished request cmpl-a4b8983f694b4615a426197396fb3444-0.
INFO 01-31 12:48:13 async_llm_engine.py:140] Finished request cmpl-34506c1163554839874ddb6d9020c956-0.
INFO 01-31 12:48:13 async_llm_engine.py:140] Finished request cmpl-7206a49fe802419386b3d86c9ff7ab4c-0.
INFO 01-31 12:48:13 async_llm_engine.py:140] Finished request cmpl-298a1bf8fe3440edaec23c65510e62c0-0.
INFO 01-31 12:48:13 async_llm_engine.py:140] Finished request cmpl-9a8bd03795e0459e920d807445b0acdd-0.
INFO 01-31 12:48:13 async_llm_engine.py:140] Finished request cmpl-d0c273eabfd84da58577c5b3ffd09df5-0.
INFO 01-31 12:48:13 async_llm_engine.py:140] Finished request cmpl-7a4569b0ba53461d8d0f83d73f030ac6-0.
INFO 01-31 12:48:13 async_llm_engine.py:140] Finished request cmpl-83568760b19746c99323640a484ca002-0.
INFO 01-31 12:48:13 async_llm_engine.py:140] Finished request cmpl-06df3f5600984211825543855b3f26bb-0.
INFO 01-31 12:48:13 async_llm_engine.py:140] Finished request cmpl-9e9168baaca3490bbd7061d67ea2df13-0.
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:13 logger.py:36] Received request cmpl-34f3b2998bbd48cda3f2633cc6483c16-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:13 async_llm_engine.py:173] Added request cmpl-34f3b2998bbd48cda3f2633cc6483c16-0.
INFO 01-31 12:48:13 logger.py:36] Received request cmpl-10c12fa19a60428c96c58563ca6831c7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:13 logger.py:36] Received request cmpl-a1be3b5081674665a094ed1b7a588738-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:13 async_llm_engine.py:173] Added request cmpl-10c12fa19a60428c96c58563ca6831c7-0.
INFO 01-31 12:48:13 async_llm_engine.py:173] Added request cmpl-a1be3b5081674665a094ed1b7a588738-0.
INFO 01-31 12:48:13 logger.py:36] Received request cmpl-86d1dd23e13e4de5856862c4959839af-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:13 async_llm_engine.py:173] Added request cmpl-86d1dd23e13e4de5856862c4959839af-0.
INFO 01-31 12:48:13 logger.py:36] Received request cmpl-4a8cb50b21ae48e5b27904d3cd73e6d8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:13 async_llm_engine.py:173] Added request cmpl-4a8cb50b21ae48e5b27904d3cd73e6d8-0.
INFO 01-31 12:48:13 logger.py:36] Received request cmpl-a7cbf67b2d244b74a56acf09b54448c7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:13 async_llm_engine.py:173] Added request cmpl-a7cbf67b2d244b74a56acf09b54448c7-0.
INFO 01-31 12:48:13 logger.py:36] Received request cmpl-a2de57a3f460469e9a50ee5134254555-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:13 async_llm_engine.py:173] Added request cmpl-a2de57a3f460469e9a50ee5134254555-0.
INFO 01-31 12:48:13 logger.py:36] Received request cmpl-8fb522bfd50345f7816700983dbb53f0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:13 async_llm_engine.py:173] Added request cmpl-8fb522bfd50345f7816700983dbb53f0-0.
INFO 01-31 12:48:13 logger.py:36] Received request cmpl-6aaf264bd4944da7a78d3023a0bdced5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:13 async_llm_engine.py:173] Added request cmpl-6aaf264bd4944da7a78d3023a0bdced5-0.
INFO 01-31 12:48:13 logger.py:36] Received request cmpl-4b134b682e4a406a81130a04277e4991-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:13 async_llm_engine.py:173] Added request cmpl-4b134b682e4a406a81130a04277e4991-0.
INFO 01-31 12:48:13 metrics.py:396] Avg prompt throughput: 35.9 tokens/s, Avg generation throughput: 211.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:49976 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:49982 - "GET /health HTTP/1.1" 200 OK
INFO 01-31 12:48:16 async_llm_engine.py:140] Finished request cmpl-34f3b2998bbd48cda3f2633cc6483c16-0.
INFO 01-31 12:48:16 async_llm_engine.py:140] Finished request cmpl-10c12fa19a60428c96c58563ca6831c7-0.
INFO 01-31 12:48:16 async_llm_engine.py:140] Finished request cmpl-a1be3b5081674665a094ed1b7a588738-0.
INFO 01-31 12:48:16 async_llm_engine.py:140] Finished request cmpl-86d1dd23e13e4de5856862c4959839af-0.
INFO 01-31 12:48:16 async_llm_engine.py:140] Finished request cmpl-4a8cb50b21ae48e5b27904d3cd73e6d8-0.
INFO 01-31 12:48:16 async_llm_engine.py:140] Finished request cmpl-a7cbf67b2d244b74a56acf09b54448c7-0.
INFO 01-31 12:48:16 async_llm_engine.py:140] Finished request cmpl-a2de57a3f460469e9a50ee5134254555-0.
INFO 01-31 12:48:16 async_llm_engine.py:140] Finished request cmpl-8fb522bfd50345f7816700983dbb53f0-0.
INFO 01-31 12:48:16 async_llm_engine.py:140] Finished request cmpl-6aaf264bd4944da7a78d3023a0bdced5-0.
INFO 01-31 12:48:16 async_llm_engine.py:140] Finished request cmpl-4b134b682e4a406a81130a04277e4991-0.
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:16 logger.py:36] Received request cmpl-a1e232c684ed4dcfa4e03548573c51d2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:16 async_llm_engine.py:173] Added request cmpl-a1e232c684ed4dcfa4e03548573c51d2-0.
INFO 01-31 12:48:16 logger.py:36] Received request cmpl-587c5bbe9e9144ffaa91f3ee8ae9caa3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:16 logger.py:36] Received request cmpl-37565817bb1843e399527147db329700-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:16 logger.py:36] Received request cmpl-b8632194375147f9b0204e5d704784e3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:16 logger.py:36] Received request cmpl-e00e544fc34149129aacfc4b98347d98-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:16 async_llm_engine.py:173] Added request cmpl-587c5bbe9e9144ffaa91f3ee8ae9caa3-0.
INFO 01-31 12:48:16 async_llm_engine.py:173] Added request cmpl-37565817bb1843e399527147db329700-0.
INFO 01-31 12:48:16 async_llm_engine.py:173] Added request cmpl-b8632194375147f9b0204e5d704784e3-0.
INFO 01-31 12:48:16 async_llm_engine.py:173] Added request cmpl-e00e544fc34149129aacfc4b98347d98-0.
INFO 01-31 12:48:16 logger.py:36] Received request cmpl-87c796bc28874ab3bca1175d041355c8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:16 async_llm_engine.py:173] Added request cmpl-87c796bc28874ab3bca1175d041355c8-0.
INFO 01-31 12:48:16 logger.py:36] Received request cmpl-a2c9fe7068514d0ca14a139535b1b03e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:16 async_llm_engine.py:173] Added request cmpl-a2c9fe7068514d0ca14a139535b1b03e-0.
INFO 01-31 12:48:16 logger.py:36] Received request cmpl-120b9d45fabf4de6854b78aa4355f2a5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:16 async_llm_engine.py:173] Added request cmpl-120b9d45fabf4de6854b78aa4355f2a5-0.
INFO 01-31 12:48:16 logger.py:36] Received request cmpl-106889c2fd7841ed8540000bf82693a3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:16 async_llm_engine.py:173] Added request cmpl-106889c2fd7841ed8540000bf82693a3-0.
INFO 01-31 12:48:16 logger.py:36] Received request cmpl-16327e8b7a6f41d08baee870d8de15e9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:16 async_llm_engine.py:173] Added request cmpl-16327e8b7a6f41d08baee870d8de15e9-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:18 metrics.py:396] Avg prompt throughput: 17.9 tokens/s, Avg generation throughput: 220.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:19 async_llm_engine.py:140] Finished request cmpl-a1e232c684ed4dcfa4e03548573c51d2-0.
INFO 01-31 12:48:19 async_llm_engine.py:140] Finished request cmpl-587c5bbe9e9144ffaa91f3ee8ae9caa3-0.
INFO 01-31 12:48:19 async_llm_engine.py:140] Finished request cmpl-37565817bb1843e399527147db329700-0.
INFO 01-31 12:48:19 async_llm_engine.py:140] Finished request cmpl-b8632194375147f9b0204e5d704784e3-0.
INFO 01-31 12:48:19 async_llm_engine.py:140] Finished request cmpl-e00e544fc34149129aacfc4b98347d98-0.
INFO 01-31 12:48:19 async_llm_engine.py:140] Finished request cmpl-87c796bc28874ab3bca1175d041355c8-0.
INFO 01-31 12:48:19 async_llm_engine.py:140] Finished request cmpl-a2c9fe7068514d0ca14a139535b1b03e-0.
INFO 01-31 12:48:19 async_llm_engine.py:140] Finished request cmpl-120b9d45fabf4de6854b78aa4355f2a5-0.
INFO 01-31 12:48:19 async_llm_engine.py:140] Finished request cmpl-106889c2fd7841ed8540000bf82693a3-0.
INFO 01-31 12:48:19 async_llm_engine.py:140] Finished request cmpl-16327e8b7a6f41d08baee870d8de15e9-0.
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:19 logger.py:36] Received request cmpl-8e593e04da6a45c49d59ca2df13ef9ab-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:19 async_llm_engine.py:173] Added request cmpl-8e593e04da6a45c49d59ca2df13ef9ab-0.
INFO 01-31 12:48:19 logger.py:36] Received request cmpl-e42439de0f944e76a45ca91be957d92f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:19 async_llm_engine.py:173] Added request cmpl-e42439de0f944e76a45ca91be957d92f-0.
INFO 01-31 12:48:19 logger.py:36] Received request cmpl-3ffa9a17638240bca4d5a1673c40998c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:19 logger.py:36] Received request cmpl-374faca886da416fb72a4f71bb0739a8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:19 logger.py:36] Received request cmpl-1df24511c6e941df9678174361d8ab90-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:19 async_llm_engine.py:173] Added request cmpl-3ffa9a17638240bca4d5a1673c40998c-0.
INFO 01-31 12:48:19 async_llm_engine.py:173] Added request cmpl-374faca886da416fb72a4f71bb0739a8-0.
INFO 01-31 12:48:19 async_llm_engine.py:173] Added request cmpl-1df24511c6e941df9678174361d8ab90-0.
INFO 01-31 12:48:19 logger.py:36] Received request cmpl-d3df21f9baa945e0acf94ae629b393db-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:19 async_llm_engine.py:173] Added request cmpl-d3df21f9baa945e0acf94ae629b393db-0.
INFO 01-31 12:48:19 logger.py:36] Received request cmpl-89edf8d6812b443b83a56d95e7aeb610-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:19 async_llm_engine.py:173] Added request cmpl-89edf8d6812b443b83a56d95e7aeb610-0.
INFO 01-31 12:48:19 logger.py:36] Received request cmpl-016dd9fe9fbf41cdb432c23225633ad2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:19 async_llm_engine.py:173] Added request cmpl-016dd9fe9fbf41cdb432c23225633ad2-0.
INFO 01-31 12:48:19 logger.py:36] Received request cmpl-f9a56930f8c94ad2b9bd2d8db1ce2481-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:19 async_llm_engine.py:173] Added request cmpl-f9a56930f8c94ad2b9bd2d8db1ce2481-0.
INFO 01-31 12:48:19 logger.py:36] Received request cmpl-b9abfb513b3949a2a3cb685facd630fc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:19 async_llm_engine.py:173] Added request cmpl-b9abfb513b3949a2a3cb685facd630fc-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:22 async_llm_engine.py:140] Finished request cmpl-8e593e04da6a45c49d59ca2df13ef9ab-0.
INFO 01-31 12:48:22 async_llm_engine.py:140] Finished request cmpl-e42439de0f944e76a45ca91be957d92f-0.
INFO 01-31 12:48:22 async_llm_engine.py:140] Finished request cmpl-3ffa9a17638240bca4d5a1673c40998c-0.
INFO 01-31 12:48:22 async_llm_engine.py:140] Finished request cmpl-374faca886da416fb72a4f71bb0739a8-0.
INFO 01-31 12:48:22 async_llm_engine.py:140] Finished request cmpl-1df24511c6e941df9678174361d8ab90-0.
INFO 01-31 12:48:22 async_llm_engine.py:140] Finished request cmpl-d3df21f9baa945e0acf94ae629b393db-0.
INFO 01-31 12:48:22 async_llm_engine.py:140] Finished request cmpl-89edf8d6812b443b83a56d95e7aeb610-0.
INFO 01-31 12:48:22 async_llm_engine.py:140] Finished request cmpl-016dd9fe9fbf41cdb432c23225633ad2-0.
INFO 01-31 12:48:22 async_llm_engine.py:140] Finished request cmpl-f9a56930f8c94ad2b9bd2d8db1ce2481-0.
INFO 01-31 12:48:22 async_llm_engine.py:140] Finished request cmpl-b9abfb513b3949a2a3cb685facd630fc-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:22 logger.py:36] Received request cmpl-5ebe69458dd54ade8763a87334bf6282-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:22 async_llm_engine.py:173] Added request cmpl-5ebe69458dd54ade8763a87334bf6282-0.
INFO 01-31 12:48:22 logger.py:36] Received request cmpl-3055e141a83e4691b7691e50e1180551-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:22 async_llm_engine.py:173] Added request cmpl-3055e141a83e4691b7691e50e1180551-0.
INFO 01-31 12:48:22 logger.py:36] Received request cmpl-c95381e23d2742218f8f6434cb1b5d71-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:22 async_llm_engine.py:173] Added request cmpl-c95381e23d2742218f8f6434cb1b5d71-0.
INFO 01-31 12:48:22 logger.py:36] Received request cmpl-64081e28cb89446aa5524cec0551974a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:22 async_llm_engine.py:173] Added request cmpl-64081e28cb89446aa5524cec0551974a-0.
INFO 01-31 12:48:22 logger.py:36] Received request cmpl-93f30aed06cc4526ac782751535bbacf-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:22 async_llm_engine.py:173] Added request cmpl-93f30aed06cc4526ac782751535bbacf-0.
INFO 01-31 12:48:22 logger.py:36] Received request cmpl-f9dcadefd58b4aaf81410cbe98b03c3b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:22 logger.py:36] Received request cmpl-a4d881cc6bfc4a20b74bf06bf0012aa0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:22 async_llm_engine.py:173] Added request cmpl-f9dcadefd58b4aaf81410cbe98b03c3b-0.
INFO 01-31 12:48:22 async_llm_engine.py:173] Added request cmpl-a4d881cc6bfc4a20b74bf06bf0012aa0-0.
INFO 01-31 12:48:22 logger.py:36] Received request cmpl-c650a1ea58644569a8c212a793d789f3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:22 logger.py:36] Received request cmpl-d78742b8fc2b4975937901171f48f473-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:22 async_llm_engine.py:173] Added request cmpl-c650a1ea58644569a8c212a793d789f3-0.
INFO 01-31 12:48:22 async_llm_engine.py:173] Added request cmpl-d78742b8fc2b4975937901171f48f473-0.
INFO 01-31 12:48:22 logger.py:36] Received request cmpl-51d873bb620b4cfaa3464bc40a4dcb50-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:22 async_llm_engine.py:173] Added request cmpl-51d873bb620b4cfaa3464bc40a4dcb50-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:23 metrics.py:396] Avg prompt throughput: 35.8 tokens/s, Avg generation throughput: 211.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:25 async_llm_engine.py:140] Finished request cmpl-5ebe69458dd54ade8763a87334bf6282-0.
INFO 01-31 12:48:25 async_llm_engine.py:140] Finished request cmpl-3055e141a83e4691b7691e50e1180551-0.
INFO 01-31 12:48:25 async_llm_engine.py:140] Finished request cmpl-c95381e23d2742218f8f6434cb1b5d71-0.
INFO 01-31 12:48:25 async_llm_engine.py:140] Finished request cmpl-64081e28cb89446aa5524cec0551974a-0.
INFO 01-31 12:48:25 async_llm_engine.py:140] Finished request cmpl-93f30aed06cc4526ac782751535bbacf-0.
INFO 01-31 12:48:25 async_llm_engine.py:140] Finished request cmpl-f9dcadefd58b4aaf81410cbe98b03c3b-0.
INFO 01-31 12:48:25 async_llm_engine.py:140] Finished request cmpl-a4d881cc6bfc4a20b74bf06bf0012aa0-0.
INFO 01-31 12:48:25 async_llm_engine.py:140] Finished request cmpl-c650a1ea58644569a8c212a793d789f3-0.
INFO 01-31 12:48:25 async_llm_engine.py:140] Finished request cmpl-d78742b8fc2b4975937901171f48f473-0.
INFO 01-31 12:48:25 async_llm_engine.py:140] Finished request cmpl-51d873bb620b4cfaa3464bc40a4dcb50-0.
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:25 logger.py:36] Received request cmpl-b1c9678dad5244ec8c911dc64225a6d6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:25 async_llm_engine.py:173] Added request cmpl-b1c9678dad5244ec8c911dc64225a6d6-0.
INFO 01-31 12:48:25 logger.py:36] Received request cmpl-b814dfd5e34942c0b9d92374fb261dca-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:25 async_llm_engine.py:173] Added request cmpl-b814dfd5e34942c0b9d92374fb261dca-0.
INFO 01-31 12:48:25 logger.py:36] Received request cmpl-6089102c6fbb462d929a4fb484792b3b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:25 async_llm_engine.py:173] Added request cmpl-6089102c6fbb462d929a4fb484792b3b-0.
INFO 01-31 12:48:25 logger.py:36] Received request cmpl-6230da8880654941ab89d3f81f3427a7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:25 async_llm_engine.py:173] Added request cmpl-6230da8880654941ab89d3f81f3427a7-0.
INFO 01-31 12:48:25 logger.py:36] Received request cmpl-f3bbf8a49f524f24bb169b8781b9c41f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:25 async_llm_engine.py:173] Added request cmpl-f3bbf8a49f524f24bb169b8781b9c41f-0.
INFO 01-31 12:48:25 logger.py:36] Received request cmpl-e35fc1ec528f4f25ba59cea5e3c458c3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:25 async_llm_engine.py:173] Added request cmpl-e35fc1ec528f4f25ba59cea5e3c458c3-0.
INFO 01-31 12:48:25 logger.py:36] Received request cmpl-4e8511e434f94359b624aae5af16e89e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:25 async_llm_engine.py:173] Added request cmpl-4e8511e434f94359b624aae5af16e89e-0.
INFO 01-31 12:48:25 logger.py:36] Received request cmpl-971b8c95834740b09410465d5dfc5995-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:25 async_llm_engine.py:173] Added request cmpl-971b8c95834740b09410465d5dfc5995-0.
INFO 01-31 12:48:25 logger.py:36] Received request cmpl-6f1266a254f84d88a4603a4fa6c77f7c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:25 logger.py:36] Received request cmpl-a48e06e3513e467e90b8508aad98dae4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:25 async_llm_engine.py:173] Added request cmpl-6f1266a254f84d88a4603a4fa6c77f7c-0.
INFO 01-31 12:48:25 async_llm_engine.py:173] Added request cmpl-a48e06e3513e467e90b8508aad98dae4-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:39922 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:39920 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:28 async_llm_engine.py:140] Finished request cmpl-b1c9678dad5244ec8c911dc64225a6d6-0.
INFO 01-31 12:48:28 async_llm_engine.py:140] Finished request cmpl-b814dfd5e34942c0b9d92374fb261dca-0.
INFO 01-31 12:48:28 async_llm_engine.py:140] Finished request cmpl-6089102c6fbb462d929a4fb484792b3b-0.
INFO 01-31 12:48:28 async_llm_engine.py:140] Finished request cmpl-6230da8880654941ab89d3f81f3427a7-0.
INFO 01-31 12:48:28 async_llm_engine.py:140] Finished request cmpl-f3bbf8a49f524f24bb169b8781b9c41f-0.
INFO 01-31 12:48:28 async_llm_engine.py:140] Finished request cmpl-e35fc1ec528f4f25ba59cea5e3c458c3-0.
INFO 01-31 12:48:28 async_llm_engine.py:140] Finished request cmpl-4e8511e434f94359b624aae5af16e89e-0.
INFO 01-31 12:48:28 async_llm_engine.py:140] Finished request cmpl-971b8c95834740b09410465d5dfc5995-0.
INFO 01-31 12:48:28 async_llm_engine.py:140] Finished request cmpl-6f1266a254f84d88a4603a4fa6c77f7c-0.
INFO 01-31 12:48:28 async_llm_engine.py:140] Finished request cmpl-a48e06e3513e467e90b8508aad98dae4-0.
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:28 logger.py:36] Received request cmpl-3ff4cd8db54c4501878f7e374f6a4465-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:28 async_llm_engine.py:173] Added request cmpl-3ff4cd8db54c4501878f7e374f6a4465-0.
INFO 01-31 12:48:28 logger.py:36] Received request cmpl-a65335494c18463abe286ef58f062704-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:28 async_llm_engine.py:173] Added request cmpl-a65335494c18463abe286ef58f062704-0.
INFO 01-31 12:48:28 logger.py:36] Received request cmpl-aa1412d85ca54323b947af921954793b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:28 async_llm_engine.py:173] Added request cmpl-aa1412d85ca54323b947af921954793b-0.
INFO 01-31 12:48:28 logger.py:36] Received request cmpl-561d4ec9b8964b2f96e57aabfd5c6d3c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:28 async_llm_engine.py:173] Added request cmpl-561d4ec9b8964b2f96e57aabfd5c6d3c-0.
INFO 01-31 12:48:28 logger.py:36] Received request cmpl-523f48ecc6984896b64ee5af343a094b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:28 async_llm_engine.py:173] Added request cmpl-523f48ecc6984896b64ee5af343a094b-0.
INFO 01-31 12:48:28 logger.py:36] Received request cmpl-b0c0a79b0b834c91b890890c403ca132-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:28 async_llm_engine.py:173] Added request cmpl-b0c0a79b0b834c91b890890c403ca132-0.
INFO 01-31 12:48:28 logger.py:36] Received request cmpl-fdd8e54dd7934cc1b06f9a3459456318-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:28 async_llm_engine.py:173] Added request cmpl-fdd8e54dd7934cc1b06f9a3459456318-0.
INFO 01-31 12:48:28 logger.py:36] Received request cmpl-ad71a2758b724bedb53a533bb05777c2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:28 logger.py:36] Received request cmpl-9b9414ad52f84cec94bfde9ea930776b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:28 async_llm_engine.py:173] Added request cmpl-ad71a2758b724bedb53a533bb05777c2-0.
INFO 01-31 12:48:28 async_llm_engine.py:173] Added request cmpl-9b9414ad52f84cec94bfde9ea930776b-0.
INFO 01-31 12:48:28 logger.py:36] Received request cmpl-0650fd588c6d498090b4101556e84647-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:28 async_llm_engine.py:173] Added request cmpl-0650fd588c6d498090b4101556e84647-0.
INFO 01-31 12:48:28 metrics.py:396] Avg prompt throughput: 35.8 tokens/s, Avg generation throughput: 210.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:31 async_llm_engine.py:140] Finished request cmpl-3ff4cd8db54c4501878f7e374f6a4465-0.
INFO 01-31 12:48:31 async_llm_engine.py:140] Finished request cmpl-a65335494c18463abe286ef58f062704-0.
INFO 01-31 12:48:31 async_llm_engine.py:140] Finished request cmpl-aa1412d85ca54323b947af921954793b-0.
INFO 01-31 12:48:31 async_llm_engine.py:140] Finished request cmpl-561d4ec9b8964b2f96e57aabfd5c6d3c-0.
INFO 01-31 12:48:31 async_llm_engine.py:140] Finished request cmpl-523f48ecc6984896b64ee5af343a094b-0.
INFO 01-31 12:48:31 async_llm_engine.py:140] Finished request cmpl-b0c0a79b0b834c91b890890c403ca132-0.
INFO 01-31 12:48:31 async_llm_engine.py:140] Finished request cmpl-fdd8e54dd7934cc1b06f9a3459456318-0.
INFO 01-31 12:48:31 async_llm_engine.py:140] Finished request cmpl-ad71a2758b724bedb53a533bb05777c2-0.
INFO 01-31 12:48:31 async_llm_engine.py:140] Finished request cmpl-9b9414ad52f84cec94bfde9ea930776b-0.
INFO 01-31 12:48:31 async_llm_engine.py:140] Finished request cmpl-0650fd588c6d498090b4101556e84647-0.
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:31 logger.py:36] Received request cmpl-ddfaff7153584869ad879b1ed0083ab9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:31 async_llm_engine.py:173] Added request cmpl-ddfaff7153584869ad879b1ed0083ab9-0.
INFO 01-31 12:48:31 logger.py:36] Received request cmpl-88a911530fc3421bbea577c703ac3d30-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:31 async_llm_engine.py:173] Added request cmpl-88a911530fc3421bbea577c703ac3d30-0.
INFO 01-31 12:48:31 logger.py:36] Received request cmpl-a31ff66a4cf04eca9e1844122e7396c9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:31 async_llm_engine.py:173] Added request cmpl-a31ff66a4cf04eca9e1844122e7396c9-0.
INFO 01-31 12:48:31 logger.py:36] Received request cmpl-97b4774e97ec4857ac80678b28321789-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:31 async_llm_engine.py:173] Added request cmpl-97b4774e97ec4857ac80678b28321789-0.
INFO 01-31 12:48:31 logger.py:36] Received request cmpl-b54823937b10461496f26995351a0244-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:31 async_llm_engine.py:173] Added request cmpl-b54823937b10461496f26995351a0244-0.
INFO 01-31 12:48:31 logger.py:36] Received request cmpl-c7dda0483cb94ffea4e616f7dc2ad60f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:31 async_llm_engine.py:173] Added request cmpl-c7dda0483cb94ffea4e616f7dc2ad60f-0.
INFO 01-31 12:48:31 logger.py:36] Received request cmpl-532b1e2a6d3d4c92b15bf62c225cb12f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:31 async_llm_engine.py:173] Added request cmpl-532b1e2a6d3d4c92b15bf62c225cb12f-0.
INFO 01-31 12:48:31 logger.py:36] Received request cmpl-22b973f062414f9eadd4e6443b8068ba-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:31 async_llm_engine.py:173] Added request cmpl-22b973f062414f9eadd4e6443b8068ba-0.
INFO 01-31 12:48:31 logger.py:36] Received request cmpl-7cb2a2dd8b31432284cbf60651c481aa-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:31 async_llm_engine.py:173] Added request cmpl-7cb2a2dd8b31432284cbf60651c481aa-0.
INFO 01-31 12:48:31 logger.py:36] Received request cmpl-43b6e4b3ccaf400aac611c6b07fddd31-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:31 async_llm_engine.py:173] Added request cmpl-43b6e4b3ccaf400aac611c6b07fddd31-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:33 metrics.py:396] Avg prompt throughput: 17.9 tokens/s, Avg generation throughput: 220.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 01-31 12:48:34 async_llm_engine.py:140] Finished request cmpl-ddfaff7153584869ad879b1ed0083ab9-0.
INFO 01-31 12:48:34 async_llm_engine.py:140] Finished request cmpl-88a911530fc3421bbea577c703ac3d30-0.
INFO 01-31 12:48:34 async_llm_engine.py:140] Finished request cmpl-a31ff66a4cf04eca9e1844122e7396c9-0.
INFO 01-31 12:48:34 async_llm_engine.py:140] Finished request cmpl-97b4774e97ec4857ac80678b28321789-0.
INFO 01-31 12:48:34 async_llm_engine.py:140] Finished request cmpl-b54823937b10461496f26995351a0244-0.
INFO 01-31 12:48:34 async_llm_engine.py:140] Finished request cmpl-c7dda0483cb94ffea4e616f7dc2ad60f-0.
INFO 01-31 12:48:34 async_llm_engine.py:140] Finished request cmpl-532b1e2a6d3d4c92b15bf62c225cb12f-0.
INFO 01-31 12:48:34 async_llm_engine.py:140] Finished request cmpl-22b973f062414f9eadd4e6443b8068ba-0.
INFO 01-31 12:48:34 async_llm_engine.py:140] Finished request cmpl-7cb2a2dd8b31432284cbf60651c481aa-0.
INFO 01-31 12:48:34 async_llm_engine.py:140] Finished request cmpl-43b6e4b3ccaf400aac611c6b07fddd31-0.
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:34 logger.py:36] Received request cmpl-ffa2e71f2b624eca9bd733349829f265-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:34 async_llm_engine.py:173] Added request cmpl-ffa2e71f2b624eca9bd733349829f265-0.
INFO 01-31 12:48:34 logger.py:36] Received request cmpl-e25238c486bb480aa9029792049a9a81-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:34 async_llm_engine.py:173] Added request cmpl-e25238c486bb480aa9029792049a9a81-0.
INFO 01-31 12:48:34 logger.py:36] Received request cmpl-417a80e4c7ad44709953157dbc0dad4f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:34 async_llm_engine.py:173] Added request cmpl-417a80e4c7ad44709953157dbc0dad4f-0.
INFO 01-31 12:48:34 logger.py:36] Received request cmpl-a48fff2e780e4c5787ac7f40db767933-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:34 logger.py:36] Received request cmpl-cd3cd23be46c41878530dfd387ef081c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:34 async_llm_engine.py:173] Added request cmpl-a48fff2e780e4c5787ac7f40db767933-0.
INFO 01-31 12:48:34 async_llm_engine.py:173] Added request cmpl-cd3cd23be46c41878530dfd387ef081c-0.
INFO 01-31 12:48:34 logger.py:36] Received request cmpl-83f19de95b0349638aeeb4cf2668ff48-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:34 async_llm_engine.py:173] Added request cmpl-83f19de95b0349638aeeb4cf2668ff48-0.
INFO 01-31 12:48:34 logger.py:36] Received request cmpl-5cab47c2249145658b8b0dfaf73a785c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:34 async_llm_engine.py:173] Added request cmpl-5cab47c2249145658b8b0dfaf73a785c-0.
INFO 01-31 12:48:34 logger.py:36] Received request cmpl-856112952e594ded821ad4207ff64605-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:34 async_llm_engine.py:173] Added request cmpl-856112952e594ded821ad4207ff64605-0.
INFO 01-31 12:48:34 logger.py:36] Received request cmpl-c543ab31cbec4eeb960f1e315b5b5d11-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:34 async_llm_engine.py:173] Added request cmpl-c543ab31cbec4eeb960f1e315b5b5d11-0.
INFO 01-31 12:48:34 logger.py:36] Received request cmpl-e9777b0362da4b1f8c27302f94f666b3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:34 async_llm_engine.py:173] Added request cmpl-e9777b0362da4b1f8c27302f94f666b3-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:44842 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:44848 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:37 async_llm_engine.py:140] Finished request cmpl-ffa2e71f2b624eca9bd733349829f265-0.
INFO 01-31 12:48:37 async_llm_engine.py:140] Finished request cmpl-e25238c486bb480aa9029792049a9a81-0.
INFO 01-31 12:48:37 async_llm_engine.py:140] Finished request cmpl-417a80e4c7ad44709953157dbc0dad4f-0.
INFO 01-31 12:48:37 async_llm_engine.py:140] Finished request cmpl-a48fff2e780e4c5787ac7f40db767933-0.
INFO 01-31 12:48:37 async_llm_engine.py:140] Finished request cmpl-cd3cd23be46c41878530dfd387ef081c-0.
INFO 01-31 12:48:37 async_llm_engine.py:140] Finished request cmpl-83f19de95b0349638aeeb4cf2668ff48-0.
INFO 01-31 12:48:37 async_llm_engine.py:140] Finished request cmpl-5cab47c2249145658b8b0dfaf73a785c-0.
INFO 01-31 12:48:37 async_llm_engine.py:140] Finished request cmpl-856112952e594ded821ad4207ff64605-0.
INFO 01-31 12:48:37 async_llm_engine.py:140] Finished request cmpl-c543ab31cbec4eeb960f1e315b5b5d11-0.
INFO 01-31 12:48:37 async_llm_engine.py:140] Finished request cmpl-e9777b0362da4b1f8c27302f94f666b3-0.
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:37 logger.py:36] Received request cmpl-db59e0b1106e4f5cbdb71b26e28590dc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:37 async_llm_engine.py:173] Added request cmpl-db59e0b1106e4f5cbdb71b26e28590dc-0.
INFO 01-31 12:48:37 logger.py:36] Received request cmpl-c85368cca990401faaed7ccc2159b286-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:37 async_llm_engine.py:173] Added request cmpl-c85368cca990401faaed7ccc2159b286-0.
INFO 01-31 12:48:37 logger.py:36] Received request cmpl-34c6ff32b0134c5fa86de20c399302fa-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:37 async_llm_engine.py:173] Added request cmpl-34c6ff32b0134c5fa86de20c399302fa-0.
INFO 01-31 12:48:37 logger.py:36] Received request cmpl-26f59612cd0142aba43b22f8469c1ec1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:37 async_llm_engine.py:173] Added request cmpl-26f59612cd0142aba43b22f8469c1ec1-0.
INFO 01-31 12:48:37 logger.py:36] Received request cmpl-534d91b83ecf42a7a5f1632c7322250c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:37 async_llm_engine.py:173] Added request cmpl-534d91b83ecf42a7a5f1632c7322250c-0.
INFO 01-31 12:48:37 logger.py:36] Received request cmpl-0baeb40479904eda97ca6b9439985c54-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:37 async_llm_engine.py:173] Added request cmpl-0baeb40479904eda97ca6b9439985c54-0.
INFO 01-31 12:48:37 logger.py:36] Received request cmpl-d2e3eeed909f44e29adac0d192ab5ada-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:37 async_llm_engine.py:173] Added request cmpl-d2e3eeed909f44e29adac0d192ab5ada-0.
INFO 01-31 12:48:37 logger.py:36] Received request cmpl-b72b9e431a0b45ec8aa40e1263dc3788-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:37 async_llm_engine.py:173] Added request cmpl-b72b9e431a0b45ec8aa40e1263dc3788-0.
INFO 01-31 12:48:37 logger.py:36] Received request cmpl-04923bdc1b094883b6467e3466838e4a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:37 async_llm_engine.py:173] Added request cmpl-04923bdc1b094883b6467e3466838e4a-0.
INFO 01-31 12:48:37 logger.py:36] Received request cmpl-9431c9f6c06444c6877457611c65694c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:37 async_llm_engine.py:173] Added request cmpl-9431c9f6c06444c6877457611c65694c-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:38 metrics.py:396] Avg prompt throughput: 35.9 tokens/s, Avg generation throughput: 205.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:40 async_llm_engine.py:140] Finished request cmpl-db59e0b1106e4f5cbdb71b26e28590dc-0.
INFO 01-31 12:48:40 async_llm_engine.py:140] Finished request cmpl-c85368cca990401faaed7ccc2159b286-0.
INFO 01-31 12:48:40 async_llm_engine.py:140] Finished request cmpl-34c6ff32b0134c5fa86de20c399302fa-0.
INFO 01-31 12:48:40 async_llm_engine.py:140] Finished request cmpl-26f59612cd0142aba43b22f8469c1ec1-0.
INFO 01-31 12:48:40 async_llm_engine.py:140] Finished request cmpl-534d91b83ecf42a7a5f1632c7322250c-0.
INFO 01-31 12:48:40 async_llm_engine.py:140] Finished request cmpl-0baeb40479904eda97ca6b9439985c54-0.
INFO 01-31 12:48:40 async_llm_engine.py:140] Finished request cmpl-d2e3eeed909f44e29adac0d192ab5ada-0.
INFO 01-31 12:48:40 async_llm_engine.py:140] Finished request cmpl-b72b9e431a0b45ec8aa40e1263dc3788-0.
INFO 01-31 12:48:40 async_llm_engine.py:140] Finished request cmpl-04923bdc1b094883b6467e3466838e4a-0.
INFO 01-31 12:48:40 async_llm_engine.py:140] Finished request cmpl-9431c9f6c06444c6877457611c65694c-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:40 logger.py:36] Received request cmpl-a05276c16fb940098a922fefb67ea1fb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:40 async_llm_engine.py:173] Added request cmpl-a05276c16fb940098a922fefb67ea1fb-0.
INFO 01-31 12:48:40 logger.py:36] Received request cmpl-320823275d744165ad82986c190357e2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:40 async_llm_engine.py:173] Added request cmpl-320823275d744165ad82986c190357e2-0.
INFO 01-31 12:48:40 logger.py:36] Received request cmpl-0faf9cc1e0214372be4bb6db025f35ff-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:40 logger.py:36] Received request cmpl-935f77374294469297db2fd2cdce04c7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:40 async_llm_engine.py:173] Added request cmpl-0faf9cc1e0214372be4bb6db025f35ff-0.
INFO 01-31 12:48:40 async_llm_engine.py:173] Added request cmpl-935f77374294469297db2fd2cdce04c7-0.
INFO 01-31 12:48:40 logger.py:36] Received request cmpl-35e660e0010e4870bd64427855a29839-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:40 async_llm_engine.py:173] Added request cmpl-35e660e0010e4870bd64427855a29839-0.
INFO 01-31 12:48:40 logger.py:36] Received request cmpl-8c4e53c67d6849cbaff1ad96f3cbad93-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:40 async_llm_engine.py:173] Added request cmpl-8c4e53c67d6849cbaff1ad96f3cbad93-0.
INFO 01-31 12:48:40 logger.py:36] Received request cmpl-48f4b38c27fc48baa2effce0d38dee5a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:40 async_llm_engine.py:173] Added request cmpl-48f4b38c27fc48baa2effce0d38dee5a-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:40 logger.py:36] Received request cmpl-0753f0017b4140d5975f46314b341824-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:40 async_llm_engine.py:173] Added request cmpl-0753f0017b4140d5975f46314b341824-0.
INFO 01-31 12:48:40 logger.py:36] Received request cmpl-445233c7e08b44d394157805c5f19465-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:40 logger.py:36] Received request cmpl-ecefe37356c449d0aa280bcb2130e204-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:40 async_llm_engine.py:173] Added request cmpl-445233c7e08b44d394157805c5f19465-0.
INFO 01-31 12:48:40 async_llm_engine.py:173] Added request cmpl-ecefe37356c449d0aa280bcb2130e204-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:43 async_llm_engine.py:140] Finished request cmpl-a05276c16fb940098a922fefb67ea1fb-0.
INFO 01-31 12:48:43 async_llm_engine.py:140] Finished request cmpl-320823275d744165ad82986c190357e2-0.
INFO 01-31 12:48:43 async_llm_engine.py:140] Finished request cmpl-0faf9cc1e0214372be4bb6db025f35ff-0.
INFO 01-31 12:48:43 async_llm_engine.py:140] Finished request cmpl-935f77374294469297db2fd2cdce04c7-0.
INFO 01-31 12:48:43 async_llm_engine.py:140] Finished request cmpl-35e660e0010e4870bd64427855a29839-0.
INFO 01-31 12:48:43 async_llm_engine.py:140] Finished request cmpl-8c4e53c67d6849cbaff1ad96f3cbad93-0.
INFO 01-31 12:48:43 async_llm_engine.py:140] Finished request cmpl-48f4b38c27fc48baa2effce0d38dee5a-0.
INFO 01-31 12:48:43 async_llm_engine.py:140] Finished request cmpl-0753f0017b4140d5975f46314b341824-0.
INFO 01-31 12:48:43 async_llm_engine.py:140] Finished request cmpl-445233c7e08b44d394157805c5f19465-0.
INFO 01-31 12:48:43 async_llm_engine.py:140] Finished request cmpl-ecefe37356c449d0aa280bcb2130e204-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:43 logger.py:36] Received request cmpl-9d268ff6669d42339b97dd8c32b6cc1f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:43 async_llm_engine.py:173] Added request cmpl-9d268ff6669d42339b97dd8c32b6cc1f-0.
INFO 01-31 12:48:43 logger.py:36] Received request cmpl-99aff9ddbd0e4a1a8dac6681677b70b2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:43 logger.py:36] Received request cmpl-126faf4ba83242b7b992e3ec91e8ba42-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:43 async_llm_engine.py:173] Added request cmpl-99aff9ddbd0e4a1a8dac6681677b70b2-0.
INFO 01-31 12:48:43 async_llm_engine.py:173] Added request cmpl-126faf4ba83242b7b992e3ec91e8ba42-0.
INFO 01-31 12:48:43 logger.py:36] Received request cmpl-9ff2539a95af4965ad6c1a4adbfbade4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:43 async_llm_engine.py:173] Added request cmpl-9ff2539a95af4965ad6c1a4adbfbade4-0.
INFO 01-31 12:48:43 logger.py:36] Received request cmpl-f739f2227776416795de2beeafc53355-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:43 async_llm_engine.py:173] Added request cmpl-f739f2227776416795de2beeafc53355-0.
INFO 01-31 12:48:43 logger.py:36] Received request cmpl-c0b131b812d344439647655a3c8e99f3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:43 async_llm_engine.py:173] Added request cmpl-c0b131b812d344439647655a3c8e99f3-0.
INFO 01-31 12:48:43 logger.py:36] Received request cmpl-2e974cda7f1340b09b8ae5c1e0b81ffe-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:43 async_llm_engine.py:173] Added request cmpl-2e974cda7f1340b09b8ae5c1e0b81ffe-0.
INFO 01-31 12:48:43 logger.py:36] Received request cmpl-7ec70564883c4ffd86ace8df4ed6684d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:43 async_llm_engine.py:173] Added request cmpl-7ec70564883c4ffd86ace8df4ed6684d-0.
INFO 01-31 12:48:43 logger.py:36] Received request cmpl-c730dc622b5a4720b1b2315a1e09a060-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:43 async_llm_engine.py:173] Added request cmpl-c730dc622b5a4720b1b2315a1e09a060-0.
INFO 01-31 12:48:43 logger.py:36] Received request cmpl-552ae3647cda4ecc943445fadbbe09df-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:43 async_llm_engine.py:173] Added request cmpl-552ae3647cda4ecc943445fadbbe09df-0.
INFO 01-31 12:48:43 metrics.py:396] Avg prompt throughput: 35.7 tokens/s, Avg generation throughput: 210.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:33128 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:33140 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:46 async_llm_engine.py:140] Finished request cmpl-9d268ff6669d42339b97dd8c32b6cc1f-0.
INFO 01-31 12:48:46 async_llm_engine.py:140] Finished request cmpl-99aff9ddbd0e4a1a8dac6681677b70b2-0.
INFO 01-31 12:48:46 async_llm_engine.py:140] Finished request cmpl-126faf4ba83242b7b992e3ec91e8ba42-0.
INFO 01-31 12:48:46 async_llm_engine.py:140] Finished request cmpl-9ff2539a95af4965ad6c1a4adbfbade4-0.
INFO 01-31 12:48:46 async_llm_engine.py:140] Finished request cmpl-f739f2227776416795de2beeafc53355-0.
INFO 01-31 12:48:46 async_llm_engine.py:140] Finished request cmpl-c0b131b812d344439647655a3c8e99f3-0.
INFO 01-31 12:48:46 async_llm_engine.py:140] Finished request cmpl-2e974cda7f1340b09b8ae5c1e0b81ffe-0.
INFO 01-31 12:48:46 async_llm_engine.py:140] Finished request cmpl-7ec70564883c4ffd86ace8df4ed6684d-0.
INFO 01-31 12:48:46 async_llm_engine.py:140] Finished request cmpl-c730dc622b5a4720b1b2315a1e09a060-0.
INFO 01-31 12:48:46 async_llm_engine.py:140] Finished request cmpl-552ae3647cda4ecc943445fadbbe09df-0.
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:46 logger.py:36] Received request cmpl-a8f56d54a3a64098b0138eee46454ffe-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:46 async_llm_engine.py:173] Added request cmpl-a8f56d54a3a64098b0138eee46454ffe-0.
INFO 01-31 12:48:46 logger.py:36] Received request cmpl-a91f891f6ae14fe98ce311b4c4120d04-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:46 async_llm_engine.py:173] Added request cmpl-a91f891f6ae14fe98ce311b4c4120d04-0.
INFO 01-31 12:48:46 logger.py:36] Received request cmpl-cf7878becdeb41deb751ca652686ecc8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:46 async_llm_engine.py:173] Added request cmpl-cf7878becdeb41deb751ca652686ecc8-0.
INFO 01-31 12:48:46 logger.py:36] Received request cmpl-fd01210716e64dfda3469b50d6c20ffb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:46 async_llm_engine.py:173] Added request cmpl-fd01210716e64dfda3469b50d6c20ffb-0.
INFO 01-31 12:48:46 logger.py:36] Received request cmpl-d6efc4aa71984beabaf0781288f8e69c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:46 async_llm_engine.py:173] Added request cmpl-d6efc4aa71984beabaf0781288f8e69c-0.
INFO 01-31 12:48:46 logger.py:36] Received request cmpl-002cbbeccbf04ca784dea07218fd91a5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:46 async_llm_engine.py:173] Added request cmpl-002cbbeccbf04ca784dea07218fd91a5-0.
INFO 01-31 12:48:46 logger.py:36] Received request cmpl-cdb1adcb39e14ffe905e57c7b1db6934-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:46 async_llm_engine.py:173] Added request cmpl-cdb1adcb39e14ffe905e57c7b1db6934-0.
INFO 01-31 12:48:46 logger.py:36] Received request cmpl-9f960bf94adf45898f77a1bfb7b0d92b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:46 logger.py:36] Received request cmpl-0e1943c4fcf74a84835795f14ab9f58b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:46 async_llm_engine.py:173] Added request cmpl-9f960bf94adf45898f77a1bfb7b0d92b-0.
INFO 01-31 12:48:46 async_llm_engine.py:173] Added request cmpl-0e1943c4fcf74a84835795f14ab9f58b-0.
INFO 01-31 12:48:46 logger.py:36] Received request cmpl-56d08eaa13de4c21a81948d14fd951dc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:46 async_llm_engine.py:173] Added request cmpl-56d08eaa13de4c21a81948d14fd951dc-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:48 metrics.py:396] Avg prompt throughput: 17.9 tokens/s, Avg generation throughput: 210.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 01-31 12:48:49 async_llm_engine.py:140] Finished request cmpl-a8f56d54a3a64098b0138eee46454ffe-0.
INFO 01-31 12:48:49 async_llm_engine.py:140] Finished request cmpl-a91f891f6ae14fe98ce311b4c4120d04-0.
INFO 01-31 12:48:49 async_llm_engine.py:140] Finished request cmpl-cf7878becdeb41deb751ca652686ecc8-0.
INFO 01-31 12:48:49 async_llm_engine.py:140] Finished request cmpl-fd01210716e64dfda3469b50d6c20ffb-0.
INFO 01-31 12:48:49 async_llm_engine.py:140] Finished request cmpl-d6efc4aa71984beabaf0781288f8e69c-0.
INFO 01-31 12:48:49 async_llm_engine.py:140] Finished request cmpl-002cbbeccbf04ca784dea07218fd91a5-0.
INFO 01-31 12:48:49 async_llm_engine.py:140] Finished request cmpl-cdb1adcb39e14ffe905e57c7b1db6934-0.
INFO 01-31 12:48:49 async_llm_engine.py:140] Finished request cmpl-9f960bf94adf45898f77a1bfb7b0d92b-0.
INFO 01-31 12:48:49 async_llm_engine.py:140] Finished request cmpl-0e1943c4fcf74a84835795f14ab9f58b-0.
INFO 01-31 12:48:49 async_llm_engine.py:140] Finished request cmpl-56d08eaa13de4c21a81948d14fd951dc-0.
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:49 logger.py:36] Received request cmpl-b1e495e040e347e68a61b89843f6cda7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:49 async_llm_engine.py:173] Added request cmpl-b1e495e040e347e68a61b89843f6cda7-0.
INFO 01-31 12:48:49 logger.py:36] Received request cmpl-3156267cdf2a4feebf81944a676d3b21-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO:     192.168.200.241:56840 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:49 async_llm_engine.py:173] Added request cmpl-3156267cdf2a4feebf81944a676d3b21-0.
INFO 01-31 12:48:49 logger.py:36] Received request cmpl-a57d1727c8f146afb1e777f85de1eb9d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:49 async_llm_engine.py:173] Added request cmpl-a57d1727c8f146afb1e777f85de1eb9d-0.
INFO 01-31 12:48:49 logger.py:36] Received request cmpl-1a3b7dea7d494933a1e2fa5e7f317253-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:49 async_llm_engine.py:173] Added request cmpl-1a3b7dea7d494933a1e2fa5e7f317253-0.
INFO 01-31 12:48:49 logger.py:36] Received request cmpl-f4077e2be97044f69e9a8ebfc28862fe-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:49 logger.py:36] Received request cmpl-f9e1b93c81964ae68958569fffbb6f93-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:49 async_llm_engine.py:173] Added request cmpl-f4077e2be97044f69e9a8ebfc28862fe-0.
INFO 01-31 12:48:49 async_llm_engine.py:173] Added request cmpl-f9e1b93c81964ae68958569fffbb6f93-0.
INFO 01-31 12:48:49 logger.py:36] Received request cmpl-05ce2ce937164b53afca9e1f56a0de58-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:49 async_llm_engine.py:173] Added request cmpl-05ce2ce937164b53afca9e1f56a0de58-0.
INFO 01-31 12:48:49 logger.py:36] Received request cmpl-03bc5b11d4524326971f6268acf569b8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:49 async_llm_engine.py:173] Added request cmpl-03bc5b11d4524326971f6268acf569b8-0.
INFO 01-31 12:48:49 logger.py:36] Received request cmpl-4c1688bc5e574d82abd114d1d5ec7021-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:49 async_llm_engine.py:173] Added request cmpl-4c1688bc5e574d82abd114d1d5ec7021-0.
INFO 01-31 12:48:49 logger.py:36] Received request cmpl-425c3c692fa14f78a69a52e5c6c28c32-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:49 async_llm_engine.py:173] Added request cmpl-425c3c692fa14f78a69a52e5c6c28c32-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:52 async_llm_engine.py:140] Finished request cmpl-b1e495e040e347e68a61b89843f6cda7-0.
INFO 01-31 12:48:52 async_llm_engine.py:140] Finished request cmpl-3156267cdf2a4feebf81944a676d3b21-0.
INFO 01-31 12:48:52 async_llm_engine.py:140] Finished request cmpl-a57d1727c8f146afb1e777f85de1eb9d-0.
INFO 01-31 12:48:52 async_llm_engine.py:140] Finished request cmpl-1a3b7dea7d494933a1e2fa5e7f317253-0.
INFO 01-31 12:48:52 async_llm_engine.py:140] Finished request cmpl-f4077e2be97044f69e9a8ebfc28862fe-0.
INFO 01-31 12:48:52 async_llm_engine.py:140] Finished request cmpl-f9e1b93c81964ae68958569fffbb6f93-0.
INFO 01-31 12:48:52 async_llm_engine.py:140] Finished request cmpl-05ce2ce937164b53afca9e1f56a0de58-0.
INFO 01-31 12:48:52 async_llm_engine.py:140] Finished request cmpl-03bc5b11d4524326971f6268acf569b8-0.
INFO 01-31 12:48:52 async_llm_engine.py:140] Finished request cmpl-4c1688bc5e574d82abd114d1d5ec7021-0.
INFO 01-31 12:48:52 async_llm_engine.py:140] Finished request cmpl-425c3c692fa14f78a69a52e5c6c28c32-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:52 logger.py:36] Received request cmpl-505e5b241a0f4371948ee415a86571bd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:52 async_llm_engine.py:173] Added request cmpl-505e5b241a0f4371948ee415a86571bd-0.
INFO 01-31 12:48:52 logger.py:36] Received request cmpl-4c357303999d4bc499e8ce700f43e358-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:52 async_llm_engine.py:173] Added request cmpl-4c357303999d4bc499e8ce700f43e358-0.
INFO 01-31 12:48:52 logger.py:36] Received request cmpl-287ef2b79559496abf4bae4489543064-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:52 async_llm_engine.py:173] Added request cmpl-287ef2b79559496abf4bae4489543064-0.
INFO 01-31 12:48:52 logger.py:36] Received request cmpl-90eec27e2caa4ac4a26bf69fa350e986-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:52 async_llm_engine.py:173] Added request cmpl-90eec27e2caa4ac4a26bf69fa350e986-0.
INFO 01-31 12:48:52 logger.py:36] Received request cmpl-fdcfea362edc4fc4abe43a050cee2cf0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:52 async_llm_engine.py:173] Added request cmpl-fdcfea362edc4fc4abe43a050cee2cf0-0.
INFO 01-31 12:48:52 logger.py:36] Received request cmpl-150c227299524249b59f0ac0d21a607f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:52 async_llm_engine.py:173] Added request cmpl-150c227299524249b59f0ac0d21a607f-0.
INFO 01-31 12:48:52 logger.py:36] Received request cmpl-7c5ad9e5178f488495b76871a268036c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:52 async_llm_engine.py:173] Added request cmpl-7c5ad9e5178f488495b76871a268036c-0.
INFO 01-31 12:48:52 logger.py:36] Received request cmpl-888872ae5aca4ea6be5d02b5f9ec0fc6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:52 async_llm_engine.py:173] Added request cmpl-888872ae5aca4ea6be5d02b5f9ec0fc6-0.
INFO 01-31 12:48:52 logger.py:36] Received request cmpl-65383cb6078e4b529e411cc8316f8515-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:52 async_llm_engine.py:173] Added request cmpl-65383cb6078e4b529e411cc8316f8515-0.
INFO 01-31 12:48:52 logger.py:36] Received request cmpl-7073df8882884f8e82d341ca8a924c3b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:52 async_llm_engine.py:173] Added request cmpl-7073df8882884f8e82d341ca8a924c3b-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:53 metrics.py:396] Avg prompt throughput: 35.9 tokens/s, Avg generation throughput: 211.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:55 async_llm_engine.py:140] Finished request cmpl-505e5b241a0f4371948ee415a86571bd-0.
INFO 01-31 12:48:55 async_llm_engine.py:140] Finished request cmpl-4c357303999d4bc499e8ce700f43e358-0.
INFO 01-31 12:48:55 async_llm_engine.py:140] Finished request cmpl-287ef2b79559496abf4bae4489543064-0.
INFO 01-31 12:48:55 async_llm_engine.py:140] Finished request cmpl-90eec27e2caa4ac4a26bf69fa350e986-0.
INFO 01-31 12:48:55 async_llm_engine.py:140] Finished request cmpl-fdcfea362edc4fc4abe43a050cee2cf0-0.
INFO 01-31 12:48:55 async_llm_engine.py:140] Finished request cmpl-150c227299524249b59f0ac0d21a607f-0.
INFO 01-31 12:48:55 async_llm_engine.py:140] Finished request cmpl-7c5ad9e5178f488495b76871a268036c-0.
INFO 01-31 12:48:55 async_llm_engine.py:140] Finished request cmpl-888872ae5aca4ea6be5d02b5f9ec0fc6-0.
INFO 01-31 12:48:55 async_llm_engine.py:140] Finished request cmpl-65383cb6078e4b529e411cc8316f8515-0.
INFO 01-31 12:48:55 async_llm_engine.py:140] Finished request cmpl-7073df8882884f8e82d341ca8a924c3b-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:55 logger.py:36] Received request cmpl-3faf72fe1de442008956b9dfb30f8478-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:55 logger.py:36] Received request cmpl-d1c244ee4d3647659758803165cd7e8c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:55 async_llm_engine.py:173] Added request cmpl-3faf72fe1de442008956b9dfb30f8478-0.
INFO 01-31 12:48:55 async_llm_engine.py:173] Added request cmpl-d1c244ee4d3647659758803165cd7e8c-0.
INFO 01-31 12:48:55 logger.py:36] Received request cmpl-4d7684a592e641d5990fba76325df78b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:55 async_llm_engine.py:173] Added request cmpl-4d7684a592e641d5990fba76325df78b-0.
INFO 01-31 12:48:55 logger.py:36] Received request cmpl-0ac52bce66ac4ac7bbc411b9e3cf52e8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:55 async_llm_engine.py:173] Added request cmpl-0ac52bce66ac4ac7bbc411b9e3cf52e8-0.
INFO 01-31 12:48:55 logger.py:36] Received request cmpl-aad6915e673e4e36a330852dbc71af5d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:55 async_llm_engine.py:173] Added request cmpl-aad6915e673e4e36a330852dbc71af5d-0.
INFO 01-31 12:48:55 logger.py:36] Received request cmpl-2137582ace5843e08a3cb5dee6e5c106-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:55 async_llm_engine.py:173] Added request cmpl-2137582ace5843e08a3cb5dee6e5c106-0.
INFO 01-31 12:48:55 logger.py:36] Received request cmpl-8eec581b87ed45cd96f3c9c80b29837a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:55 async_llm_engine.py:173] Added request cmpl-8eec581b87ed45cd96f3c9c80b29837a-0.
INFO 01-31 12:48:55 logger.py:36] Received request cmpl-c99d0cdc95674f308035d23942143516-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:55 async_llm_engine.py:173] Added request cmpl-c99d0cdc95674f308035d23942143516-0.
INFO:     89.105.200.105:50804 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:50818 - "GET /health HTTP/1.1" 200 OK
INFO 01-31 12:48:55 logger.py:36] Received request cmpl-9fa2567d5f734645af0afbb60016804e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:55 async_llm_engine.py:173] Added request cmpl-9fa2567d5f734645af0afbb60016804e-0.
INFO 01-31 12:48:55 logger.py:36] Received request cmpl-06656ff1bc0e4ef998795c5ca4a52708-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:55 async_llm_engine.py:173] Added request cmpl-06656ff1bc0e4ef998795c5ca4a52708-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:48:58 async_llm_engine.py:140] Finished request cmpl-3faf72fe1de442008956b9dfb30f8478-0.
INFO 01-31 12:48:58 async_llm_engine.py:140] Finished request cmpl-d1c244ee4d3647659758803165cd7e8c-0.
INFO 01-31 12:48:58 async_llm_engine.py:140] Finished request cmpl-4d7684a592e641d5990fba76325df78b-0.
INFO 01-31 12:48:58 async_llm_engine.py:140] Finished request cmpl-0ac52bce66ac4ac7bbc411b9e3cf52e8-0.
INFO 01-31 12:48:58 async_llm_engine.py:140] Finished request cmpl-aad6915e673e4e36a330852dbc71af5d-0.
INFO 01-31 12:48:58 async_llm_engine.py:140] Finished request cmpl-2137582ace5843e08a3cb5dee6e5c106-0.
INFO 01-31 12:48:58 async_llm_engine.py:140] Finished request cmpl-8eec581b87ed45cd96f3c9c80b29837a-0.
INFO 01-31 12:48:58 async_llm_engine.py:140] Finished request cmpl-c99d0cdc95674f308035d23942143516-0.
INFO 01-31 12:48:58 async_llm_engine.py:140] Finished request cmpl-9fa2567d5f734645af0afbb60016804e-0.
INFO 01-31 12:48:58 async_llm_engine.py:140] Finished request cmpl-06656ff1bc0e4ef998795c5ca4a52708-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:48:58 logger.py:36] Received request cmpl-66da860e1147458b8ca1c2eb6fe55cb4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:58 async_llm_engine.py:173] Added request cmpl-66da860e1147458b8ca1c2eb6fe55cb4-0.
INFO 01-31 12:48:58 logger.py:36] Received request cmpl-a0d7f10f715d453ab6505ce83ec99e03-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:58 async_llm_engine.py:173] Added request cmpl-a0d7f10f715d453ab6505ce83ec99e03-0.
INFO 01-31 12:48:58 logger.py:36] Received request cmpl-1cbaae4a5c204c12b225adada548882e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:58 async_llm_engine.py:173] Added request cmpl-1cbaae4a5c204c12b225adada548882e-0.
INFO 01-31 12:48:58 logger.py:36] Received request cmpl-5f0e5c5622ab4ebe888301cb9874b5e5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:58 logger.py:36] Received request cmpl-8b45e3654393495697a3e2a03b3f0781-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:58 async_llm_engine.py:173] Added request cmpl-5f0e5c5622ab4ebe888301cb9874b5e5-0.
INFO 01-31 12:48:58 async_llm_engine.py:173] Added request cmpl-8b45e3654393495697a3e2a03b3f0781-0.
INFO 01-31 12:48:58 logger.py:36] Received request cmpl-6524ded950df4f53afacf3fbfdab7a04-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:58 logger.py:36] Received request cmpl-2e2f5cc74cbc4be2a79fd603eb80c97c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:58 async_llm_engine.py:173] Added request cmpl-6524ded950df4f53afacf3fbfdab7a04-0.
INFO 01-31 12:48:58 async_llm_engine.py:173] Added request cmpl-2e2f5cc74cbc4be2a79fd603eb80c97c-0.
INFO 01-31 12:48:58 logger.py:36] Received request cmpl-878a00c327bb4b0fbbbb51d7f4b65a4f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:58 async_llm_engine.py:173] Added request cmpl-878a00c327bb4b0fbbbb51d7f4b65a4f-0.
INFO 01-31 12:48:58 logger.py:36] Received request cmpl-3a14a2efef854175b0c039459e7b8456-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:58 async_llm_engine.py:173] Added request cmpl-3a14a2efef854175b0c039459e7b8456-0.
INFO 01-31 12:48:58 logger.py:36] Received request cmpl-dc2df244bddc4b1c922041577bb2aa54-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:48:58 async_llm_engine.py:173] Added request cmpl-dc2df244bddc4b1c922041577bb2aa54-0.
INFO 01-31 12:48:58 metrics.py:396] Avg prompt throughput: 35.8 tokens/s, Avg generation throughput: 210.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:01 async_llm_engine.py:140] Finished request cmpl-66da860e1147458b8ca1c2eb6fe55cb4-0.
INFO 01-31 12:49:01 async_llm_engine.py:140] Finished request cmpl-a0d7f10f715d453ab6505ce83ec99e03-0.
INFO 01-31 12:49:01 async_llm_engine.py:140] Finished request cmpl-1cbaae4a5c204c12b225adada548882e-0.
INFO 01-31 12:49:01 async_llm_engine.py:140] Finished request cmpl-5f0e5c5622ab4ebe888301cb9874b5e5-0.
INFO 01-31 12:49:01 async_llm_engine.py:140] Finished request cmpl-8b45e3654393495697a3e2a03b3f0781-0.
INFO 01-31 12:49:01 async_llm_engine.py:140] Finished request cmpl-6524ded950df4f53afacf3fbfdab7a04-0.
INFO 01-31 12:49:01 async_llm_engine.py:140] Finished request cmpl-2e2f5cc74cbc4be2a79fd603eb80c97c-0.
INFO 01-31 12:49:01 async_llm_engine.py:140] Finished request cmpl-878a00c327bb4b0fbbbb51d7f4b65a4f-0.
INFO 01-31 12:49:01 async_llm_engine.py:140] Finished request cmpl-3a14a2efef854175b0c039459e7b8456-0.
INFO 01-31 12:49:01 async_llm_engine.py:140] Finished request cmpl-dc2df244bddc4b1c922041577bb2aa54-0.
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:01 logger.py:36] Received request cmpl-8680531fa2f04e56ba944493515901fe-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:01 async_llm_engine.py:173] Added request cmpl-8680531fa2f04e56ba944493515901fe-0.
INFO 01-31 12:49:01 logger.py:36] Received request cmpl-e80ecd429de94b288f27d93f31695247-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:01 logger.py:36] Received request cmpl-824215c07df244ebb257908aa9a309d7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:01 async_llm_engine.py:173] Added request cmpl-e80ecd429de94b288f27d93f31695247-0.
INFO 01-31 12:49:01 async_llm_engine.py:173] Added request cmpl-824215c07df244ebb257908aa9a309d7-0.
INFO 01-31 12:49:01 logger.py:36] Received request cmpl-1b5b52f8fc974cb489b23a648d85c0c3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:01 async_llm_engine.py:173] Added request cmpl-1b5b52f8fc974cb489b23a648d85c0c3-0.
INFO 01-31 12:49:01 logger.py:36] Received request cmpl-670d5ddc84214687872afdcb458dc56b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:01 logger.py:36] Received request cmpl-452653f4b41947d5941bd8f0ce7bda9c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:01 async_llm_engine.py:173] Added request cmpl-670d5ddc84214687872afdcb458dc56b-0.
INFO 01-31 12:49:01 async_llm_engine.py:173] Added request cmpl-452653f4b41947d5941bd8f0ce7bda9c-0.
INFO 01-31 12:49:01 logger.py:36] Received request cmpl-f21c0ce4e32742ffb89e0e9a9e50d6d0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:01 async_llm_engine.py:173] Added request cmpl-f21c0ce4e32742ffb89e0e9a9e50d6d0-0.
INFO 01-31 12:49:01 logger.py:36] Received request cmpl-b000334354494fed97989bca72f9dd63-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:01 async_llm_engine.py:173] Added request cmpl-b000334354494fed97989bca72f9dd63-0.
INFO 01-31 12:49:01 logger.py:36] Received request cmpl-9bcbe0eaae0f43a4b073ea8961c0b0fc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:01 async_llm_engine.py:173] Added request cmpl-9bcbe0eaae0f43a4b073ea8961c0b0fc-0.
INFO 01-31 12:49:01 logger.py:36] Received request cmpl-1dd8a59224784e329e166de0b4817a35-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:01 async_llm_engine.py:173] Added request cmpl-1dd8a59224784e329e166de0b4817a35-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:03 metrics.py:396] Avg prompt throughput: 17.8 tokens/s, Avg generation throughput: 220.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 01-31 12:49:04 async_llm_engine.py:140] Finished request cmpl-8680531fa2f04e56ba944493515901fe-0.
INFO 01-31 12:49:04 async_llm_engine.py:140] Finished request cmpl-e80ecd429de94b288f27d93f31695247-0.
INFO 01-31 12:49:04 async_llm_engine.py:140] Finished request cmpl-824215c07df244ebb257908aa9a309d7-0.
INFO 01-31 12:49:04 async_llm_engine.py:140] Finished request cmpl-1b5b52f8fc974cb489b23a648d85c0c3-0.
INFO 01-31 12:49:04 async_llm_engine.py:140] Finished request cmpl-670d5ddc84214687872afdcb458dc56b-0.
INFO 01-31 12:49:04 async_llm_engine.py:140] Finished request cmpl-452653f4b41947d5941bd8f0ce7bda9c-0.
INFO 01-31 12:49:04 async_llm_engine.py:140] Finished request cmpl-f21c0ce4e32742ffb89e0e9a9e50d6d0-0.
INFO 01-31 12:49:04 async_llm_engine.py:140] Finished request cmpl-b000334354494fed97989bca72f9dd63-0.
INFO 01-31 12:49:04 async_llm_engine.py:140] Finished request cmpl-9bcbe0eaae0f43a4b073ea8961c0b0fc-0.
INFO 01-31 12:49:04 async_llm_engine.py:140] Finished request cmpl-1dd8a59224784e329e166de0b4817a35-0.
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:04 logger.py:36] Received request cmpl-7c917968cc8c4d19bd8dc7ee715a0d9f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:04 async_llm_engine.py:173] Added request cmpl-7c917968cc8c4d19bd8dc7ee715a0d9f-0.
INFO 01-31 12:49:04 logger.py:36] Received request cmpl-601044049d7d47d6aee346a033d9f809-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:04 async_llm_engine.py:173] Added request cmpl-601044049d7d47d6aee346a033d9f809-0.
INFO 01-31 12:49:04 logger.py:36] Received request cmpl-17397cdae6cb4b8bb67c8720b48f5233-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:04 async_llm_engine.py:173] Added request cmpl-17397cdae6cb4b8bb67c8720b48f5233-0.
INFO 01-31 12:49:04 logger.py:36] Received request cmpl-b45c27cd3c464d48bd0e6f9367c8761f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:04 logger.py:36] Received request cmpl-193f133fe20f4b8b84cfbacfac70517d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:04 async_llm_engine.py:173] Added request cmpl-b45c27cd3c464d48bd0e6f9367c8761f-0.
INFO 01-31 12:49:04 async_llm_engine.py:173] Added request cmpl-193f133fe20f4b8b84cfbacfac70517d-0.
INFO 01-31 12:49:04 logger.py:36] Received request cmpl-e48b07ecb51641d7a49997aa065921a4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:04 logger.py:36] Received request cmpl-bff2f178ce054ab9899c9a3d49cfbc71-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:04 async_llm_engine.py:173] Added request cmpl-e48b07ecb51641d7a49997aa065921a4-0.
INFO 01-31 12:49:04 async_llm_engine.py:173] Added request cmpl-bff2f178ce054ab9899c9a3d49cfbc71-0.
INFO 01-31 12:49:04 logger.py:36] Received request cmpl-83eb68528947435790a9d59605b1af6a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:04 logger.py:36] Received request cmpl-56e6f8b4b2e6458b8eac6293dda52277-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:04 async_llm_engine.py:173] Added request cmpl-83eb68528947435790a9d59605b1af6a-0.
INFO 01-31 12:49:04 async_llm_engine.py:173] Added request cmpl-56e6f8b4b2e6458b8eac6293dda52277-0.
INFO 01-31 12:49:04 logger.py:36] Received request cmpl-211a7d0b58094f0db76c1d9c03d91301-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:04 async_llm_engine.py:173] Added request cmpl-211a7d0b58094f0db76c1d9c03d91301-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:34666 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:34674 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:07 async_llm_engine.py:140] Finished request cmpl-7c917968cc8c4d19bd8dc7ee715a0d9f-0.
INFO 01-31 12:49:07 async_llm_engine.py:140] Finished request cmpl-601044049d7d47d6aee346a033d9f809-0.
INFO 01-31 12:49:07 async_llm_engine.py:140] Finished request cmpl-17397cdae6cb4b8bb67c8720b48f5233-0.
INFO 01-31 12:49:07 async_llm_engine.py:140] Finished request cmpl-b45c27cd3c464d48bd0e6f9367c8761f-0.
INFO 01-31 12:49:07 async_llm_engine.py:140] Finished request cmpl-193f133fe20f4b8b84cfbacfac70517d-0.
INFO 01-31 12:49:07 async_llm_engine.py:140] Finished request cmpl-e48b07ecb51641d7a49997aa065921a4-0.
INFO 01-31 12:49:07 async_llm_engine.py:140] Finished request cmpl-bff2f178ce054ab9899c9a3d49cfbc71-0.
INFO 01-31 12:49:07 async_llm_engine.py:140] Finished request cmpl-83eb68528947435790a9d59605b1af6a-0.
INFO 01-31 12:49:07 async_llm_engine.py:140] Finished request cmpl-56e6f8b4b2e6458b8eac6293dda52277-0.
INFO 01-31 12:49:07 async_llm_engine.py:140] Finished request cmpl-211a7d0b58094f0db76c1d9c03d91301-0.
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:07 logger.py:36] Received request cmpl-8fe308e1285d4a6abc008766fdfd537f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:07 async_llm_engine.py:173] Added request cmpl-8fe308e1285d4a6abc008766fdfd537f-0.
INFO 01-31 12:49:07 logger.py:36] Received request cmpl-f0d35f6016f14a2e86147788bb838d92-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:07 async_llm_engine.py:173] Added request cmpl-f0d35f6016f14a2e86147788bb838d92-0.
INFO 01-31 12:49:07 logger.py:36] Received request cmpl-ff718fb6ee514593ad13e11987ed47c4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:07 async_llm_engine.py:173] Added request cmpl-ff718fb6ee514593ad13e11987ed47c4-0.
INFO 01-31 12:49:07 logger.py:36] Received request cmpl-cab61a1238eb461b86fba36d055b667d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:07 logger.py:36] Received request cmpl-14bb93c74ba145049e642c86cc8d9c15-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:07 async_llm_engine.py:173] Added request cmpl-cab61a1238eb461b86fba36d055b667d-0.
INFO 01-31 12:49:07 async_llm_engine.py:173] Added request cmpl-14bb93c74ba145049e642c86cc8d9c15-0.
INFO 01-31 12:49:07 logger.py:36] Received request cmpl-71727a95eb1c493b8d4f1ace981eb702-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:07 async_llm_engine.py:173] Added request cmpl-71727a95eb1c493b8d4f1ace981eb702-0.
INFO 01-31 12:49:07 logger.py:36] Received request cmpl-084913f185b84d399378937efe5d0d7e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:07 async_llm_engine.py:173] Added request cmpl-084913f185b84d399378937efe5d0d7e-0.
INFO 01-31 12:49:07 logger.py:36] Received request cmpl-887d828f00874bc3bcba6032f82a7330-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:07 async_llm_engine.py:173] Added request cmpl-887d828f00874bc3bcba6032f82a7330-0.
INFO 01-31 12:49:07 logger.py:36] Received request cmpl-fcc320e98024414fb46d55b214edb84d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:07 async_llm_engine.py:173] Added request cmpl-fcc320e98024414fb46d55b214edb84d-0.
INFO 01-31 12:49:07 logger.py:36] Received request cmpl-2ba3fc03a32048e098f34b15cf053e6f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:07 async_llm_engine.py:173] Added request cmpl-2ba3fc03a32048e098f34b15cf053e6f-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:08 metrics.py:396] Avg prompt throughput: 35.9 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:10 async_llm_engine.py:140] Finished request cmpl-8fe308e1285d4a6abc008766fdfd537f-0.
INFO 01-31 12:49:10 async_llm_engine.py:140] Finished request cmpl-f0d35f6016f14a2e86147788bb838d92-0.
INFO 01-31 12:49:10 async_llm_engine.py:140] Finished request cmpl-ff718fb6ee514593ad13e11987ed47c4-0.
INFO 01-31 12:49:10 async_llm_engine.py:140] Finished request cmpl-cab61a1238eb461b86fba36d055b667d-0.
INFO 01-31 12:49:10 async_llm_engine.py:140] Finished request cmpl-14bb93c74ba145049e642c86cc8d9c15-0.
INFO 01-31 12:49:10 async_llm_engine.py:140] Finished request cmpl-71727a95eb1c493b8d4f1ace981eb702-0.
INFO 01-31 12:49:10 async_llm_engine.py:140] Finished request cmpl-084913f185b84d399378937efe5d0d7e-0.
INFO 01-31 12:49:10 async_llm_engine.py:140] Finished request cmpl-887d828f00874bc3bcba6032f82a7330-0.
INFO 01-31 12:49:10 async_llm_engine.py:140] Finished request cmpl-fcc320e98024414fb46d55b214edb84d-0.
INFO 01-31 12:49:10 async_llm_engine.py:140] Finished request cmpl-2ba3fc03a32048e098f34b15cf053e6f-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:10 logger.py:36] Received request cmpl-c97f4af3b057474d8c40571777334e86-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:10 async_llm_engine.py:173] Added request cmpl-c97f4af3b057474d8c40571777334e86-0.
INFO 01-31 12:49:10 logger.py:36] Received request cmpl-54c71662b1554a1a9db9a171aa5f88ba-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:10 async_llm_engine.py:173] Added request cmpl-54c71662b1554a1a9db9a171aa5f88ba-0.
INFO 01-31 12:49:10 logger.py:36] Received request cmpl-760776382ae44380aa6e8461425deceb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:10 async_llm_engine.py:173] Added request cmpl-760776382ae44380aa6e8461425deceb-0.
INFO 01-31 12:49:10 logger.py:36] Received request cmpl-eac5c902bef948b58acd13a6047cf98b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:10 async_llm_engine.py:173] Added request cmpl-eac5c902bef948b58acd13a6047cf98b-0.
INFO 01-31 12:49:10 logger.py:36] Received request cmpl-3b004593aaf346b0814e0da4202775a2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:10 async_llm_engine.py:173] Added request cmpl-3b004593aaf346b0814e0da4202775a2-0.
INFO 01-31 12:49:10 logger.py:36] Received request cmpl-78297fd1435245c6a579d73108001394-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:10 async_llm_engine.py:173] Added request cmpl-78297fd1435245c6a579d73108001394-0.
INFO 01-31 12:49:10 logger.py:36] Received request cmpl-716bb04d24944f7993afb25b83fe6b2a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:10 async_llm_engine.py:173] Added request cmpl-716bb04d24944f7993afb25b83fe6b2a-0.
INFO 01-31 12:49:10 logger.py:36] Received request cmpl-5cb14a88ada04d5995e032b03e878f33-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:10 async_llm_engine.py:173] Added request cmpl-5cb14a88ada04d5995e032b03e878f33-0.
INFO 01-31 12:49:10 logger.py:36] Received request cmpl-a87cb0f8664e4cb38288930857a42f89-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:10 logger.py:36] Received request cmpl-4188742249b140d7b88543d1ecd8bd62-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:10 async_llm_engine.py:173] Added request cmpl-a87cb0f8664e4cb38288930857a42f89-0.
INFO 01-31 12:49:10 async_llm_engine.py:173] Added request cmpl-4188742249b140d7b88543d1ecd8bd62-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:13 async_llm_engine.py:140] Finished request cmpl-c97f4af3b057474d8c40571777334e86-0.
INFO 01-31 12:49:13 async_llm_engine.py:140] Finished request cmpl-54c71662b1554a1a9db9a171aa5f88ba-0.
INFO 01-31 12:49:13 async_llm_engine.py:140] Finished request cmpl-760776382ae44380aa6e8461425deceb-0.
INFO 01-31 12:49:13 async_llm_engine.py:140] Finished request cmpl-eac5c902bef948b58acd13a6047cf98b-0.
INFO 01-31 12:49:13 async_llm_engine.py:140] Finished request cmpl-3b004593aaf346b0814e0da4202775a2-0.
INFO 01-31 12:49:13 async_llm_engine.py:140] Finished request cmpl-78297fd1435245c6a579d73108001394-0.
INFO 01-31 12:49:13 async_llm_engine.py:140] Finished request cmpl-716bb04d24944f7993afb25b83fe6b2a-0.
INFO 01-31 12:49:13 async_llm_engine.py:140] Finished request cmpl-5cb14a88ada04d5995e032b03e878f33-0.
INFO 01-31 12:49:13 async_llm_engine.py:140] Finished request cmpl-a87cb0f8664e4cb38288930857a42f89-0.
INFO 01-31 12:49:13 async_llm_engine.py:140] Finished request cmpl-4188742249b140d7b88543d1ecd8bd62-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:13 logger.py:36] Received request cmpl-f0cd9a160c6045548b8be46b4532f42e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:13 async_llm_engine.py:173] Added request cmpl-f0cd9a160c6045548b8be46b4532f42e-0.
INFO 01-31 12:49:13 logger.py:36] Received request cmpl-ff30b926bfe74cdebad51cce90a5b8cf-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:13 async_llm_engine.py:173] Added request cmpl-ff30b926bfe74cdebad51cce90a5b8cf-0.
INFO 01-31 12:49:13 logger.py:36] Received request cmpl-5371cc0a08aa4606b832e4a0e7fb0020-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:13 logger.py:36] Received request cmpl-36c712ba201d45a7a65a6997a481a091-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:13 async_llm_engine.py:173] Added request cmpl-5371cc0a08aa4606b832e4a0e7fb0020-0.
INFO 01-31 12:49:13 async_llm_engine.py:173] Added request cmpl-36c712ba201d45a7a65a6997a481a091-0.
INFO 01-31 12:49:13 logger.py:36] Received request cmpl-a7e085efc53f48a6b757821aac806747-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:13 async_llm_engine.py:173] Added request cmpl-a7e085efc53f48a6b757821aac806747-0.
INFO 01-31 12:49:13 logger.py:36] Received request cmpl-45d8205a6b8c4cbcb5c3090a7f011de1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:13 async_llm_engine.py:173] Added request cmpl-45d8205a6b8c4cbcb5c3090a7f011de1-0.
INFO 01-31 12:49:13 logger.py:36] Received request cmpl-0068888cdfdd446a90e059da304ccd8e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:13 async_llm_engine.py:173] Added request cmpl-0068888cdfdd446a90e059da304ccd8e-0.
INFO 01-31 12:49:13 logger.py:36] Received request cmpl-44bb0859a26c4c0daaec985d96039163-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:13 async_llm_engine.py:173] Added request cmpl-44bb0859a26c4c0daaec985d96039163-0.
INFO 01-31 12:49:13 logger.py:36] Received request cmpl-7b70342ee0604af1aa3183b1b7a6b2f8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:13 async_llm_engine.py:173] Added request cmpl-7b70342ee0604af1aa3183b1b7a6b2f8-0.
INFO 01-31 12:49:13 logger.py:36] Received request cmpl-8a1a41f1e4994383bce961fab55a0981-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:13 async_llm_engine.py:173] Added request cmpl-8a1a41f1e4994383bce961fab55a0981-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:13 metrics.py:396] Avg prompt throughput: 35.9 tokens/s, Avg generation throughput: 211.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:35326 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:35334 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:16 async_llm_engine.py:140] Finished request cmpl-f0cd9a160c6045548b8be46b4532f42e-0.
INFO 01-31 12:49:16 async_llm_engine.py:140] Finished request cmpl-ff30b926bfe74cdebad51cce90a5b8cf-0.
INFO 01-31 12:49:16 async_llm_engine.py:140] Finished request cmpl-5371cc0a08aa4606b832e4a0e7fb0020-0.
INFO 01-31 12:49:16 async_llm_engine.py:140] Finished request cmpl-36c712ba201d45a7a65a6997a481a091-0.
INFO 01-31 12:49:16 async_llm_engine.py:140] Finished request cmpl-a7e085efc53f48a6b757821aac806747-0.
INFO 01-31 12:49:16 async_llm_engine.py:140] Finished request cmpl-45d8205a6b8c4cbcb5c3090a7f011de1-0.
INFO 01-31 12:49:16 async_llm_engine.py:140] Finished request cmpl-0068888cdfdd446a90e059da304ccd8e-0.
INFO 01-31 12:49:16 async_llm_engine.py:140] Finished request cmpl-44bb0859a26c4c0daaec985d96039163-0.
INFO 01-31 12:49:16 async_llm_engine.py:140] Finished request cmpl-7b70342ee0604af1aa3183b1b7a6b2f8-0.
INFO 01-31 12:49:16 async_llm_engine.py:140] Finished request cmpl-8a1a41f1e4994383bce961fab55a0981-0.
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:16 logger.py:36] Received request cmpl-cf1f1f1f71384f78837e94abc7d48708-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:16 async_llm_engine.py:173] Added request cmpl-cf1f1f1f71384f78837e94abc7d48708-0.
INFO 01-31 12:49:16 logger.py:36] Received request cmpl-b3258c2edb114762aecb0cdf79fe8272-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:16 async_llm_engine.py:173] Added request cmpl-b3258c2edb114762aecb0cdf79fe8272-0.
INFO 01-31 12:49:16 logger.py:36] Received request cmpl-5ef893dac599482a81733bc1e1d40635-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:16 async_llm_engine.py:173] Added request cmpl-5ef893dac599482a81733bc1e1d40635-0.
INFO 01-31 12:49:16 logger.py:36] Received request cmpl-e8e9b895468c40f7bfb69499fd3d748c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:16 async_llm_engine.py:173] Added request cmpl-e8e9b895468c40f7bfb69499fd3d748c-0.
INFO 01-31 12:49:16 logger.py:36] Received request cmpl-345fb574dd8841d9b138e7c123b0949e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:16 async_llm_engine.py:173] Added request cmpl-345fb574dd8841d9b138e7c123b0949e-0.
INFO 01-31 12:49:16 logger.py:36] Received request cmpl-a94ef29447fb437abd53f0549a41e327-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:16 async_llm_engine.py:173] Added request cmpl-a94ef29447fb437abd53f0549a41e327-0.
INFO 01-31 12:49:16 logger.py:36] Received request cmpl-d09f429d07054aedabe059069bf8e2e8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:16 async_llm_engine.py:173] Added request cmpl-d09f429d07054aedabe059069bf8e2e8-0.
INFO 01-31 12:49:16 logger.py:36] Received request cmpl-df81e96f163a43ec98af6205d59a3fc9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:16 async_llm_engine.py:173] Added request cmpl-df81e96f163a43ec98af6205d59a3fc9-0.
INFO 01-31 12:49:16 logger.py:36] Received request cmpl-4a0fc628f0ab47f698a126ad1a28b794-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:16 async_llm_engine.py:173] Added request cmpl-4a0fc628f0ab47f698a126ad1a28b794-0.
INFO 01-31 12:49:16 logger.py:36] Received request cmpl-c0e4e14dac494ba19128c24576b52233-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:16 async_llm_engine.py:173] Added request cmpl-c0e4e14dac494ba19128c24576b52233-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:18 metrics.py:396] Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 219.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:19 async_llm_engine.py:140] Finished request cmpl-cf1f1f1f71384f78837e94abc7d48708-0.
INFO 01-31 12:49:19 async_llm_engine.py:140] Finished request cmpl-b3258c2edb114762aecb0cdf79fe8272-0.
INFO 01-31 12:49:19 async_llm_engine.py:140] Finished request cmpl-5ef893dac599482a81733bc1e1d40635-0.
INFO 01-31 12:49:19 async_llm_engine.py:140] Finished request cmpl-e8e9b895468c40f7bfb69499fd3d748c-0.
INFO 01-31 12:49:19 async_llm_engine.py:140] Finished request cmpl-345fb574dd8841d9b138e7c123b0949e-0.
INFO 01-31 12:49:19 async_llm_engine.py:140] Finished request cmpl-a94ef29447fb437abd53f0549a41e327-0.
INFO 01-31 12:49:19 async_llm_engine.py:140] Finished request cmpl-d09f429d07054aedabe059069bf8e2e8-0.
INFO 01-31 12:49:19 async_llm_engine.py:140] Finished request cmpl-df81e96f163a43ec98af6205d59a3fc9-0.
INFO 01-31 12:49:19 async_llm_engine.py:140] Finished request cmpl-4a0fc628f0ab47f698a126ad1a28b794-0.
INFO 01-31 12:49:19 async_llm_engine.py:140] Finished request cmpl-c0e4e14dac494ba19128c24576b52233-0.
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:19 logger.py:36] Received request cmpl-e761219d828449f88900d67f32c48951-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:19 async_llm_engine.py:173] Added request cmpl-e761219d828449f88900d67f32c48951-0.
INFO 01-31 12:49:19 logger.py:36] Received request cmpl-70e20496395546cabda4aa56900301cd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:19 async_llm_engine.py:173] Added request cmpl-70e20496395546cabda4aa56900301cd-0.
INFO 01-31 12:49:19 logger.py:36] Received request cmpl-747ef06d94024caeaad59dc1617f04a3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:19 logger.py:36] Received request cmpl-840ceb08ac0247ff8b481bfea7552b45-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:19 async_llm_engine.py:173] Added request cmpl-747ef06d94024caeaad59dc1617f04a3-0.
INFO 01-31 12:49:19 async_llm_engine.py:173] Added request cmpl-840ceb08ac0247ff8b481bfea7552b45-0.
INFO 01-31 12:49:19 logger.py:36] Received request cmpl-91cfcd01b5bd423bbcedc423b3ea2f16-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:19 async_llm_engine.py:173] Added request cmpl-91cfcd01b5bd423bbcedc423b3ea2f16-0.
INFO 01-31 12:49:19 logger.py:36] Received request cmpl-73b95ffb73f24a92af343d160f939807-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:19 async_llm_engine.py:173] Added request cmpl-73b95ffb73f24a92af343d160f939807-0.
INFO 01-31 12:49:19 logger.py:36] Received request cmpl-9b71fe48ba8b46fe83da329348cf91b8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:19 async_llm_engine.py:173] Added request cmpl-9b71fe48ba8b46fe83da329348cf91b8-0.
INFO 01-31 12:49:19 logger.py:36] Received request cmpl-c0632dee899542b09db3826cc6319b14-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:19 async_llm_engine.py:173] Added request cmpl-c0632dee899542b09db3826cc6319b14-0.
INFO 01-31 12:49:19 logger.py:36] Received request cmpl-664aa334523c4602bba85658b4ede124-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:19 async_llm_engine.py:173] Added request cmpl-664aa334523c4602bba85658b4ede124-0.
INFO 01-31 12:49:19 logger.py:36] Received request cmpl-4ad9ff127d344d0db4380b6ab10203ee-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:19 async_llm_engine.py:173] Added request cmpl-4ad9ff127d344d0db4380b6ab10203ee-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:22 async_llm_engine.py:140] Finished request cmpl-e761219d828449f88900d67f32c48951-0.
INFO 01-31 12:49:22 async_llm_engine.py:140] Finished request cmpl-70e20496395546cabda4aa56900301cd-0.
INFO 01-31 12:49:22 async_llm_engine.py:140] Finished request cmpl-747ef06d94024caeaad59dc1617f04a3-0.
INFO 01-31 12:49:22 async_llm_engine.py:140] Finished request cmpl-840ceb08ac0247ff8b481bfea7552b45-0.
INFO 01-31 12:49:22 async_llm_engine.py:140] Finished request cmpl-91cfcd01b5bd423bbcedc423b3ea2f16-0.
INFO 01-31 12:49:22 async_llm_engine.py:140] Finished request cmpl-73b95ffb73f24a92af343d160f939807-0.
INFO 01-31 12:49:22 async_llm_engine.py:140] Finished request cmpl-9b71fe48ba8b46fe83da329348cf91b8-0.
INFO 01-31 12:49:22 async_llm_engine.py:140] Finished request cmpl-c0632dee899542b09db3826cc6319b14-0.
INFO 01-31 12:49:22 async_llm_engine.py:140] Finished request cmpl-664aa334523c4602bba85658b4ede124-0.
INFO 01-31 12:49:22 async_llm_engine.py:140] Finished request cmpl-4ad9ff127d344d0db4380b6ab10203ee-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:22 logger.py:36] Received request cmpl-bff54266bae04065ae514bec1bbbd436-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:22 async_llm_engine.py:173] Added request cmpl-bff54266bae04065ae514bec1bbbd436-0.
INFO 01-31 12:49:22 logger.py:36] Received request cmpl-6a5e46ce5de542e5ba8df25afbd1e0ab-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:22 async_llm_engine.py:173] Added request cmpl-6a5e46ce5de542e5ba8df25afbd1e0ab-0.
INFO 01-31 12:49:22 logger.py:36] Received request cmpl-18db6daf5d8343c29d43693e9ea57621-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:22 logger.py:36] Received request cmpl-195ec59a1b8141d1bf9bd8e0615ad745-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:22 async_llm_engine.py:173] Added request cmpl-18db6daf5d8343c29d43693e9ea57621-0.
INFO 01-31 12:49:22 async_llm_engine.py:173] Added request cmpl-195ec59a1b8141d1bf9bd8e0615ad745-0.
INFO 01-31 12:49:22 logger.py:36] Received request cmpl-3094476ce8874206b5e301ba0f6462f1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:22 logger.py:36] Received request cmpl-8cc6d7f88053446db1e688e193cebc89-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:22 async_llm_engine.py:173] Added request cmpl-3094476ce8874206b5e301ba0f6462f1-0.
INFO 01-31 12:49:22 async_llm_engine.py:173] Added request cmpl-8cc6d7f88053446db1e688e193cebc89-0.
INFO 01-31 12:49:22 logger.py:36] Received request cmpl-ccec38c2553b4a77a3b68771aa8e51fc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:22 async_llm_engine.py:173] Added request cmpl-ccec38c2553b4a77a3b68771aa8e51fc-0.
INFO 01-31 12:49:22 logger.py:36] Received request cmpl-03d24e4aeefe451e825d77d5091ac8c4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:22 async_llm_engine.py:173] Added request cmpl-03d24e4aeefe451e825d77d5091ac8c4-0.
INFO 01-31 12:49:22 logger.py:36] Received request cmpl-022884edf0af4ff9803f2dfd1a9847ca-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:22 async_llm_engine.py:173] Added request cmpl-022884edf0af4ff9803f2dfd1a9847ca-0.
INFO 01-31 12:49:22 logger.py:36] Received request cmpl-f42a99521b30433cb2249cb137338536-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:22 async_llm_engine.py:173] Added request cmpl-f42a99521b30433cb2249cb137338536-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:23 metrics.py:396] Avg prompt throughput: 35.9 tokens/s, Avg generation throughput: 211.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:25 async_llm_engine.py:140] Finished request cmpl-bff54266bae04065ae514bec1bbbd436-0.
INFO 01-31 12:49:25 async_llm_engine.py:140] Finished request cmpl-6a5e46ce5de542e5ba8df25afbd1e0ab-0.
INFO 01-31 12:49:25 async_llm_engine.py:140] Finished request cmpl-18db6daf5d8343c29d43693e9ea57621-0.
INFO 01-31 12:49:25 async_llm_engine.py:140] Finished request cmpl-195ec59a1b8141d1bf9bd8e0615ad745-0.
INFO 01-31 12:49:25 async_llm_engine.py:140] Finished request cmpl-3094476ce8874206b5e301ba0f6462f1-0.
INFO 01-31 12:49:25 async_llm_engine.py:140] Finished request cmpl-8cc6d7f88053446db1e688e193cebc89-0.
INFO 01-31 12:49:25 async_llm_engine.py:140] Finished request cmpl-ccec38c2553b4a77a3b68771aa8e51fc-0.
INFO 01-31 12:49:25 async_llm_engine.py:140] Finished request cmpl-03d24e4aeefe451e825d77d5091ac8c4-0.
INFO 01-31 12:49:25 async_llm_engine.py:140] Finished request cmpl-022884edf0af4ff9803f2dfd1a9847ca-0.
INFO 01-31 12:49:25 async_llm_engine.py:140] Finished request cmpl-f42a99521b30433cb2249cb137338536-0.
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     89.105.200.105:39182 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:39184 - "GET /health HTTP/1.1" 200 OK
INFO 01-31 12:49:25 logger.py:36] Received request cmpl-4287db6b7edb4060b0de0e711cb042bf-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:25 async_llm_engine.py:173] Added request cmpl-4287db6b7edb4060b0de0e711cb042bf-0.
INFO 01-31 12:49:25 logger.py:36] Received request cmpl-847f126117a041b3b0c11de4bdd6850c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:25 logger.py:36] Received request cmpl-f10844f0aa8842939e8293efc520da07-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:25 async_llm_engine.py:173] Added request cmpl-847f126117a041b3b0c11de4bdd6850c-0.
INFO 01-31 12:49:25 async_llm_engine.py:173] Added request cmpl-f10844f0aa8842939e8293efc520da07-0.
INFO 01-31 12:49:25 logger.py:36] Received request cmpl-8ecae235d24443e293605fc1ca35e0c0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:25 async_llm_engine.py:173] Added request cmpl-8ecae235d24443e293605fc1ca35e0c0-0.
INFO 01-31 12:49:25 logger.py:36] Received request cmpl-3ea99dca9a5b4151b99b8f9cda8b6362-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:25 async_llm_engine.py:173] Added request cmpl-3ea99dca9a5b4151b99b8f9cda8b6362-0.
INFO 01-31 12:49:25 logger.py:36] Received request cmpl-13cf4ac018d9434cac8b2763d93fc277-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:25 async_llm_engine.py:173] Added request cmpl-13cf4ac018d9434cac8b2763d93fc277-0.
INFO 01-31 12:49:25 logger.py:36] Received request cmpl-e2a49072b19947fdb86efa2a0c14a725-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:25 async_llm_engine.py:173] Added request cmpl-e2a49072b19947fdb86efa2a0c14a725-0.
INFO 01-31 12:49:25 logger.py:36] Received request cmpl-ffdf0e949caf401cb5f8d01822eae854-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:25 async_llm_engine.py:173] Added request cmpl-ffdf0e949caf401cb5f8d01822eae854-0.
INFO 01-31 12:49:25 logger.py:36] Received request cmpl-9afc2b61d2f0491bb86985020433e3f0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:25 async_llm_engine.py:173] Added request cmpl-9afc2b61d2f0491bb86985020433e3f0-0.
INFO 01-31 12:49:25 logger.py:36] Received request cmpl-4eead1d9ff3f41b6ba46d7994617834e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:25 async_llm_engine.py:173] Added request cmpl-4eead1d9ff3f41b6ba46d7994617834e-0.
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:28 async_llm_engine.py:140] Finished request cmpl-4287db6b7edb4060b0de0e711cb042bf-0.
INFO 01-31 12:49:28 async_llm_engine.py:140] Finished request cmpl-847f126117a041b3b0c11de4bdd6850c-0.
INFO 01-31 12:49:28 async_llm_engine.py:140] Finished request cmpl-f10844f0aa8842939e8293efc520da07-0.
INFO 01-31 12:49:28 async_llm_engine.py:140] Finished request cmpl-8ecae235d24443e293605fc1ca35e0c0-0.
INFO 01-31 12:49:28 async_llm_engine.py:140] Finished request cmpl-3ea99dca9a5b4151b99b8f9cda8b6362-0.
INFO 01-31 12:49:28 async_llm_engine.py:140] Finished request cmpl-13cf4ac018d9434cac8b2763d93fc277-0.
INFO 01-31 12:49:28 async_llm_engine.py:140] Finished request cmpl-e2a49072b19947fdb86efa2a0c14a725-0.
INFO 01-31 12:49:28 async_llm_engine.py:140] Finished request cmpl-ffdf0e949caf401cb5f8d01822eae854-0.
INFO 01-31 12:49:28 async_llm_engine.py:140] Finished request cmpl-9afc2b61d2f0491bb86985020433e3f0-0.
INFO 01-31 12:49:28 async_llm_engine.py:140] Finished request cmpl-4eead1d9ff3f41b6ba46d7994617834e-0.
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:28 logger.py:36] Received request cmpl-892ea7f5b0c24742a9602cad522e6a26-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:28 async_llm_engine.py:173] Added request cmpl-892ea7f5b0c24742a9602cad522e6a26-0.
INFO 01-31 12:49:28 logger.py:36] Received request cmpl-19e2a743dc6d4fb0b6f2e93483b00bbb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:28 async_llm_engine.py:173] Added request cmpl-19e2a743dc6d4fb0b6f2e93483b00bbb-0.
INFO 01-31 12:49:28 logger.py:36] Received request cmpl-1b55ca70c5274b3bb2f3565b1bbcd5d2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:28 async_llm_engine.py:173] Added request cmpl-1b55ca70c5274b3bb2f3565b1bbcd5d2-0.
INFO 01-31 12:49:28 logger.py:36] Received request cmpl-b1eca9b414c7487686c90f0a7ed2580a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:28 async_llm_engine.py:173] Added request cmpl-b1eca9b414c7487686c90f0a7ed2580a-0.
INFO 01-31 12:49:28 logger.py:36] Received request cmpl-bc65b438cdef4045bb4b37e226d00a4e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:28 async_llm_engine.py:173] Added request cmpl-bc65b438cdef4045bb4b37e226d00a4e-0.
INFO 01-31 12:49:28 logger.py:36] Received request cmpl-9364461e6f5841508a5f7d48392f6089-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:28 async_llm_engine.py:173] Added request cmpl-9364461e6f5841508a5f7d48392f6089-0.
INFO 01-31 12:49:28 logger.py:36] Received request cmpl-c0fde1db19284d478674892942454833-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:28 async_llm_engine.py:173] Added request cmpl-c0fde1db19284d478674892942454833-0.
INFO 01-31 12:49:28 logger.py:36] Received request cmpl-eba28c02886a44f2a342ce983968aed0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:28 async_llm_engine.py:173] Added request cmpl-eba28c02886a44f2a342ce983968aed0-0.
INFO 01-31 12:49:28 logger.py:36] Received request cmpl-658884a3793349fd8697f740c518c13b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:28 async_llm_engine.py:173] Added request cmpl-658884a3793349fd8697f740c518c13b-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:28 logger.py:36] Received request cmpl-fe231054cab8484a9aec8c2a30479775-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:28 async_llm_engine.py:173] Added request cmpl-fe231054cab8484a9aec8c2a30479775-0.
INFO 01-31 12:49:28 metrics.py:396] Avg prompt throughput: 35.8 tokens/s, Avg generation throughput: 210.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:31 async_llm_engine.py:140] Finished request cmpl-892ea7f5b0c24742a9602cad522e6a26-0.
INFO 01-31 12:49:31 async_llm_engine.py:140] Finished request cmpl-19e2a743dc6d4fb0b6f2e93483b00bbb-0.
INFO 01-31 12:49:31 async_llm_engine.py:140] Finished request cmpl-1b55ca70c5274b3bb2f3565b1bbcd5d2-0.
INFO 01-31 12:49:31 async_llm_engine.py:140] Finished request cmpl-b1eca9b414c7487686c90f0a7ed2580a-0.
INFO 01-31 12:49:31 async_llm_engine.py:140] Finished request cmpl-bc65b438cdef4045bb4b37e226d00a4e-0.
INFO 01-31 12:49:31 async_llm_engine.py:140] Finished request cmpl-9364461e6f5841508a5f7d48392f6089-0.
INFO 01-31 12:49:31 async_llm_engine.py:140] Finished request cmpl-c0fde1db19284d478674892942454833-0.
INFO 01-31 12:49:31 async_llm_engine.py:140] Finished request cmpl-eba28c02886a44f2a342ce983968aed0-0.
INFO 01-31 12:49:31 async_llm_engine.py:140] Finished request cmpl-658884a3793349fd8697f740c518c13b-0.
INFO 01-31 12:49:31 async_llm_engine.py:140] Finished request cmpl-fe231054cab8484a9aec8c2a30479775-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:31 logger.py:36] Received request cmpl-577a91a094724c33b1b6c2b95dfff234-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:31 async_llm_engine.py:173] Added request cmpl-577a91a094724c33b1b6c2b95dfff234-0.
INFO 01-31 12:49:31 logger.py:36] Received request cmpl-8c4dd004030d4a16a9e2e968f659c4b8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:31 async_llm_engine.py:173] Added request cmpl-8c4dd004030d4a16a9e2e968f659c4b8-0.
INFO 01-31 12:49:31 logger.py:36] Received request cmpl-80ed85b4ca764bbc8c35a2bc945d8862-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:31 logger.py:36] Received request cmpl-e31f6d7f4e3741459d7cadf4d1049a1d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:31 async_llm_engine.py:173] Added request cmpl-80ed85b4ca764bbc8c35a2bc945d8862-0.
INFO 01-31 12:49:31 async_llm_engine.py:173] Added request cmpl-e31f6d7f4e3741459d7cadf4d1049a1d-0.
INFO 01-31 12:49:31 logger.py:36] Received request cmpl-a496042fce9d45fd8f185932bf69ffa1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:31 async_llm_engine.py:173] Added request cmpl-a496042fce9d45fd8f185932bf69ffa1-0.
INFO 01-31 12:49:31 logger.py:36] Received request cmpl-94526292077b47848409866f8f3ccbc3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:31 async_llm_engine.py:173] Added request cmpl-94526292077b47848409866f8f3ccbc3-0.
INFO 01-31 12:49:31 logger.py:36] Received request cmpl-81e3efd8310e4267b12e41ce8e25c506-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:31 async_llm_engine.py:173] Added request cmpl-81e3efd8310e4267b12e41ce8e25c506-0.
INFO 01-31 12:49:31 logger.py:36] Received request cmpl-9e188fe437ba4fb285b1908f5a1064ea-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:31 logger.py:36] Received request cmpl-aed726e11148470e970793f06d4aff7b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:31 logger.py:36] Received request cmpl-1641b384d0754fa6b964ee17dbc71d43-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:31 async_llm_engine.py:173] Added request cmpl-9e188fe437ba4fb285b1908f5a1064ea-0.
INFO 01-31 12:49:31 async_llm_engine.py:173] Added request cmpl-aed726e11148470e970793f06d4aff7b-0.
INFO 01-31 12:49:31 async_llm_engine.py:173] Added request cmpl-1641b384d0754fa6b964ee17dbc71d43-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:33 metrics.py:396] Avg prompt throughput: 17.9 tokens/s, Avg generation throughput: 218.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:34 async_llm_engine.py:140] Finished request cmpl-577a91a094724c33b1b6c2b95dfff234-0.
INFO 01-31 12:49:34 async_llm_engine.py:140] Finished request cmpl-8c4dd004030d4a16a9e2e968f659c4b8-0.
INFO 01-31 12:49:34 async_llm_engine.py:140] Finished request cmpl-80ed85b4ca764bbc8c35a2bc945d8862-0.
INFO 01-31 12:49:34 async_llm_engine.py:140] Finished request cmpl-e31f6d7f4e3741459d7cadf4d1049a1d-0.
INFO 01-31 12:49:34 async_llm_engine.py:140] Finished request cmpl-a496042fce9d45fd8f185932bf69ffa1-0.
INFO 01-31 12:49:34 async_llm_engine.py:140] Finished request cmpl-94526292077b47848409866f8f3ccbc3-0.
INFO 01-31 12:49:34 async_llm_engine.py:140] Finished request cmpl-81e3efd8310e4267b12e41ce8e25c506-0.
INFO 01-31 12:49:34 async_llm_engine.py:140] Finished request cmpl-9e188fe437ba4fb285b1908f5a1064ea-0.
INFO 01-31 12:49:34 async_llm_engine.py:140] Finished request cmpl-aed726e11148470e970793f06d4aff7b-0.
INFO 01-31 12:49:34 async_llm_engine.py:140] Finished request cmpl-1641b384d0754fa6b964ee17dbc71d43-0.
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:34 logger.py:36] Received request cmpl-9e1e46d0a9db4280b7efe1bdc4e4a7bd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:34 async_llm_engine.py:173] Added request cmpl-9e1e46d0a9db4280b7efe1bdc4e4a7bd-0.
INFO 01-31 12:49:34 logger.py:36] Received request cmpl-ea4d679028dd43bdb1f150bcac039b92-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:34 async_llm_engine.py:173] Added request cmpl-ea4d679028dd43bdb1f150bcac039b92-0.
INFO 01-31 12:49:34 logger.py:36] Received request cmpl-ccdee709ed5641a7ad40499a7b346aa9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:34 async_llm_engine.py:173] Added request cmpl-ccdee709ed5641a7ad40499a7b346aa9-0.
INFO 01-31 12:49:34 logger.py:36] Received request cmpl-fced0a1e6ff44c1a9197230c426343f9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:34 async_llm_engine.py:173] Added request cmpl-fced0a1e6ff44c1a9197230c426343f9-0.
INFO 01-31 12:49:34 logger.py:36] Received request cmpl-e589b86602f047559482882419ab9d80-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:34 async_llm_engine.py:173] Added request cmpl-e589b86602f047559482882419ab9d80-0.
INFO 01-31 12:49:34 logger.py:36] Received request cmpl-3a986e534eba4dc69b1ea0988a9c60fd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:34 async_llm_engine.py:173] Added request cmpl-3a986e534eba4dc69b1ea0988a9c60fd-0.
INFO 01-31 12:49:34 logger.py:36] Received request cmpl-1b859aa89aa0401984274017719c443f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:34 async_llm_engine.py:173] Added request cmpl-1b859aa89aa0401984274017719c443f-0.
INFO 01-31 12:49:34 logger.py:36] Received request cmpl-58d3b3b2088044d5b69579ebe878f8ba-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:34 async_llm_engine.py:173] Added request cmpl-58d3b3b2088044d5b69579ebe878f8ba-0.
INFO 01-31 12:49:34 logger.py:36] Received request cmpl-d685df65e7d74b60b1f07bcd641b3f0b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:34 async_llm_engine.py:173] Added request cmpl-d685df65e7d74b60b1f07bcd641b3f0b-0.
INFO 01-31 12:49:34 logger.py:36] Received request cmpl-844aebb377644f238c4709c79f6391f5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:34 async_llm_engine.py:173] Added request cmpl-844aebb377644f238c4709c79f6391f5-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:35950 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:35952 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:37 async_llm_engine.py:140] Finished request cmpl-9e1e46d0a9db4280b7efe1bdc4e4a7bd-0.
INFO 01-31 12:49:37 async_llm_engine.py:140] Finished request cmpl-ea4d679028dd43bdb1f150bcac039b92-0.
INFO 01-31 12:49:37 async_llm_engine.py:140] Finished request cmpl-ccdee709ed5641a7ad40499a7b346aa9-0.
INFO 01-31 12:49:37 async_llm_engine.py:140] Finished request cmpl-fced0a1e6ff44c1a9197230c426343f9-0.
INFO 01-31 12:49:37 async_llm_engine.py:140] Finished request cmpl-e589b86602f047559482882419ab9d80-0.
INFO 01-31 12:49:37 async_llm_engine.py:140] Finished request cmpl-3a986e534eba4dc69b1ea0988a9c60fd-0.
INFO 01-31 12:49:37 async_llm_engine.py:140] Finished request cmpl-1b859aa89aa0401984274017719c443f-0.
INFO 01-31 12:49:37 async_llm_engine.py:140] Finished request cmpl-58d3b3b2088044d5b69579ebe878f8ba-0.
INFO 01-31 12:49:37 async_llm_engine.py:140] Finished request cmpl-d685df65e7d74b60b1f07bcd641b3f0b-0.
INFO 01-31 12:49:37 async_llm_engine.py:140] Finished request cmpl-844aebb377644f238c4709c79f6391f5-0.
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:37 logger.py:36] Received request cmpl-02ff676b54c34ab890f0c00f1e7f7017-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:37 async_llm_engine.py:173] Added request cmpl-02ff676b54c34ab890f0c00f1e7f7017-0.
INFO 01-31 12:49:37 logger.py:36] Received request cmpl-4f1766aeab4341ca9991f6e82b2635dd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:37 async_llm_engine.py:173] Added request cmpl-4f1766aeab4341ca9991f6e82b2635dd-0.
INFO 01-31 12:49:37 logger.py:36] Received request cmpl-1b6824aaa017462fbae5863ede4f5f91-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:37 async_llm_engine.py:173] Added request cmpl-1b6824aaa017462fbae5863ede4f5f91-0.
INFO 01-31 12:49:37 logger.py:36] Received request cmpl-4f56b6c3bf52428693121098e1547e3b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:37 async_llm_engine.py:173] Added request cmpl-4f56b6c3bf52428693121098e1547e3b-0.
INFO 01-31 12:49:37 logger.py:36] Received request cmpl-e51eae53134d48ab8c615d300fae02db-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:37 async_llm_engine.py:173] Added request cmpl-e51eae53134d48ab8c615d300fae02db-0.
INFO 01-31 12:49:37 logger.py:36] Received request cmpl-fca2b98282d4461cab39de1ad8752dac-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:37 async_llm_engine.py:173] Added request cmpl-fca2b98282d4461cab39de1ad8752dac-0.
INFO 01-31 12:49:37 logger.py:36] Received request cmpl-c061dc5629fd437b9917ba48e383b5d9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:37 logger.py:36] Received request cmpl-0201bc4ea9b64739b3350e230ac2642b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:37 async_llm_engine.py:173] Added request cmpl-c061dc5629fd437b9917ba48e383b5d9-0.
INFO 01-31 12:49:37 async_llm_engine.py:173] Added request cmpl-0201bc4ea9b64739b3350e230ac2642b-0.
INFO 01-31 12:49:37 logger.py:36] Received request cmpl-dd5c2423c0f14595a0588abd1b0e7586-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:37 async_llm_engine.py:173] Added request cmpl-dd5c2423c0f14595a0588abd1b0e7586-0.
INFO 01-31 12:49:37 logger.py:36] Received request cmpl-c661718a5e864264a6694b38e9f1ad25-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:37 async_llm_engine.py:173] Added request cmpl-c661718a5e864264a6694b38e9f1ad25-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:38 metrics.py:396] Avg prompt throughput: 35.9 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:40 async_llm_engine.py:140] Finished request cmpl-02ff676b54c34ab890f0c00f1e7f7017-0.
INFO 01-31 12:49:40 async_llm_engine.py:140] Finished request cmpl-4f1766aeab4341ca9991f6e82b2635dd-0.
INFO 01-31 12:49:40 async_llm_engine.py:140] Finished request cmpl-1b6824aaa017462fbae5863ede4f5f91-0.
INFO 01-31 12:49:40 async_llm_engine.py:140] Finished request cmpl-4f56b6c3bf52428693121098e1547e3b-0.
INFO 01-31 12:49:40 async_llm_engine.py:140] Finished request cmpl-e51eae53134d48ab8c615d300fae02db-0.
INFO 01-31 12:49:40 async_llm_engine.py:140] Finished request cmpl-fca2b98282d4461cab39de1ad8752dac-0.
INFO 01-31 12:49:40 async_llm_engine.py:140] Finished request cmpl-c061dc5629fd437b9917ba48e383b5d9-0.
INFO 01-31 12:49:40 async_llm_engine.py:140] Finished request cmpl-0201bc4ea9b64739b3350e230ac2642b-0.
INFO 01-31 12:49:40 async_llm_engine.py:140] Finished request cmpl-dd5c2423c0f14595a0588abd1b0e7586-0.
INFO 01-31 12:49:40 async_llm_engine.py:140] Finished request cmpl-c661718a5e864264a6694b38e9f1ad25-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:40 logger.py:36] Received request cmpl-288bb10381b04640865401deadcf198a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:40 async_llm_engine.py:173] Added request cmpl-288bb10381b04640865401deadcf198a-0.
INFO 01-31 12:49:40 logger.py:36] Received request cmpl-a2418426dabd4021b4070104b43112c7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:40 async_llm_engine.py:173] Added request cmpl-a2418426dabd4021b4070104b43112c7-0.
INFO 01-31 12:49:40 logger.py:36] Received request cmpl-51b6fff05ac64546bb5210753f6bf21c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:40 async_llm_engine.py:173] Added request cmpl-51b6fff05ac64546bb5210753f6bf21c-0.
INFO 01-31 12:49:40 logger.py:36] Received request cmpl-0d4745db905242d4977d085666e1ad64-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:40 async_llm_engine.py:173] Added request cmpl-0d4745db905242d4977d085666e1ad64-0.
INFO 01-31 12:49:40 logger.py:36] Received request cmpl-40c37f13ed2a4910b5c613425f34835a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:40 logger.py:36] Received request cmpl-c462794b66ef4e44aee733248f3ca4c9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:40 async_llm_engine.py:173] Added request cmpl-40c37f13ed2a4910b5c613425f34835a-0.
INFO 01-31 12:49:40 async_llm_engine.py:173] Added request cmpl-c462794b66ef4e44aee733248f3ca4c9-0.
INFO 01-31 12:49:40 logger.py:36] Received request cmpl-9bbcf3d18b1b4558bb00175f376b0d75-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:40 async_llm_engine.py:173] Added request cmpl-9bbcf3d18b1b4558bb00175f376b0d75-0.
INFO 01-31 12:49:40 logger.py:36] Received request cmpl-e275833569a148d69d64cdda16d7aa46-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:40 async_llm_engine.py:173] Added request cmpl-e275833569a148d69d64cdda16d7aa46-0.
INFO 01-31 12:49:40 logger.py:36] Received request cmpl-51e8dbb4af4348bb9d933b95d42da0e6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:40 logger.py:36] Received request cmpl-4835627ea3964fcb809708600c2cdfcb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:40 async_llm_engine.py:173] Added request cmpl-51e8dbb4af4348bb9d933b95d42da0e6-0.
INFO 01-31 12:49:40 async_llm_engine.py:173] Added request cmpl-4835627ea3964fcb809708600c2cdfcb-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:43 async_llm_engine.py:140] Finished request cmpl-288bb10381b04640865401deadcf198a-0.
INFO 01-31 12:49:43 async_llm_engine.py:140] Finished request cmpl-a2418426dabd4021b4070104b43112c7-0.
INFO 01-31 12:49:43 async_llm_engine.py:140] Finished request cmpl-51b6fff05ac64546bb5210753f6bf21c-0.
INFO 01-31 12:49:43 async_llm_engine.py:140] Finished request cmpl-0d4745db905242d4977d085666e1ad64-0.
INFO 01-31 12:49:43 async_llm_engine.py:140] Finished request cmpl-40c37f13ed2a4910b5c613425f34835a-0.
INFO 01-31 12:49:43 async_llm_engine.py:140] Finished request cmpl-c462794b66ef4e44aee733248f3ca4c9-0.
INFO 01-31 12:49:43 async_llm_engine.py:140] Finished request cmpl-9bbcf3d18b1b4558bb00175f376b0d75-0.
INFO 01-31 12:49:43 async_llm_engine.py:140] Finished request cmpl-e275833569a148d69d64cdda16d7aa46-0.
INFO 01-31 12:49:43 async_llm_engine.py:140] Finished request cmpl-51e8dbb4af4348bb9d933b95d42da0e6-0.
INFO 01-31 12:49:43 async_llm_engine.py:140] Finished request cmpl-4835627ea3964fcb809708600c2cdfcb-0.
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:43 logger.py:36] Received request cmpl-8cf6bfbe92d2451da830a0e15630f7f5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:43 async_llm_engine.py:173] Added request cmpl-8cf6bfbe92d2451da830a0e15630f7f5-0.
INFO 01-31 12:49:43 logger.py:36] Received request cmpl-64d16a3549464da287298d72ae8d5791-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:43 logger.py:36] Received request cmpl-4ac42d580205414bb837efba6f189c12-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:43 async_llm_engine.py:173] Added request cmpl-64d16a3549464da287298d72ae8d5791-0.
INFO 01-31 12:49:43 async_llm_engine.py:173] Added request cmpl-4ac42d580205414bb837efba6f189c12-0.
INFO 01-31 12:49:43 logger.py:36] Received request cmpl-d49e943a5a434eb595030b26ba94aafd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:43 async_llm_engine.py:173] Added request cmpl-d49e943a5a434eb595030b26ba94aafd-0.
INFO 01-31 12:49:43 logger.py:36] Received request cmpl-6a7ff346bcc14842b57b84ff40aa2115-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:43 async_llm_engine.py:173] Added request cmpl-6a7ff346bcc14842b57b84ff40aa2115-0.
INFO 01-31 12:49:43 logger.py:36] Received request cmpl-21c28692fef4408f95dd71cb746c1971-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:43 async_llm_engine.py:173] Added request cmpl-21c28692fef4408f95dd71cb746c1971-0.
INFO 01-31 12:49:43 logger.py:36] Received request cmpl-8aa30ed9ad404e7e922ec48ceff4e1d4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:43 async_llm_engine.py:173] Added request cmpl-8aa30ed9ad404e7e922ec48ceff4e1d4-0.
INFO 01-31 12:49:43 logger.py:36] Received request cmpl-a0d3b24251954df68a5aac16a05d86e5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:43 async_llm_engine.py:173] Added request cmpl-a0d3b24251954df68a5aac16a05d86e5-0.
INFO 01-31 12:49:43 logger.py:36] Received request cmpl-bce3b02355e547ef9b101f3d32e950ff-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:43 logger.py:36] Received request cmpl-28ccc01a852d489e91660a8cd1fe0386-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:43 async_llm_engine.py:173] Added request cmpl-bce3b02355e547ef9b101f3d32e950ff-0.
INFO 01-31 12:49:43 async_llm_engine.py:173] Added request cmpl-28ccc01a852d489e91660a8cd1fe0386-0.
INFO 01-31 12:49:43 metrics.py:396] Avg prompt throughput: 23.2 tokens/s, Avg generation throughput: 204.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:41976 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:41986 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:46 async_llm_engine.py:140] Finished request cmpl-8cf6bfbe92d2451da830a0e15630f7f5-0.
INFO 01-31 12:49:46 async_llm_engine.py:140] Finished request cmpl-64d16a3549464da287298d72ae8d5791-0.
INFO 01-31 12:49:46 async_llm_engine.py:140] Finished request cmpl-4ac42d580205414bb837efba6f189c12-0.
INFO 01-31 12:49:46 async_llm_engine.py:140] Finished request cmpl-d49e943a5a434eb595030b26ba94aafd-0.
INFO 01-31 12:49:46 async_llm_engine.py:140] Finished request cmpl-6a7ff346bcc14842b57b84ff40aa2115-0.
INFO 01-31 12:49:46 async_llm_engine.py:140] Finished request cmpl-21c28692fef4408f95dd71cb746c1971-0.
INFO 01-31 12:49:46 async_llm_engine.py:140] Finished request cmpl-8aa30ed9ad404e7e922ec48ceff4e1d4-0.
INFO 01-31 12:49:46 async_llm_engine.py:140] Finished request cmpl-a0d3b24251954df68a5aac16a05d86e5-0.
INFO 01-31 12:49:46 async_llm_engine.py:140] Finished request cmpl-bce3b02355e547ef9b101f3d32e950ff-0.
INFO 01-31 12:49:46 async_llm_engine.py:140] Finished request cmpl-28ccc01a852d489e91660a8cd1fe0386-0.
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:46 logger.py:36] Received request cmpl-619d0305b2ac4557a8d3d07941163ea4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:46 async_llm_engine.py:173] Added request cmpl-619d0305b2ac4557a8d3d07941163ea4-0.
INFO 01-31 12:49:46 logger.py:36] Received request cmpl-42046713636e444eb6472a27f0aa365d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:46 async_llm_engine.py:173] Added request cmpl-42046713636e444eb6472a27f0aa365d-0.
INFO 01-31 12:49:46 logger.py:36] Received request cmpl-ce02fc8a59714036a68421ddc6cb69dd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:46 async_llm_engine.py:173] Added request cmpl-ce02fc8a59714036a68421ddc6cb69dd-0.
INFO 01-31 12:49:46 logger.py:36] Received request cmpl-7ca9d116a5a44e60aa56f8feb4768f29-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:46 logger.py:36] Received request cmpl-afb500bee4144d6789e0a10a1807d884-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:46 logger.py:36] Received request cmpl-d03df85ae1eb45d386afba9dbdb82eca-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:46 logger.py:36] Received request cmpl-6461b2ed46ca4a719556108ad4bbf233-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:46 async_llm_engine.py:173] Added request cmpl-7ca9d116a5a44e60aa56f8feb4768f29-0.
INFO 01-31 12:49:46 async_llm_engine.py:173] Added request cmpl-afb500bee4144d6789e0a10a1807d884-0.
INFO 01-31 12:49:46 async_llm_engine.py:173] Added request cmpl-d03df85ae1eb45d386afba9dbdb82eca-0.
INFO 01-31 12:49:46 async_llm_engine.py:173] Added request cmpl-6461b2ed46ca4a719556108ad4bbf233-0.
INFO 01-31 12:49:46 logger.py:36] Received request cmpl-e38cd23c6024426ba2d10ed9633e4900-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:46 async_llm_engine.py:173] Added request cmpl-e38cd23c6024426ba2d10ed9633e4900-0.
INFO 01-31 12:49:46 logger.py:36] Received request cmpl-2110be8790024bf68ea9a621b16930f8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:46 async_llm_engine.py:173] Added request cmpl-2110be8790024bf68ea9a621b16930f8-0.
INFO 01-31 12:49:46 logger.py:36] Received request cmpl-1d54f4b10873451e998dadb6eafb2d5e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:46 async_llm_engine.py:173] Added request cmpl-1d54f4b10873451e998dadb6eafb2d5e-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:48 metrics.py:396] Avg prompt throughput: 30.5 tokens/s, Avg generation throughput: 218.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:49 async_llm_engine.py:140] Finished request cmpl-619d0305b2ac4557a8d3d07941163ea4-0.
INFO 01-31 12:49:49 async_llm_engine.py:140] Finished request cmpl-42046713636e444eb6472a27f0aa365d-0.
INFO 01-31 12:49:49 async_llm_engine.py:140] Finished request cmpl-ce02fc8a59714036a68421ddc6cb69dd-0.
INFO 01-31 12:49:49 async_llm_engine.py:140] Finished request cmpl-7ca9d116a5a44e60aa56f8feb4768f29-0.
INFO 01-31 12:49:49 async_llm_engine.py:140] Finished request cmpl-afb500bee4144d6789e0a10a1807d884-0.
INFO 01-31 12:49:49 async_llm_engine.py:140] Finished request cmpl-d03df85ae1eb45d386afba9dbdb82eca-0.
INFO 01-31 12:49:49 async_llm_engine.py:140] Finished request cmpl-6461b2ed46ca4a719556108ad4bbf233-0.
INFO 01-31 12:49:49 async_llm_engine.py:140] Finished request cmpl-e38cd23c6024426ba2d10ed9633e4900-0.
INFO 01-31 12:49:49 async_llm_engine.py:140] Finished request cmpl-2110be8790024bf68ea9a621b16930f8-0.
INFO 01-31 12:49:49 async_llm_engine.py:140] Finished request cmpl-1d54f4b10873451e998dadb6eafb2d5e-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:49 logger.py:36] Received request cmpl-9cca9d42381240efb9781cb01705e91e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:49 async_llm_engine.py:173] Added request cmpl-9cca9d42381240efb9781cb01705e91e-0.
INFO 01-31 12:49:49 logger.py:36] Received request cmpl-e06b94d9c02246e1a9fe9cb1ce93ead7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:49 async_llm_engine.py:173] Added request cmpl-e06b94d9c02246e1a9fe9cb1ce93ead7-0.
INFO 01-31 12:49:49 logger.py:36] Received request cmpl-08fcb726b02a425c806f2cbf329aa0b2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:49 async_llm_engine.py:173] Added request cmpl-08fcb726b02a425c806f2cbf329aa0b2-0.
INFO 01-31 12:49:49 logger.py:36] Received request cmpl-f06eed11f7ff486a85ebe5ee07d8d296-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:49 logger.py:36] Received request cmpl-b28f882afdc648bbb3472050886036f0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:49 async_llm_engine.py:173] Added request cmpl-f06eed11f7ff486a85ebe5ee07d8d296-0.
INFO 01-31 12:49:49 async_llm_engine.py:173] Added request cmpl-b28f882afdc648bbb3472050886036f0-0.
INFO 01-31 12:49:49 logger.py:36] Received request cmpl-2ee35096b0a34f769353d09f66f1922f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:49 async_llm_engine.py:173] Added request cmpl-2ee35096b0a34f769353d09f66f1922f-0.
INFO 01-31 12:49:49 logger.py:36] Received request cmpl-d3b337fd4b2c4b6eb761343ad5ecbfb3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:49 async_llm_engine.py:173] Added request cmpl-d3b337fd4b2c4b6eb761343ad5ecbfb3-0.
INFO 01-31 12:49:49 logger.py:36] Received request cmpl-d899cc932a0147a7bc95362478fdbc03-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:49 async_llm_engine.py:173] Added request cmpl-d899cc932a0147a7bc95362478fdbc03-0.
INFO 01-31 12:49:49 logger.py:36] Received request cmpl-212689ea3782424da934ece270ded118-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:49 async_llm_engine.py:173] Added request cmpl-212689ea3782424da934ece270ded118-0.
INFO 01-31 12:49:49 logger.py:36] Received request cmpl-ad08ea023ed04fa5ac88949ea24c5b09-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:49 async_llm_engine.py:173] Added request cmpl-ad08ea023ed04fa5ac88949ea24c5b09-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:52 async_llm_engine.py:140] Finished request cmpl-9cca9d42381240efb9781cb01705e91e-0.
INFO 01-31 12:49:52 async_llm_engine.py:140] Finished request cmpl-e06b94d9c02246e1a9fe9cb1ce93ead7-0.
INFO 01-31 12:49:52 async_llm_engine.py:140] Finished request cmpl-08fcb726b02a425c806f2cbf329aa0b2-0.
INFO 01-31 12:49:52 async_llm_engine.py:140] Finished request cmpl-f06eed11f7ff486a85ebe5ee07d8d296-0.
INFO 01-31 12:49:52 async_llm_engine.py:140] Finished request cmpl-b28f882afdc648bbb3472050886036f0-0.
INFO 01-31 12:49:52 async_llm_engine.py:140] Finished request cmpl-2ee35096b0a34f769353d09f66f1922f-0.
INFO 01-31 12:49:52 async_llm_engine.py:140] Finished request cmpl-d3b337fd4b2c4b6eb761343ad5ecbfb3-0.
INFO 01-31 12:49:52 async_llm_engine.py:140] Finished request cmpl-d899cc932a0147a7bc95362478fdbc03-0.
INFO 01-31 12:49:52 async_llm_engine.py:140] Finished request cmpl-212689ea3782424da934ece270ded118-0.
INFO 01-31 12:49:52 async_llm_engine.py:140] Finished request cmpl-ad08ea023ed04fa5ac88949ea24c5b09-0.
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:52 logger.py:36] Received request cmpl-0fc3342040924caba0faac61ba6c9c00-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:52 async_llm_engine.py:173] Added request cmpl-0fc3342040924caba0faac61ba6c9c00-0.
INFO 01-31 12:49:52 logger.py:36] Received request cmpl-4d8209e85aaa42eb9675197c2829ff8b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:52 logger.py:36] Received request cmpl-4e977913d12d4ac8ae83edf0ebdc8f4a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:52 async_llm_engine.py:173] Added request cmpl-4d8209e85aaa42eb9675197c2829ff8b-0.
INFO 01-31 12:49:52 async_llm_engine.py:173] Added request cmpl-4e977913d12d4ac8ae83edf0ebdc8f4a-0.
INFO 01-31 12:49:52 logger.py:36] Received request cmpl-127454f958b54eb38788d34c7a270fce-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:52 async_llm_engine.py:173] Added request cmpl-127454f958b54eb38788d34c7a270fce-0.
INFO 01-31 12:49:52 logger.py:36] Received request cmpl-4bf4f8f93bcb4ac2823bc7e245a1640b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:52 logger.py:36] Received request cmpl-b44f22b0b0294b4fafe50a64426856b7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:52 async_llm_engine.py:173] Added request cmpl-4bf4f8f93bcb4ac2823bc7e245a1640b-0.
INFO 01-31 12:49:52 async_llm_engine.py:173] Added request cmpl-b44f22b0b0294b4fafe50a64426856b7-0.
INFO 01-31 12:49:52 logger.py:36] Received request cmpl-3c337ffc90aa48c6a9aae793d46f568c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:52 async_llm_engine.py:173] Added request cmpl-3c337ffc90aa48c6a9aae793d46f568c-0.
INFO 01-31 12:49:52 logger.py:36] Received request cmpl-550cc717fde84aaba0e8ed83ca0f2595-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:52 async_llm_engine.py:173] Added request cmpl-550cc717fde84aaba0e8ed83ca0f2595-0.
INFO 01-31 12:49:52 logger.py:36] Received request cmpl-dd13036a0f91422eaa6df90dc625e7c3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:52 async_llm_engine.py:173] Added request cmpl-dd13036a0f91422eaa6df90dc625e7c3-0.
INFO 01-31 12:49:52 logger.py:36] Received request cmpl-1eff02f92c474a63b05ba573aa7c77f8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:52 async_llm_engine.py:173] Added request cmpl-1eff02f92c474a63b05ba573aa7c77f8-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:53 metrics.py:396] Avg prompt throughput: 35.9 tokens/s, Avg generation throughput: 207.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:58128 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:58130 - "GET /health HTTP/1.1" 200 OK
INFO 01-31 12:49:55 async_llm_engine.py:140] Finished request cmpl-0fc3342040924caba0faac61ba6c9c00-0.
INFO 01-31 12:49:55 async_llm_engine.py:140] Finished request cmpl-4d8209e85aaa42eb9675197c2829ff8b-0.
INFO 01-31 12:49:55 async_llm_engine.py:140] Finished request cmpl-4e977913d12d4ac8ae83edf0ebdc8f4a-0.
INFO 01-31 12:49:55 async_llm_engine.py:140] Finished request cmpl-127454f958b54eb38788d34c7a270fce-0.
INFO 01-31 12:49:55 async_llm_engine.py:140] Finished request cmpl-4bf4f8f93bcb4ac2823bc7e245a1640b-0.
INFO 01-31 12:49:55 async_llm_engine.py:140] Finished request cmpl-b44f22b0b0294b4fafe50a64426856b7-0.
INFO 01-31 12:49:55 async_llm_engine.py:140] Finished request cmpl-3c337ffc90aa48c6a9aae793d46f568c-0.
INFO 01-31 12:49:55 async_llm_engine.py:140] Finished request cmpl-550cc717fde84aaba0e8ed83ca0f2595-0.
INFO 01-31 12:49:55 async_llm_engine.py:140] Finished request cmpl-dd13036a0f91422eaa6df90dc625e7c3-0.
INFO 01-31 12:49:55 async_llm_engine.py:140] Finished request cmpl-1eff02f92c474a63b05ba573aa7c77f8-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:56 logger.py:36] Received request cmpl-e851cb8591b14adeb6b3b2292e77fcee-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:56 async_llm_engine.py:173] Added request cmpl-e851cb8591b14adeb6b3b2292e77fcee-0.
INFO 01-31 12:49:56 logger.py:36] Received request cmpl-45234a5bb0174777bc0e563d7d581d2c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:56 async_llm_engine.py:173] Added request cmpl-45234a5bb0174777bc0e563d7d581d2c-0.
INFO 01-31 12:49:56 logger.py:36] Received request cmpl-2c277f5354144d6b9a9c23becce594ea-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:56 async_llm_engine.py:173] Added request cmpl-2c277f5354144d6b9a9c23becce594ea-0.
INFO 01-31 12:49:56 logger.py:36] Received request cmpl-75cdcc94c7af48788d3e650cc75324c2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:56 async_llm_engine.py:173] Added request cmpl-75cdcc94c7af48788d3e650cc75324c2-0.
INFO 01-31 12:49:56 logger.py:36] Received request cmpl-6e5f9c77e593447aba49b895b284d924-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:56 async_llm_engine.py:173] Added request cmpl-6e5f9c77e593447aba49b895b284d924-0.
INFO 01-31 12:49:56 logger.py:36] Received request cmpl-702a8d4b899045b28e57cc583c94897d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:56 logger.py:36] Received request cmpl-83f42ff53fcc4819b2aeea9aec073940-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:56 async_llm_engine.py:173] Added request cmpl-702a8d4b899045b28e57cc583c94897d-0.
INFO 01-31 12:49:56 async_llm_engine.py:173] Added request cmpl-83f42ff53fcc4819b2aeea9aec073940-0.
INFO 01-31 12:49:56 logger.py:36] Received request cmpl-205d7261b1664c1c96986bc070721c55-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:56 async_llm_engine.py:173] Added request cmpl-205d7261b1664c1c96986bc070721c55-0.
INFO 01-31 12:49:56 logger.py:36] Received request cmpl-89f24a6a14fb4f28bf38ccde98c8f76a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:56 async_llm_engine.py:173] Added request cmpl-89f24a6a14fb4f28bf38ccde98c8f76a-0.
INFO 01-31 12:49:56 logger.py:36] Received request cmpl-836b2e79a0274306b7bdccf074f5c777-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:56 async_llm_engine.py:173] Added request cmpl-836b2e79a0274306b7bdccf074f5c777-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:49:58 metrics.py:396] Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 207.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 01-31 12:49:58 async_llm_engine.py:140] Finished request cmpl-e851cb8591b14adeb6b3b2292e77fcee-0.
INFO 01-31 12:49:58 async_llm_engine.py:140] Finished request cmpl-45234a5bb0174777bc0e563d7d581d2c-0.
INFO 01-31 12:49:58 async_llm_engine.py:140] Finished request cmpl-2c277f5354144d6b9a9c23becce594ea-0.
INFO 01-31 12:49:58 async_llm_engine.py:140] Finished request cmpl-75cdcc94c7af48788d3e650cc75324c2-0.
INFO 01-31 12:49:58 async_llm_engine.py:140] Finished request cmpl-6e5f9c77e593447aba49b895b284d924-0.
INFO 01-31 12:49:58 async_llm_engine.py:140] Finished request cmpl-702a8d4b899045b28e57cc583c94897d-0.
INFO 01-31 12:49:58 async_llm_engine.py:140] Finished request cmpl-83f42ff53fcc4819b2aeea9aec073940-0.
INFO 01-31 12:49:58 async_llm_engine.py:140] Finished request cmpl-205d7261b1664c1c96986bc070721c55-0.
INFO 01-31 12:49:58 async_llm_engine.py:140] Finished request cmpl-89f24a6a14fb4f28bf38ccde98c8f76a-0.
INFO 01-31 12:49:58 async_llm_engine.py:140] Finished request cmpl-836b2e79a0274306b7bdccf074f5c777-0.
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:49:59 logger.py:36] Received request cmpl-2d3fb379d515466e811eb8e8dd97ecc7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:59 async_llm_engine.py:173] Added request cmpl-2d3fb379d515466e811eb8e8dd97ecc7-0.
INFO 01-31 12:49:59 logger.py:36] Received request cmpl-fd669bcd975b4d9ba020bc1ce07fc4e2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:59 logger.py:36] Received request cmpl-db8b9e1ce7704d7ca595b7e990ecbc1f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:59 async_llm_engine.py:173] Added request cmpl-fd669bcd975b4d9ba020bc1ce07fc4e2-0.
INFO 01-31 12:49:59 async_llm_engine.py:173] Added request cmpl-db8b9e1ce7704d7ca595b7e990ecbc1f-0.
INFO 01-31 12:49:59 logger.py:36] Received request cmpl-0ff9f44cacbf471bb8b674c4e0371448-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:59 async_llm_engine.py:173] Added request cmpl-0ff9f44cacbf471bb8b674c4e0371448-0.
INFO 01-31 12:49:59 logger.py:36] Received request cmpl-064f5cbe61894f298820bb080e4070e2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:59 logger.py:36] Received request cmpl-49c17a4b32e34de99abef093315633cc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:59 logger.py:36] Received request cmpl-a866fd7cb5e64c909ccb942f9a97ab85-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:59 logger.py:36] Received request cmpl-d04f8bc08a174d9381207c200f823e99-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:59 async_llm_engine.py:173] Added request cmpl-064f5cbe61894f298820bb080e4070e2-0.
INFO 01-31 12:49:59 async_llm_engine.py:173] Added request cmpl-49c17a4b32e34de99abef093315633cc-0.
INFO 01-31 12:49:59 async_llm_engine.py:173] Added request cmpl-a866fd7cb5e64c909ccb942f9a97ab85-0.
INFO 01-31 12:49:59 async_llm_engine.py:173] Added request cmpl-d04f8bc08a174d9381207c200f823e99-0.
INFO 01-31 12:49:59 logger.py:36] Received request cmpl-34646cf199714160a8b23bb8afd0e14f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:59 async_llm_engine.py:173] Added request cmpl-34646cf199714160a8b23bb8afd0e14f-0.
INFO 01-31 12:49:59 logger.py:36] Received request cmpl-6abf30af6c094f14baa2b919e2a8233f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:49:59 async_llm_engine.py:173] Added request cmpl-6abf30af6c094f14baa2b919e2a8233f-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:50:01 async_llm_engine.py:140] Finished request cmpl-2d3fb379d515466e811eb8e8dd97ecc7-0.
INFO 01-31 12:50:01 async_llm_engine.py:140] Finished request cmpl-fd669bcd975b4d9ba020bc1ce07fc4e2-0.
INFO 01-31 12:50:01 async_llm_engine.py:140] Finished request cmpl-db8b9e1ce7704d7ca595b7e990ecbc1f-0.
INFO 01-31 12:50:01 async_llm_engine.py:140] Finished request cmpl-0ff9f44cacbf471bb8b674c4e0371448-0.
INFO 01-31 12:50:01 async_llm_engine.py:140] Finished request cmpl-064f5cbe61894f298820bb080e4070e2-0.
INFO 01-31 12:50:01 async_llm_engine.py:140] Finished request cmpl-49c17a4b32e34de99abef093315633cc-0.
INFO 01-31 12:50:01 async_llm_engine.py:140] Finished request cmpl-a866fd7cb5e64c909ccb942f9a97ab85-0.
INFO 01-31 12:50:01 async_llm_engine.py:140] Finished request cmpl-d04f8bc08a174d9381207c200f823e99-0.
INFO 01-31 12:50:01 async_llm_engine.py:140] Finished request cmpl-34646cf199714160a8b23bb8afd0e14f-0.
INFO 01-31 12:50:01 async_llm_engine.py:140] Finished request cmpl-6abf30af6c094f14baa2b919e2a8233f-0.
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:50:02 logger.py:36] Received request cmpl-4620787398334be4a636d77569d38602-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:02 async_llm_engine.py:173] Added request cmpl-4620787398334be4a636d77569d38602-0.
INFO 01-31 12:50:02 logger.py:36] Received request cmpl-5b886e7784654593a041edbde22538dc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:02 async_llm_engine.py:173] Added request cmpl-5b886e7784654593a041edbde22538dc-0.
INFO 01-31 12:50:02 logger.py:36] Received request cmpl-5519075e1f0e474c8ff2d3cca9e056ea-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:02 async_llm_engine.py:173] Added request cmpl-5519075e1f0e474c8ff2d3cca9e056ea-0.
INFO 01-31 12:50:02 logger.py:36] Received request cmpl-46af1a28522c43e3bc09707589b90222-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:02 async_llm_engine.py:173] Added request cmpl-46af1a28522c43e3bc09707589b90222-0.
INFO 01-31 12:50:02 logger.py:36] Received request cmpl-1c93e1dd78e24693aa0a2a566a96cb47-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:02 async_llm_engine.py:173] Added request cmpl-1c93e1dd78e24693aa0a2a566a96cb47-0.
INFO 01-31 12:50:02 logger.py:36] Received request cmpl-3436b9c403c34cb6887e33677edef390-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:02 async_llm_engine.py:173] Added request cmpl-3436b9c403c34cb6887e33677edef390-0.
INFO 01-31 12:50:02 logger.py:36] Received request cmpl-207ad9c0331d418c8ff15fb6c58cc556-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:02 async_llm_engine.py:173] Added request cmpl-207ad9c0331d418c8ff15fb6c58cc556-0.
INFO 01-31 12:50:02 logger.py:36] Received request cmpl-e7421720cc7340f69003637b6dcfc9c4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:02 async_llm_engine.py:173] Added request cmpl-e7421720cc7340f69003637b6dcfc9c4-0.
INFO 01-31 12:50:02 logger.py:36] Received request cmpl-314c8fa2428949aaa18803936a811eeb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:02 logger.py:36] Received request cmpl-4045b991414f41a5877209629cb9e2ab-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:02 async_llm_engine.py:173] Added request cmpl-314c8fa2428949aaa18803936a811eeb-0.
INFO 01-31 12:50:02 async_llm_engine.py:173] Added request cmpl-4045b991414f41a5877209629cb9e2ab-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:50:03 metrics.py:396] Avg prompt throughput: 35.8 tokens/s, Avg generation throughput: 203.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:50:05 async_llm_engine.py:140] Finished request cmpl-4620787398334be4a636d77569d38602-0.
INFO 01-31 12:50:05 async_llm_engine.py:140] Finished request cmpl-5b886e7784654593a041edbde22538dc-0.
INFO 01-31 12:50:05 async_llm_engine.py:140] Finished request cmpl-5519075e1f0e474c8ff2d3cca9e056ea-0.
INFO 01-31 12:50:05 async_llm_engine.py:140] Finished request cmpl-46af1a28522c43e3bc09707589b90222-0.
INFO 01-31 12:50:05 async_llm_engine.py:140] Finished request cmpl-1c93e1dd78e24693aa0a2a566a96cb47-0.
INFO 01-31 12:50:05 async_llm_engine.py:140] Finished request cmpl-3436b9c403c34cb6887e33677edef390-0.
INFO 01-31 12:50:05 async_llm_engine.py:140] Finished request cmpl-207ad9c0331d418c8ff15fb6c58cc556-0.
INFO 01-31 12:50:05 async_llm_engine.py:140] Finished request cmpl-e7421720cc7340f69003637b6dcfc9c4-0.
INFO 01-31 12:50:05 async_llm_engine.py:140] Finished request cmpl-314c8fa2428949aaa18803936a811eeb-0.
INFO 01-31 12:50:05 async_llm_engine.py:140] Finished request cmpl-4045b991414f41a5877209629cb9e2ab-0.
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:50:05 logger.py:36] Received request cmpl-3681fddf17384762abf6653d5a2244e9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:05 logger.py:36] Received request cmpl-42144442d7274853ac3d480b6748d982-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:05 async_llm_engine.py:173] Added request cmpl-3681fddf17384762abf6653d5a2244e9-0.
INFO 01-31 12:50:05 async_llm_engine.py:173] Added request cmpl-42144442d7274853ac3d480b6748d982-0.
INFO 01-31 12:50:05 logger.py:36] Received request cmpl-5d1ee0937b614ce38e548e7e87c0f4c0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:05 async_llm_engine.py:173] Added request cmpl-5d1ee0937b614ce38e548e7e87c0f4c0-0.
INFO 01-31 12:50:05 logger.py:36] Received request cmpl-8b441b16a02846cc9d7b628583b433b2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:05 async_llm_engine.py:173] Added request cmpl-8b441b16a02846cc9d7b628583b433b2-0.
INFO 01-31 12:50:05 logger.py:36] Received request cmpl-0dfa70b8e72e4965bac541ed6788b39a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:05 async_llm_engine.py:173] Added request cmpl-0dfa70b8e72e4965bac541ed6788b39a-0.
INFO 01-31 12:50:05 logger.py:36] Received request cmpl-b3e03264df6047dca198fdde39e7e124-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:05 async_llm_engine.py:173] Added request cmpl-b3e03264df6047dca198fdde39e7e124-0.
INFO 01-31 12:50:05 logger.py:36] Received request cmpl-133c98444aab4299b5750c10735a61df-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:05 async_llm_engine.py:173] Added request cmpl-133c98444aab4299b5750c10735a61df-0.
INFO:     89.105.200.105:38990 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:38988 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:50:06 logger.py:36] Received request cmpl-c05b76342a4440d6a40e06c9d3a3d093-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:06 logger.py:36] Received request cmpl-9e4df39d4d2d41ccab138c3568306d13-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:06 async_llm_engine.py:173] Added request cmpl-c05b76342a4440d6a40e06c9d3a3d093-0.
INFO 01-31 12:50:06 async_llm_engine.py:173] Added request cmpl-9e4df39d4d2d41ccab138c3568306d13-0.
INFO 01-31 12:50:06 logger.py:36] Received request cmpl-6a2399b7639d4248825e7f9e046c5a1e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:06 async_llm_engine.py:173] Added request cmpl-6a2399b7639d4248825e7f9e046c5a1e-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:50:08 async_llm_engine.py:140] Finished request cmpl-3681fddf17384762abf6653d5a2244e9-0.
INFO 01-31 12:50:08 async_llm_engine.py:140] Finished request cmpl-42144442d7274853ac3d480b6748d982-0.
INFO 01-31 12:50:08 async_llm_engine.py:140] Finished request cmpl-5d1ee0937b614ce38e548e7e87c0f4c0-0.
INFO 01-31 12:50:08 async_llm_engine.py:140] Finished request cmpl-8b441b16a02846cc9d7b628583b433b2-0.
INFO 01-31 12:50:08 async_llm_engine.py:140] Finished request cmpl-0dfa70b8e72e4965bac541ed6788b39a-0.
INFO 01-31 12:50:08 async_llm_engine.py:140] Finished request cmpl-b3e03264df6047dca198fdde39e7e124-0.
INFO 01-31 12:50:08 async_llm_engine.py:140] Finished request cmpl-133c98444aab4299b5750c10735a61df-0.
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:50:08 logger.py:36] Received request cmpl-5bd23a98d16a4399aeb5df4ff57b05a6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:08 async_llm_engine.py:173] Added request cmpl-5bd23a98d16a4399aeb5df4ff57b05a6-0.
INFO 01-31 12:50:08 logger.py:36] Received request cmpl-a7159086a16a41b594bf39ad30093b91-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:08 async_llm_engine.py:173] Added request cmpl-a7159086a16a41b594bf39ad30093b91-0.
INFO 01-31 12:50:08 logger.py:36] Received request cmpl-778baadf2101483985563ae3af420f25-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:08 async_llm_engine.py:173] Added request cmpl-778baadf2101483985563ae3af420f25-0.
INFO 01-31 12:50:08 logger.py:36] Received request cmpl-a610fb6327e84e33bc2877c6faef7c89-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:08 async_llm_engine.py:173] Added request cmpl-a610fb6327e84e33bc2877c6faef7c89-0.
INFO 01-31 12:50:08 logger.py:36] Received request cmpl-11c89088e10540608733e8aed4556d06-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:08 async_llm_engine.py:173] Added request cmpl-11c89088e10540608733e8aed4556d06-0.
INFO 01-31 12:50:08 logger.py:36] Received request cmpl-38b41c51d52d474fa332727e54333311-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:08 async_llm_engine.py:173] Added request cmpl-38b41c51d52d474fa332727e54333311-0.
INFO 01-31 12:50:08 logger.py:36] Received request cmpl-a3673a3f7e7448faa4c76fd085a02716-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:08 async_llm_engine.py:173] Added request cmpl-a3673a3f7e7448faa4c76fd085a02716-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:50:08 metrics.py:396] Avg prompt throughput: 30.4 tokens/s, Avg generation throughput: 197.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 01-31 12:50:09 async_llm_engine.py:140] Finished request cmpl-c05b76342a4440d6a40e06c9d3a3d093-0.
INFO 01-31 12:50:09 async_llm_engine.py:140] Finished request cmpl-9e4df39d4d2d41ccab138c3568306d13-0.
INFO 01-31 12:50:09 async_llm_engine.py:140] Finished request cmpl-6a2399b7639d4248825e7f9e046c5a1e-0.
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:50:09 logger.py:36] Received request cmpl-2078bcf758ae45f692908ffaa6277230-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:09 async_llm_engine.py:173] Added request cmpl-2078bcf758ae45f692908ffaa6277230-0.
INFO 01-31 12:50:09 logger.py:36] Received request cmpl-6b096aee470b456eb7124b58a44ce501-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:09 async_llm_engine.py:173] Added request cmpl-6b096aee470b456eb7124b58a44ce501-0.
INFO 01-31 12:50:09 logger.py:36] Received request cmpl-3e4c1128b58746fd8eddb49d4ecf0ddc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:09 async_llm_engine.py:173] Added request cmpl-3e4c1128b58746fd8eddb49d4ecf0ddc-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:50:11 async_llm_engine.py:140] Finished request cmpl-5bd23a98d16a4399aeb5df4ff57b05a6-0.
INFO 01-31 12:50:11 async_llm_engine.py:140] Finished request cmpl-a7159086a16a41b594bf39ad30093b91-0.
INFO 01-31 12:50:11 async_llm_engine.py:140] Finished request cmpl-778baadf2101483985563ae3af420f25-0.
INFO 01-31 12:50:11 async_llm_engine.py:140] Finished request cmpl-a610fb6327e84e33bc2877c6faef7c89-0.
INFO 01-31 12:50:11 async_llm_engine.py:140] Finished request cmpl-11c89088e10540608733e8aed4556d06-0.
INFO 01-31 12:50:11 async_llm_engine.py:140] Finished request cmpl-38b41c51d52d474fa332727e54333311-0.
INFO 01-31 12:50:11 async_llm_engine.py:140] Finished request cmpl-a3673a3f7e7448faa4c76fd085a02716-0.
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:50:11 logger.py:36] Received request cmpl-43d827b6074a473ca8db643d7046ddb6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:11 async_llm_engine.py:173] Added request cmpl-43d827b6074a473ca8db643d7046ddb6-0.
INFO 01-31 12:50:11 logger.py:36] Received request cmpl-ffe1e338ba464168888ac42f145e0ee9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:11 async_llm_engine.py:173] Added request cmpl-ffe1e338ba464168888ac42f145e0ee9-0.
INFO 01-31 12:50:11 logger.py:36] Received request cmpl-cd4cae0f8e4f44a1a0ad5682c0ed8a1e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:11 logger.py:36] Received request cmpl-2fce8dca4470484a8d64e83c562b4455-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:11 async_llm_engine.py:173] Added request cmpl-cd4cae0f8e4f44a1a0ad5682c0ed8a1e-0.
INFO 01-31 12:50:11 async_llm_engine.py:173] Added request cmpl-2fce8dca4470484a8d64e83c562b4455-0.
INFO 01-31 12:50:11 logger.py:36] Received request cmpl-bfe207f384314e37a954acb0c52d6751-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:11 async_llm_engine.py:173] Added request cmpl-bfe207f384314e37a954acb0c52d6751-0.
INFO 01-31 12:50:11 logger.py:36] Received request cmpl-c7bf71f543f6498594326faf20aa190e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:11 async_llm_engine.py:173] Added request cmpl-c7bf71f543f6498594326faf20aa190e-0.
INFO 01-31 12:50:11 logger.py:36] Received request cmpl-1f7f777f905a40abb694d56a5ab2ed38-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:11 async_llm_engine.py:173] Added request cmpl-1f7f777f905a40abb694d56a5ab2ed38-0.
INFO:     192.168.200.241:56858 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:50:12 async_llm_engine.py:140] Finished request cmpl-2078bcf758ae45f692908ffaa6277230-0.
INFO 01-31 12:50:12 async_llm_engine.py:140] Finished request cmpl-6b096aee470b456eb7124b58a44ce501-0.
INFO 01-31 12:50:12 async_llm_engine.py:140] Finished request cmpl-3e4c1128b58746fd8eddb49d4ecf0ddc-0.
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:50:12 logger.py:36] Received request cmpl-4de7f47593184e1288e8076c87078171-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:12 async_llm_engine.py:173] Added request cmpl-4de7f47593184e1288e8076c87078171-0.
INFO 01-31 12:50:12 logger.py:36] Received request cmpl-59a9c646eddf4444a4dd6f3310931125-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151646, 93313, 13186, 1045, 17646, 12624, 369, 8003, 12779], lora_request: None, prompt_adapter_request: None.
INFO 01-31 12:50:12 async_llm_engine.py:173] Added request cmpl-59a9c646eddf4444a4dd6f3310931125-0.
INFO:     192.168.200.241:56800 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:50:13 metrics.py:396] Avg prompt throughput: 21.4 tokens/s, Avg generation throughput: 199.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:56800 - "GET /metrics HTTP/1.1" 200 OK
INFO 01-31 12:50:14 async_llm_engine.py:140] Finished request cmpl-43d827b6074a473ca8db643d7046ddb6-0.
INFO 01-31 12:50:14 async_llm_engine.py:140] Finished request cmpl-ffe1e338ba464168888ac42f145e0ee9-0.
INFO 01-31 12:50:14 async_llm_engine.py:140] Finished request cmpl-cd4cae0f8e4f44a1a0ad5682c0ed8a1e-0.
INFO 01-31 12:50:14 async_llm_engine.py:140] Finished request cmpl-2fce8dca4470484a8d64e83c562b4455-0.
INFO 01-31 12:50:14 async_llm_engine.py:140] Finished request cmpl-bfe207f384314e37a954acb0c52d6751-0.
INFO 01-31 12:50:14 async_llm_engine.py:140] Finished request cmpl-c7bf71f543f6498594326faf20aa190e-0.
INFO 01-31 12:50:14 async_llm_engine.py:140] Finished request cmpl-1f7f777f905a40abb694d56a5ab2ed38-0.
INFO:     192.168.200.241:56810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-31 12:50:15 async_llm_engine.py:140] Finished request cmpl-4de7f47593184e1288e8076c87078171-0.
INFO 01-31 12:50:15 async_llm_engine.py:140] Finished request cmpl-59a9c646eddf4444a4dd6f3310931125-0.
INFO:     192.168.200.241:56840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:56844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     89.105.200.105:37420 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:37436 - "GET /health HTTP/1.1" 200 OK
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [1]
INFO 01-31 12:50:21 async_llm_engine.py:53] Engine is gracefully shutting down.
