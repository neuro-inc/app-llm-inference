The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
0it [00:00, ?it/s]0it [00:00, ?it/s]
INFO 02-05 12:20:16 api_server.py:219] vLLM API server version 0.5.3.post1
INFO 02-05 12:20:16 api_server.py:220] args: Namespace(host='0.0.0.0', port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision='main', tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='half', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=128000, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
WARNING 02-05 12:20:16 config.py:1425] Casting torch.bfloat16 to torch.float16.
WARNING 02-05 12:20:16 arg_utils.py:762] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 02-05 12:20:16 config.py:806] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 02-05 12:20:16 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=128000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 02-05 12:20:18 model_runner.py:680] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-05 12:20:18 weight_utils.py:223] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.47s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.57s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.27s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.30s/it]

INFO 02-05 12:20:24 model_runner.py:692] Loading model weights took 14.9888 GB
INFO 02-05 12:20:25 gpu_executor.py:102] # GPU blocks: 28172, # CPU blocks: 2048
WARNING 02-05 12:20:28 serving_embedding.py:170] embedding_mode is False. Embedding API will not work.
INFO 02-05 12:20:28 api_server.py:292] Available routes are:
INFO 02-05 12:20:28 api_server.py:297] Route: /openapi.json, Methods: HEAD, GET
INFO 02-05 12:20:28 api_server.py:297] Route: /docs, Methods: HEAD, GET
INFO 02-05 12:20:28 api_server.py:297] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 02-05 12:20:28 api_server.py:297] Route: /redoc, Methods: HEAD, GET
INFO 02-05 12:20:28 api_server.py:297] Route: /health, Methods: GET
INFO 02-05 12:20:28 api_server.py:297] Route: /tokenize, Methods: POST
INFO 02-05 12:20:28 api_server.py:297] Route: /detokenize, Methods: POST
INFO 02-05 12:20:28 api_server.py:297] Route: /v1/models, Methods: GET
INFO 02-05 12:20:28 api_server.py:297] Route: /version, Methods: GET
INFO 02-05 12:20:28 api_server.py:297] Route: /v1/chat/completions, Methods: POST
INFO 02-05 12:20:28 api_server.py:297] Route: /v1/completions, Methods: POST
INFO 02-05 12:20:28 api_server.py:297] Route: /v1/embeddings, Methods: POST
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO 02-05 12:20:38 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:20:48 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:20:58 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:21:08 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:21:18 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:21:28 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:21:38 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:21:48 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:21:58 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:22:08 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:22:18 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:22:28 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:22:38 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:22:48 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:22:58 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:23:08 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:23:18 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:23:28 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:23:38 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:23:48 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:23:58 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:24:08 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:24:18 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:24:28 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:24:38 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:24:48 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:24:58 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:25:08 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:55278 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:55276 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:25:18 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:45884 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:23 logger.py:36] Received request cmpl-c9dbb8839d3646629372d827a2736030-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:23 async_llm_engine.py:173] Added request cmpl-c9dbb8839d3646629372d827a2736030-0.
INFO 02-05 12:25:23 logger.py:36] Received request cmpl-c2d1cabaea584c93a98afe63ea271625-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:23 async_llm_engine.py:173] Added request cmpl-c2d1cabaea584c93a98afe63ea271625-0.
INFO 02-05 12:25:23 logger.py:36] Received request cmpl-dd3939c7f6034fe6b112f4dd02012e0b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:23 async_llm_engine.py:173] Added request cmpl-dd3939c7f6034fe6b112f4dd02012e0b-0.
INFO 02-05 12:25:23 logger.py:36] Received request cmpl-b38d5fae43c242569a990bdbbdc98db5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:23 async_llm_engine.py:173] Added request cmpl-b38d5fae43c242569a990bdbbdc98db5-0.
INFO 02-05 12:25:23 logger.py:36] Received request cmpl-04b0433b53ec451694641ece06773a66-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:23 async_llm_engine.py:173] Added request cmpl-04b0433b53ec451694641ece06773a66-0.
INFO 02-05 12:25:23 logger.py:36] Received request cmpl-070ebf42058049fcbe0ddc03e48ae6f0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:23 async_llm_engine.py:173] Added request cmpl-070ebf42058049fcbe0ddc03e48ae6f0-0.
INFO 02-05 12:25:23 logger.py:36] Received request cmpl-708e776092264423ad61ee8547f4a3bc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:23 async_llm_engine.py:173] Added request cmpl-708e776092264423ad61ee8547f4a3bc-0.
INFO 02-05 12:25:23 logger.py:36] Received request cmpl-ef19b1f9d149484db3f801e8daff5404-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:23 async_llm_engine.py:173] Added request cmpl-ef19b1f9d149484db3f801e8daff5404-0.
INFO 02-05 12:25:23 logger.py:36] Received request cmpl-0dd15d9bb9c04537a9cc4f20b66a2719-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:23 async_llm_engine.py:173] Added request cmpl-0dd15d9bb9c04537a9cc4f20b66a2719-0.
INFO 02-05 12:25:23 logger.py:36] Received request cmpl-847e7047ece9450c8b5497ecaa7d500d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:23 async_llm_engine.py:173] Added request cmpl-847e7047ece9450c8b5497ecaa7d500d-0.
INFO 02-05 12:25:23 metrics.py:396] Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:24 async_llm_engine.py:140] Finished request cmpl-c9dbb8839d3646629372d827a2736030-0.
INFO 02-05 12:25:24 async_llm_engine.py:140] Finished request cmpl-c2d1cabaea584c93a98afe63ea271625-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:24 async_llm_engine.py:140] Finished request cmpl-dd3939c7f6034fe6b112f4dd02012e0b-0.
INFO 02-05 12:25:24 async_llm_engine.py:140] Finished request cmpl-b38d5fae43c242569a990bdbbdc98db5-0.
INFO 02-05 12:25:24 async_llm_engine.py:140] Finished request cmpl-04b0433b53ec451694641ece06773a66-0.
INFO 02-05 12:25:24 async_llm_engine.py:140] Finished request cmpl-070ebf42058049fcbe0ddc03e48ae6f0-0.
INFO 02-05 12:25:24 async_llm_engine.py:140] Finished request cmpl-708e776092264423ad61ee8547f4a3bc-0.
INFO 02-05 12:25:24 async_llm_engine.py:140] Finished request cmpl-ef19b1f9d149484db3f801e8daff5404-0.
INFO 02-05 12:25:24 async_llm_engine.py:140] Finished request cmpl-0dd15d9bb9c04537a9cc4f20b66a2719-0.
INFO 02-05 12:25:24 async_llm_engine.py:140] Finished request cmpl-847e7047ece9450c8b5497ecaa7d500d-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:24 logger.py:36] Received request cmpl-d48461060a4c4d66a580eeec02308fda-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:24 async_llm_engine.py:173] Added request cmpl-d48461060a4c4d66a580eeec02308fda-0.
INFO 02-05 12:25:24 logger.py:36] Received request cmpl-06abff8d8a9a4bdaa3bae7e095c56f19-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:24 async_llm_engine.py:173] Added request cmpl-06abff8d8a9a4bdaa3bae7e095c56f19-0.
INFO 02-05 12:25:24 logger.py:36] Received request cmpl-d84caed8eeed40fda1e426784df80d52-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:24 async_llm_engine.py:173] Added request cmpl-d84caed8eeed40fda1e426784df80d52-0.
INFO 02-05 12:25:24 logger.py:36] Received request cmpl-dde59f1428d04875ad2df0bb05b4d95e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:24 logger.py:36] Received request cmpl-c63ff919a3bd432dad29d4bfb6edef4d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:24 async_llm_engine.py:173] Added request cmpl-dde59f1428d04875ad2df0bb05b4d95e-0.
INFO 02-05 12:25:24 async_llm_engine.py:173] Added request cmpl-c63ff919a3bd432dad29d4bfb6edef4d-0.
INFO 02-05 12:25:24 logger.py:36] Received request cmpl-905883a3d43847219b6ee8c827836982-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:24 logger.py:36] Received request cmpl-80cca0169b694585a9944d905766b742-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:24 async_llm_engine.py:173] Added request cmpl-905883a3d43847219b6ee8c827836982-0.
INFO 02-05 12:25:24 async_llm_engine.py:173] Added request cmpl-80cca0169b694585a9944d905766b742-0.
INFO 02-05 12:25:24 logger.py:36] Received request cmpl-7bb23fd90928418cb550fe1f63f012ea-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:24 async_llm_engine.py:173] Added request cmpl-7bb23fd90928418cb550fe1f63f012ea-0.
INFO 02-05 12:25:24 logger.py:36] Received request cmpl-95717afe16f84a02ab3d144e41932aca-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:24 async_llm_engine.py:173] Added request cmpl-95717afe16f84a02ab3d144e41932aca-0.
INFO 02-05 12:25:24 logger.py:36] Received request cmpl-97abfdd68fe34bb288d3593f04f53286-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:24 async_llm_engine.py:173] Added request cmpl-97abfdd68fe34bb288d3593f04f53286-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:25 async_llm_engine.py:140] Finished request cmpl-d48461060a4c4d66a580eeec02308fda-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:25 async_llm_engine.py:140] Finished request cmpl-06abff8d8a9a4bdaa3bae7e095c56f19-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:25 async_llm_engine.py:140] Finished request cmpl-d84caed8eeed40fda1e426784df80d52-0.
INFO 02-05 12:25:25 async_llm_engine.py:140] Finished request cmpl-dde59f1428d04875ad2df0bb05b4d95e-0.
INFO 02-05 12:25:25 async_llm_engine.py:140] Finished request cmpl-c63ff919a3bd432dad29d4bfb6edef4d-0.
INFO 02-05 12:25:25 async_llm_engine.py:140] Finished request cmpl-905883a3d43847219b6ee8c827836982-0.
INFO 02-05 12:25:25 async_llm_engine.py:140] Finished request cmpl-80cca0169b694585a9944d905766b742-0.
INFO 02-05 12:25:25 async_llm_engine.py:140] Finished request cmpl-7bb23fd90928418cb550fe1f63f012ea-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:25 async_llm_engine.py:140] Finished request cmpl-95717afe16f84a02ab3d144e41932aca-0.
INFO 02-05 12:25:25 async_llm_engine.py:140] Finished request cmpl-97abfdd68fe34bb288d3593f04f53286-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:25 logger.py:36] Received request cmpl-d33cdc5f8f274b47af04f44c626ff723-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:25 async_llm_engine.py:173] Added request cmpl-d33cdc5f8f274b47af04f44c626ff723-0.
INFO:     89.105.200.105:57708 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:57700 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:25:25 logger.py:36] Received request cmpl-9dbe9b70428443f1a614422f2ccf4c33-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:25 async_llm_engine.py:173] Added request cmpl-9dbe9b70428443f1a614422f2ccf4c33-0.
INFO 02-05 12:25:26 logger.py:36] Received request cmpl-2b0a50170d424071b6921d20ce624562-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:26 async_llm_engine.py:173] Added request cmpl-2b0a50170d424071b6921d20ce624562-0.
INFO 02-05 12:25:26 logger.py:36] Received request cmpl-265aa4e544584116b67e7bdcaea503bc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:26 async_llm_engine.py:173] Added request cmpl-265aa4e544584116b67e7bdcaea503bc-0.
INFO 02-05 12:25:26 logger.py:36] Received request cmpl-b0f3364c8c204daea9596df5340591ab-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:26 async_llm_engine.py:173] Added request cmpl-b0f3364c8c204daea9596df5340591ab-0.
INFO 02-05 12:25:26 logger.py:36] Received request cmpl-0cbad15c108b4b7db88b87c938d09814-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:26 async_llm_engine.py:173] Added request cmpl-0cbad15c108b4b7db88b87c938d09814-0.
INFO 02-05 12:25:26 logger.py:36] Received request cmpl-20f8a14e6018498e913cbe0062b17bd0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:26 async_llm_engine.py:173] Added request cmpl-20f8a14e6018498e913cbe0062b17bd0-0.
INFO 02-05 12:25:26 logger.py:36] Received request cmpl-e3e9fcf7df684c2ab29743e9c7004f38-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:26 async_llm_engine.py:173] Added request cmpl-e3e9fcf7df684c2ab29743e9c7004f38-0.
INFO 02-05 12:25:26 logger.py:36] Received request cmpl-8a5e59d159c14e0184d2ed4eaec443cb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:26 async_llm_engine.py:173] Added request cmpl-8a5e59d159c14e0184d2ed4eaec443cb-0.
INFO 02-05 12:25:26 logger.py:36] Received request cmpl-3dd8b5c98f9a4b5f9f2bc569f2133e63-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:26 async_llm_engine.py:173] Added request cmpl-3dd8b5c98f9a4b5f9f2bc569f2133e63-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:27 async_llm_engine.py:140] Finished request cmpl-d33cdc5f8f274b47af04f44c626ff723-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:27 async_llm_engine.py:140] Finished request cmpl-9dbe9b70428443f1a614422f2ccf4c33-0.
INFO 02-05 12:25:27 async_llm_engine.py:140] Finished request cmpl-2b0a50170d424071b6921d20ce624562-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:27 async_llm_engine.py:140] Finished request cmpl-265aa4e544584116b67e7bdcaea503bc-0.
INFO 02-05 12:25:27 async_llm_engine.py:140] Finished request cmpl-b0f3364c8c204daea9596df5340591ab-0.
INFO 02-05 12:25:27 async_llm_engine.py:140] Finished request cmpl-0cbad15c108b4b7db88b87c938d09814-0.
INFO 02-05 12:25:27 async_llm_engine.py:140] Finished request cmpl-20f8a14e6018498e913cbe0062b17bd0-0.
INFO 02-05 12:25:27 async_llm_engine.py:140] Finished request cmpl-e3e9fcf7df684c2ab29743e9c7004f38-0.
INFO 02-05 12:25:27 async_llm_engine.py:140] Finished request cmpl-8a5e59d159c14e0184d2ed4eaec443cb-0.
INFO 02-05 12:25:27 async_llm_engine.py:140] Finished request cmpl-3dd8b5c98f9a4b5f9f2bc569f2133e63-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:27 logger.py:36] Received request cmpl-45a52632395a439c8fe0757ca4b4080d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:27 async_llm_engine.py:173] Added request cmpl-45a52632395a439c8fe0757ca4b4080d-0.
INFO 02-05 12:25:27 logger.py:36] Received request cmpl-bd42b0e72ae0443b8f734bb684259584-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:27 async_llm_engine.py:173] Added request cmpl-bd42b0e72ae0443b8f734bb684259584-0.
INFO 02-05 12:25:27 logger.py:36] Received request cmpl-cf387a4924db4af1a4842127a337e4d8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:27 async_llm_engine.py:173] Added request cmpl-cf387a4924db4af1a4842127a337e4d8-0.
INFO 02-05 12:25:27 logger.py:36] Received request cmpl-d3110a0cfd2e4ec691091910f5b775c6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:27 async_llm_engine.py:173] Added request cmpl-d3110a0cfd2e4ec691091910f5b775c6-0.
INFO 02-05 12:25:27 logger.py:36] Received request cmpl-97497b10ba504a0e9fe80a133d62e5a0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:27 async_llm_engine.py:173] Added request cmpl-97497b10ba504a0e9fe80a133d62e5a0-0.
INFO 02-05 12:25:27 logger.py:36] Received request cmpl-9b6b574a03404274a89150430ecbb5f8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:27 async_llm_engine.py:173] Added request cmpl-9b6b574a03404274a89150430ecbb5f8-0.
INFO 02-05 12:25:27 logger.py:36] Received request cmpl-8ca5f97b1ebd4eabb97d1331a67206f7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:27 async_llm_engine.py:173] Added request cmpl-8ca5f97b1ebd4eabb97d1331a67206f7-0.
INFO 02-05 12:25:27 logger.py:36] Received request cmpl-0e009d7c497147e5952078bd6396cb69-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:27 async_llm_engine.py:173] Added request cmpl-0e009d7c497147e5952078bd6396cb69-0.
INFO 02-05 12:25:27 logger.py:36] Received request cmpl-25180567d4fd429e940e2150c93d0908-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:27 async_llm_engine.py:173] Added request cmpl-25180567d4fd429e940e2150c93d0908-0.
INFO 02-05 12:25:27 logger.py:36] Received request cmpl-532398d35e59443c8e3df6534eed0e96-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:27 async_llm_engine.py:173] Added request cmpl-532398d35e59443c8e3df6534eed0e96-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:28 async_llm_engine.py:140] Finished request cmpl-45a52632395a439c8fe0757ca4b4080d-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:28 async_llm_engine.py:140] Finished request cmpl-bd42b0e72ae0443b8f734bb684259584-0.
INFO 02-05 12:25:28 async_llm_engine.py:140] Finished request cmpl-cf387a4924db4af1a4842127a337e4d8-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:28 async_llm_engine.py:140] Finished request cmpl-d3110a0cfd2e4ec691091910f5b775c6-0.
INFO 02-05 12:25:28 async_llm_engine.py:140] Finished request cmpl-97497b10ba504a0e9fe80a133d62e5a0-0.
INFO 02-05 12:25:28 async_llm_engine.py:140] Finished request cmpl-9b6b574a03404274a89150430ecbb5f8-0.
INFO 02-05 12:25:28 async_llm_engine.py:140] Finished request cmpl-8ca5f97b1ebd4eabb97d1331a67206f7-0.
INFO 02-05 12:25:28 async_llm_engine.py:140] Finished request cmpl-0e009d7c497147e5952078bd6396cb69-0.
INFO 02-05 12:25:28 async_llm_engine.py:140] Finished request cmpl-25180567d4fd429e940e2150c93d0908-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:28 async_llm_engine.py:140] Finished request cmpl-532398d35e59443c8e3df6534eed0e96-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:28 logger.py:36] Received request cmpl-8ed725f4dc304058a6ec8a7655d293b9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:28 async_llm_engine.py:173] Added request cmpl-8ed725f4dc304058a6ec8a7655d293b9-0.
INFO 02-05 12:25:28 logger.py:36] Received request cmpl-0bcd29ff3ede444b851f3a5549295972-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:28 logger.py:36] Received request cmpl-6f2a061d477448129c621e67f79ffc86-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:28 async_llm_engine.py:173] Added request cmpl-0bcd29ff3ede444b851f3a5549295972-0.
INFO 02-05 12:25:28 async_llm_engine.py:173] Added request cmpl-6f2a061d477448129c621e67f79ffc86-0.
INFO 02-05 12:25:28 logger.py:36] Received request cmpl-f814c8be956e46bc9f06edf5f3d02059-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:28 async_llm_engine.py:173] Added request cmpl-f814c8be956e46bc9f06edf5f3d02059-0.
INFO 02-05 12:25:28 logger.py:36] Received request cmpl-acb6e95821874c238045faf0e596556e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:28 async_llm_engine.py:173] Added request cmpl-acb6e95821874c238045faf0e596556e-0.
INFO 02-05 12:25:28 logger.py:36] Received request cmpl-022a0d06bbcc4bec9f6235069eadb60c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:28 async_llm_engine.py:173] Added request cmpl-022a0d06bbcc4bec9f6235069eadb60c-0.
INFO 02-05 12:25:28 logger.py:36] Received request cmpl-d051c29b73e0474ea4c6f8ac99985ae4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:28 async_llm_engine.py:173] Added request cmpl-d051c29b73e0474ea4c6f8ac99985ae4-0.
INFO 02-05 12:25:28 logger.py:36] Received request cmpl-becfba207f7a451fbef669dd25f79a70-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:28 async_llm_engine.py:173] Added request cmpl-becfba207f7a451fbef669dd25f79a70-0.
INFO 02-05 12:25:28 logger.py:36] Received request cmpl-1cc587bfbd6f455b8e81759520454d79-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:28 async_llm_engine.py:173] Added request cmpl-1cc587bfbd6f455b8e81759520454d79-0.
INFO 02-05 12:25:28 logger.py:36] Received request cmpl-e30486cf5d7442ef850ccba6ecf3d2ba-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:28 async_llm_engine.py:173] Added request cmpl-e30486cf5d7442ef850ccba6ecf3d2ba-0.
INFO 02-05 12:25:28 metrics.py:396] Avg prompt throughput: 71.8 tokens/s, Avg generation throughput: 511.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:29 async_llm_engine.py:140] Finished request cmpl-8ed725f4dc304058a6ec8a7655d293b9-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:29 async_llm_engine.py:140] Finished request cmpl-0bcd29ff3ede444b851f3a5549295972-0.
INFO 02-05 12:25:29 async_llm_engine.py:140] Finished request cmpl-6f2a061d477448129c621e67f79ffc86-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:29 async_llm_engine.py:140] Finished request cmpl-f814c8be956e46bc9f06edf5f3d02059-0.
INFO 02-05 12:25:29 async_llm_engine.py:140] Finished request cmpl-acb6e95821874c238045faf0e596556e-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:29 async_llm_engine.py:140] Finished request cmpl-022a0d06bbcc4bec9f6235069eadb60c-0.
INFO 02-05 12:25:29 async_llm_engine.py:140] Finished request cmpl-d051c29b73e0474ea4c6f8ac99985ae4-0.
INFO 02-05 12:25:29 async_llm_engine.py:140] Finished request cmpl-becfba207f7a451fbef669dd25f79a70-0.
INFO 02-05 12:25:29 async_llm_engine.py:140] Finished request cmpl-1cc587bfbd6f455b8e81759520454d79-0.
INFO 02-05 12:25:29 async_llm_engine.py:140] Finished request cmpl-e30486cf5d7442ef850ccba6ecf3d2ba-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:29 logger.py:36] Received request cmpl-55d957d67fc74e68a97607890c39d749-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:29 async_llm_engine.py:173] Added request cmpl-55d957d67fc74e68a97607890c39d749-0.
INFO 02-05 12:25:29 logger.py:36] Received request cmpl-3131cd7d84bc4adfbbb9ce8a72eae656-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:29 async_llm_engine.py:173] Added request cmpl-3131cd7d84bc4adfbbb9ce8a72eae656-0.
INFO 02-05 12:25:29 logger.py:36] Received request cmpl-66d99e06793e403f9be5f1369eb43cec-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:29 async_llm_engine.py:173] Added request cmpl-66d99e06793e403f9be5f1369eb43cec-0.
INFO 02-05 12:25:29 logger.py:36] Received request cmpl-733f1a4323c443f4a507164daf11398f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:29 async_llm_engine.py:173] Added request cmpl-733f1a4323c443f4a507164daf11398f-0.
INFO 02-05 12:25:29 logger.py:36] Received request cmpl-6497d19151134f5f9fd56c4468e750e0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:29 async_llm_engine.py:173] Added request cmpl-6497d19151134f5f9fd56c4468e750e0-0.
INFO 02-05 12:25:29 logger.py:36] Received request cmpl-4794e4e546384d13811e646c5365a261-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:29 async_llm_engine.py:173] Added request cmpl-4794e4e546384d13811e646c5365a261-0.
INFO 02-05 12:25:29 logger.py:36] Received request cmpl-ed727496d18c46bfb34792b5507bb794-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:29 async_llm_engine.py:173] Added request cmpl-ed727496d18c46bfb34792b5507bb794-0.
INFO 02-05 12:25:29 logger.py:36] Received request cmpl-bd91a954ffcd449390c77532343a4e71-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:29 async_llm_engine.py:173] Added request cmpl-bd91a954ffcd449390c77532343a4e71-0.
INFO 02-05 12:25:29 logger.py:36] Received request cmpl-97cf7c68d019428398475d1482519621-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:29 logger.py:36] Received request cmpl-0b132c46b89c4122ba8a181698e8d455-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:29 async_llm_engine.py:173] Added request cmpl-97cf7c68d019428398475d1482519621-0.
INFO 02-05 12:25:29 async_llm_engine.py:173] Added request cmpl-0b132c46b89c4122ba8a181698e8d455-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:30 async_llm_engine.py:140] Finished request cmpl-55d957d67fc74e68a97607890c39d749-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:30 async_llm_engine.py:140] Finished request cmpl-3131cd7d84bc4adfbbb9ce8a72eae656-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:30 async_llm_engine.py:140] Finished request cmpl-66d99e06793e403f9be5f1369eb43cec-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:30 async_llm_engine.py:140] Finished request cmpl-733f1a4323c443f4a507164daf11398f-0.
INFO 02-05 12:25:30 async_llm_engine.py:140] Finished request cmpl-6497d19151134f5f9fd56c4468e750e0-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:30 async_llm_engine.py:140] Finished request cmpl-4794e4e546384d13811e646c5365a261-0.
INFO 02-05 12:25:30 async_llm_engine.py:140] Finished request cmpl-ed727496d18c46bfb34792b5507bb794-0.
INFO 02-05 12:25:30 async_llm_engine.py:140] Finished request cmpl-bd91a954ffcd449390c77532343a4e71-0.
INFO 02-05 12:25:30 async_llm_engine.py:140] Finished request cmpl-97cf7c68d019428398475d1482519621-0.
INFO 02-05 12:25:30 async_llm_engine.py:140] Finished request cmpl-0b132c46b89c4122ba8a181698e8d455-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:30 logger.py:36] Received request cmpl-a7d8991779a84210ad9d3ef9e290df38-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:30 async_llm_engine.py:173] Added request cmpl-a7d8991779a84210ad9d3ef9e290df38-0.
INFO 02-05 12:25:30 logger.py:36] Received request cmpl-349fb0ff38e944eaa27020d03a625d9d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:30 async_llm_engine.py:173] Added request cmpl-349fb0ff38e944eaa27020d03a625d9d-0.
INFO 02-05 12:25:30 logger.py:36] Received request cmpl-3fc2fa5565284b70b433e3558f3b505c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:30 async_llm_engine.py:173] Added request cmpl-3fc2fa5565284b70b433e3558f3b505c-0.
INFO 02-05 12:25:30 logger.py:36] Received request cmpl-c70b311ef9154f2a906e8c8a44f7cf5f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:30 logger.py:36] Received request cmpl-1f3b8d9be23c4db99caceed392ea2b3b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:30 async_llm_engine.py:173] Added request cmpl-c70b311ef9154f2a906e8c8a44f7cf5f-0.
INFO 02-05 12:25:30 async_llm_engine.py:173] Added request cmpl-1f3b8d9be23c4db99caceed392ea2b3b-0.
INFO 02-05 12:25:30 logger.py:36] Received request cmpl-9dcf8e9d720440d3ac0dc6abbad4cc27-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:30 logger.py:36] Received request cmpl-91b2e1408bbb497e80cb34bddb01f556-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:30 async_llm_engine.py:173] Added request cmpl-9dcf8e9d720440d3ac0dc6abbad4cc27-0.
INFO 02-05 12:25:30 async_llm_engine.py:173] Added request cmpl-91b2e1408bbb497e80cb34bddb01f556-0.
INFO 02-05 12:25:30 logger.py:36] Received request cmpl-2f21bb3954cf410882d248b32643a423-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:30 async_llm_engine.py:173] Added request cmpl-2f21bb3954cf410882d248b32643a423-0.
INFO 02-05 12:25:30 logger.py:36] Received request cmpl-f0717edf7e244602a8d6d41bb4afd65b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:30 async_llm_engine.py:173] Added request cmpl-f0717edf7e244602a8d6d41bb4afd65b-0.
INFO 02-05 12:25:31 logger.py:36] Received request cmpl-65c7aa16028d44d19894e825dd6d73d7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:31 async_llm_engine.py:173] Added request cmpl-65c7aa16028d44d19894e825dd6d73d7-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:31 async_llm_engine.py:140] Finished request cmpl-a7d8991779a84210ad9d3ef9e290df38-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:32 async_llm_engine.py:140] Finished request cmpl-349fb0ff38e944eaa27020d03a625d9d-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:32 async_llm_engine.py:140] Finished request cmpl-3fc2fa5565284b70b433e3558f3b505c-0.
INFO 02-05 12:25:32 async_llm_engine.py:140] Finished request cmpl-c70b311ef9154f2a906e8c8a44f7cf5f-0.
INFO 02-05 12:25:32 async_llm_engine.py:140] Finished request cmpl-1f3b8d9be23c4db99caceed392ea2b3b-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:32 async_llm_engine.py:140] Finished request cmpl-9dcf8e9d720440d3ac0dc6abbad4cc27-0.
INFO 02-05 12:25:32 async_llm_engine.py:140] Finished request cmpl-91b2e1408bbb497e80cb34bddb01f556-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:32 async_llm_engine.py:140] Finished request cmpl-2f21bb3954cf410882d248b32643a423-0.
INFO 02-05 12:25:32 async_llm_engine.py:140] Finished request cmpl-f0717edf7e244602a8d6d41bb4afd65b-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:32 async_llm_engine.py:140] Finished request cmpl-65c7aa16028d44d19894e825dd6d73d7-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:32 logger.py:36] Received request cmpl-c9a111ade9a141389f8885ba3a57617b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:32 async_llm_engine.py:173] Added request cmpl-c9a111ade9a141389f8885ba3a57617b-0.
INFO 02-05 12:25:32 logger.py:36] Received request cmpl-231932a272dd431eacd2c436b9a22507-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:32 async_llm_engine.py:173] Added request cmpl-231932a272dd431eacd2c436b9a22507-0.
INFO 02-05 12:25:32 logger.py:36] Received request cmpl-1b4c43f717414eaa8f9bfd10c51b06e5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:32 async_llm_engine.py:173] Added request cmpl-1b4c43f717414eaa8f9bfd10c51b06e5-0.
INFO 02-05 12:25:32 logger.py:36] Received request cmpl-3440c716f8e94abb8394479ad8882dc0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:32 async_llm_engine.py:173] Added request cmpl-3440c716f8e94abb8394479ad8882dc0-0.
INFO 02-05 12:25:32 logger.py:36] Received request cmpl-d85c7de5361341bcb9e61846fc5a9465-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:32 async_llm_engine.py:173] Added request cmpl-d85c7de5361341bcb9e61846fc5a9465-0.
INFO 02-05 12:25:32 logger.py:36] Received request cmpl-8f568505f7594919a26e8e946c269407-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:32 async_llm_engine.py:173] Added request cmpl-8f568505f7594919a26e8e946c269407-0.
INFO 02-05 12:25:32 logger.py:36] Received request cmpl-b77ef4ec4cee488e8e39b1dd60017d2c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:32 async_llm_engine.py:173] Added request cmpl-b77ef4ec4cee488e8e39b1dd60017d2c-0.
INFO 02-05 12:25:32 logger.py:36] Received request cmpl-23ecfe09e9be495487d923fc28a8336a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:32 async_llm_engine.py:173] Added request cmpl-23ecfe09e9be495487d923fc28a8336a-0.
INFO 02-05 12:25:32 logger.py:36] Received request cmpl-c590e8fa3df74d01b64c9072b8597fb5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:32 async_llm_engine.py:173] Added request cmpl-c590e8fa3df74d01b64c9072b8597fb5-0.
INFO 02-05 12:25:32 logger.py:36] Received request cmpl-44c338a2f02a44619223fcb333d7a28d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:32 async_llm_engine.py:173] Added request cmpl-44c338a2f02a44619223fcb333d7a28d-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:33 async_llm_engine.py:140] Finished request cmpl-c9a111ade9a141389f8885ba3a57617b-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:33 async_llm_engine.py:140] Finished request cmpl-231932a272dd431eacd2c436b9a22507-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:33 async_llm_engine.py:140] Finished request cmpl-1b4c43f717414eaa8f9bfd10c51b06e5-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:33 async_llm_engine.py:140] Finished request cmpl-3440c716f8e94abb8394479ad8882dc0-0.
INFO 02-05 12:25:33 async_llm_engine.py:140] Finished request cmpl-d85c7de5361341bcb9e61846fc5a9465-0.
INFO 02-05 12:25:33 async_llm_engine.py:140] Finished request cmpl-8f568505f7594919a26e8e946c269407-0.
INFO 02-05 12:25:33 async_llm_engine.py:140] Finished request cmpl-b77ef4ec4cee488e8e39b1dd60017d2c-0.
INFO 02-05 12:25:33 async_llm_engine.py:140] Finished request cmpl-23ecfe09e9be495487d923fc28a8336a-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:33 async_llm_engine.py:140] Finished request cmpl-c590e8fa3df74d01b64c9072b8597fb5-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:33 async_llm_engine.py:140] Finished request cmpl-44c338a2f02a44619223fcb333d7a28d-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:33 logger.py:36] Received request cmpl-efea027837584c5697c72a4f763a06e7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:33 async_llm_engine.py:173] Added request cmpl-efea027837584c5697c72a4f763a06e7-0.
INFO 02-05 12:25:33 logger.py:36] Received request cmpl-c70927d5999c4d84aac698b35c5bae36-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:33 async_llm_engine.py:173] Added request cmpl-c70927d5999c4d84aac698b35c5bae36-0.
INFO 02-05 12:25:33 logger.py:36] Received request cmpl-b793312164a94b2abff0504ab548137c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:33 logger.py:36] Received request cmpl-a427dcda10024242957ac58e89c1c064-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:33 logger.py:36] Received request cmpl-04d1a3a64e1943328a2317ce795f8be7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:33 logger.py:36] Received request cmpl-da6eae7d87754ca6b431e9d04e78ac02-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:33 logger.py:36] Received request cmpl-b6bcaa0d6f8343c4bd0c178ba7c39bdb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:33 async_llm_engine.py:173] Added request cmpl-b793312164a94b2abff0504ab548137c-0.
INFO 02-05 12:25:33 async_llm_engine.py:173] Added request cmpl-a427dcda10024242957ac58e89c1c064-0.
INFO 02-05 12:25:33 async_llm_engine.py:173] Added request cmpl-04d1a3a64e1943328a2317ce795f8be7-0.
INFO 02-05 12:25:33 async_llm_engine.py:173] Added request cmpl-da6eae7d87754ca6b431e9d04e78ac02-0.
INFO 02-05 12:25:33 async_llm_engine.py:173] Added request cmpl-b6bcaa0d6f8343c4bd0c178ba7c39bdb-0.
INFO 02-05 12:25:33 logger.py:36] Received request cmpl-32254d5faa0046fd8c2962d75a47515c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:33 async_llm_engine.py:173] Added request cmpl-32254d5faa0046fd8c2962d75a47515c-0.
INFO 02-05 12:25:33 logger.py:36] Received request cmpl-3fc345d5b05f489a977b739dd09557e6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:33 async_llm_engine.py:173] Added request cmpl-3fc345d5b05f489a977b739dd09557e6-0.
INFO 02-05 12:25:33 logger.py:36] Received request cmpl-723f45049eb749f4a9e748c448d32892-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:33 async_llm_engine.py:173] Added request cmpl-723f45049eb749f4a9e748c448d32892-0.
INFO 02-05 12:25:33 metrics.py:396] Avg prompt throughput: 71.8 tokens/s, Avg generation throughput: 518.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:34 async_llm_engine.py:140] Finished request cmpl-efea027837584c5697c72a4f763a06e7-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:34 async_llm_engine.py:140] Finished request cmpl-c70927d5999c4d84aac698b35c5bae36-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:34 async_llm_engine.py:140] Finished request cmpl-b793312164a94b2abff0504ab548137c-0.
INFO 02-05 12:25:34 async_llm_engine.py:140] Finished request cmpl-a427dcda10024242957ac58e89c1c064-0.
INFO 02-05 12:25:34 async_llm_engine.py:140] Finished request cmpl-04d1a3a64e1943328a2317ce795f8be7-0.
INFO 02-05 12:25:34 async_llm_engine.py:140] Finished request cmpl-da6eae7d87754ca6b431e9d04e78ac02-0.
INFO 02-05 12:25:34 async_llm_engine.py:140] Finished request cmpl-b6bcaa0d6f8343c4bd0c178ba7c39bdb-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:34 logger.py:36] Received request cmpl-ce814f650b294d41a0d1a1549f49d96b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:34 async_llm_engine.py:173] Added request cmpl-ce814f650b294d41a0d1a1549f49d96b-0.
INFO 02-05 12:25:34 async_llm_engine.py:140] Finished request cmpl-32254d5faa0046fd8c2962d75a47515c-0.
INFO 02-05 12:25:34 async_llm_engine.py:140] Finished request cmpl-3fc345d5b05f489a977b739dd09557e6-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:34 async_llm_engine.py:140] Finished request cmpl-723f45049eb749f4a9e748c448d32892-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:34 logger.py:36] Received request cmpl-178a81d069fc48338d460d9f7627c0f9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:34 async_llm_engine.py:173] Added request cmpl-178a81d069fc48338d460d9f7627c0f9-0.
INFO 02-05 12:25:34 logger.py:36] Received request cmpl-388353f1c5594e3b9256218def6126ef-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:34 async_llm_engine.py:173] Added request cmpl-388353f1c5594e3b9256218def6126ef-0.
INFO 02-05 12:25:34 logger.py:36] Received request cmpl-5729e560d67541f68fe3a4e45b511c8c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:34 async_llm_engine.py:173] Added request cmpl-5729e560d67541f68fe3a4e45b511c8c-0.
INFO 02-05 12:25:34 logger.py:36] Received request cmpl-3d6b710b51d748d6acfcc3167b94d26b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:34 async_llm_engine.py:173] Added request cmpl-3d6b710b51d748d6acfcc3167b94d26b-0.
INFO 02-05 12:25:34 logger.py:36] Received request cmpl-4c088b07f9c041c8a1d785b3eddb7371-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:34 async_llm_engine.py:173] Added request cmpl-4c088b07f9c041c8a1d785b3eddb7371-0.
INFO 02-05 12:25:34 logger.py:36] Received request cmpl-0b075b84a78148118456a0b0f505b39a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:34 async_llm_engine.py:173] Added request cmpl-0b075b84a78148118456a0b0f505b39a-0.
INFO 02-05 12:25:34 logger.py:36] Received request cmpl-447a31762a8b4e3f941df4cfed68d6c8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:34 logger.py:36] Received request cmpl-eb164684c65e44e786ff1b44f585075b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:34 async_llm_engine.py:173] Added request cmpl-447a31762a8b4e3f941df4cfed68d6c8-0.
INFO 02-05 12:25:34 async_llm_engine.py:173] Added request cmpl-eb164684c65e44e786ff1b44f585075b-0.
INFO 02-05 12:25:34 logger.py:36] Received request cmpl-1e82003bea9f47448babe10ba71cefb3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:34 async_llm_engine.py:173] Added request cmpl-1e82003bea9f47448babe10ba71cefb3-0.
INFO 02-05 12:25:35 async_llm_engine.py:140] Finished request cmpl-ce814f650b294d41a0d1a1549f49d96b-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:43588 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:43594 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:25:35 async_llm_engine.py:140] Finished request cmpl-178a81d069fc48338d460d9f7627c0f9-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:36 async_llm_engine.py:140] Finished request cmpl-388353f1c5594e3b9256218def6126ef-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:36 logger.py:36] Received request cmpl-6431b651f20240948e6988001ccf4cc0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:36 async_llm_engine.py:173] Added request cmpl-6431b651f20240948e6988001ccf4cc0-0.
INFO 02-05 12:25:36 async_llm_engine.py:140] Finished request cmpl-5729e560d67541f68fe3a4e45b511c8c-0.
INFO 02-05 12:25:36 async_llm_engine.py:140] Finished request cmpl-3d6b710b51d748d6acfcc3167b94d26b-0.
INFO 02-05 12:25:36 async_llm_engine.py:140] Finished request cmpl-4c088b07f9c041c8a1d785b3eddb7371-0.
INFO 02-05 12:25:36 async_llm_engine.py:140] Finished request cmpl-0b075b84a78148118456a0b0f505b39a-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:36 async_llm_engine.py:140] Finished request cmpl-447a31762a8b4e3f941df4cfed68d6c8-0.
INFO 02-05 12:25:36 async_llm_engine.py:140] Finished request cmpl-eb164684c65e44e786ff1b44f585075b-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:36 async_llm_engine.py:140] Finished request cmpl-1e82003bea9f47448babe10ba71cefb3-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:36 logger.py:36] Received request cmpl-7c1f6e809d094d5ca3eb3a911534c20e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:36 logger.py:36] Received request cmpl-8e74466eaf83491883caca88e90e145e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:36 async_llm_engine.py:173] Added request cmpl-7c1f6e809d094d5ca3eb3a911534c20e-0.
INFO 02-05 12:25:36 async_llm_engine.py:173] Added request cmpl-8e74466eaf83491883caca88e90e145e-0.
INFO 02-05 12:25:36 logger.py:36] Received request cmpl-4ec11b25a3d54c038b6ff39e061d3040-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:36 async_llm_engine.py:173] Added request cmpl-4ec11b25a3d54c038b6ff39e061d3040-0.
INFO 02-05 12:25:36 logger.py:36] Received request cmpl-53352491d4bf4fa0a59f7069d55f3d74-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:36 async_llm_engine.py:173] Added request cmpl-53352491d4bf4fa0a59f7069d55f3d74-0.
INFO 02-05 12:25:36 logger.py:36] Received request cmpl-4e6b78f3306b4b79bb89409a3a300b03-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:36 async_llm_engine.py:173] Added request cmpl-4e6b78f3306b4b79bb89409a3a300b03-0.
INFO 02-05 12:25:36 logger.py:36] Received request cmpl-f0d3ee1f3c874dfd826e7a79ad5c5004-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:36 async_llm_engine.py:173] Added request cmpl-f0d3ee1f3c874dfd826e7a79ad5c5004-0.
INFO 02-05 12:25:36 logger.py:36] Received request cmpl-0eebacb8ee8a464086a35a0f656fb7fe-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:36 async_llm_engine.py:173] Added request cmpl-0eebacb8ee8a464086a35a0f656fb7fe-0.
INFO 02-05 12:25:36 logger.py:36] Received request cmpl-8929fe63f9074cd087c9d564e0bd4cd7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:36 async_llm_engine.py:173] Added request cmpl-8929fe63f9074cd087c9d564e0bd4cd7-0.
INFO 02-05 12:25:36 logger.py:36] Received request cmpl-1738b0ea72164472b09f7f8c7fbeca89-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:36 async_llm_engine.py:173] Added request cmpl-1738b0ea72164472b09f7f8c7fbeca89-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:37 async_llm_engine.py:140] Finished request cmpl-6431b651f20240948e6988001ccf4cc0-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:37 async_llm_engine.py:140] Finished request cmpl-7c1f6e809d094d5ca3eb3a911534c20e-0.
INFO 02-05 12:25:37 async_llm_engine.py:140] Finished request cmpl-8e74466eaf83491883caca88e90e145e-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:37 async_llm_engine.py:140] Finished request cmpl-4ec11b25a3d54c038b6ff39e061d3040-0.
INFO 02-05 12:25:37 async_llm_engine.py:140] Finished request cmpl-53352491d4bf4fa0a59f7069d55f3d74-0.
INFO 02-05 12:25:37 async_llm_engine.py:140] Finished request cmpl-4e6b78f3306b4b79bb89409a3a300b03-0.
INFO 02-05 12:25:37 async_llm_engine.py:140] Finished request cmpl-f0d3ee1f3c874dfd826e7a79ad5c5004-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:37 logger.py:36] Received request cmpl-3d57b1c9ab234b75b4159c7701eeb860-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:37 async_llm_engine.py:173] Added request cmpl-3d57b1c9ab234b75b4159c7701eeb860-0.
INFO 02-05 12:25:37 async_llm_engine.py:140] Finished request cmpl-0eebacb8ee8a464086a35a0f656fb7fe-0.
INFO 02-05 12:25:37 async_llm_engine.py:140] Finished request cmpl-8929fe63f9074cd087c9d564e0bd4cd7-0.
INFO 02-05 12:25:37 async_llm_engine.py:140] Finished request cmpl-1738b0ea72164472b09f7f8c7fbeca89-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:37 logger.py:36] Received request cmpl-e21d04c1f70046e1b85cc362a3308ff3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:37 async_llm_engine.py:173] Added request cmpl-e21d04c1f70046e1b85cc362a3308ff3-0.
INFO 02-05 12:25:37 logger.py:36] Received request cmpl-26d570b99d3447ae834bbe5e6bf7d41f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:37 async_llm_engine.py:173] Added request cmpl-26d570b99d3447ae834bbe5e6bf7d41f-0.
INFO 02-05 12:25:37 logger.py:36] Received request cmpl-d684ac6ae2cb45bdbb0656e4bd330793-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:37 async_llm_engine.py:173] Added request cmpl-d684ac6ae2cb45bdbb0656e4bd330793-0.
INFO 02-05 12:25:37 logger.py:36] Received request cmpl-568b1861c4574787bec142b50e47f202-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:37 async_llm_engine.py:173] Added request cmpl-568b1861c4574787bec142b50e47f202-0.
INFO 02-05 12:25:37 logger.py:36] Received request cmpl-fccc8fb7615448fcbb4063e4d4e78ca7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:37 async_llm_engine.py:173] Added request cmpl-fccc8fb7615448fcbb4063e4d4e78ca7-0.
INFO 02-05 12:25:37 logger.py:36] Received request cmpl-b73640dc31934cd48bf72c6434ae8aad-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:37 async_llm_engine.py:173] Added request cmpl-b73640dc31934cd48bf72c6434ae8aad-0.
INFO 02-05 12:25:37 logger.py:36] Received request cmpl-dde4a9f63f6d49e0afb8759392821d7b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:37 async_llm_engine.py:173] Added request cmpl-dde4a9f63f6d49e0afb8759392821d7b-0.
INFO 02-05 12:25:37 logger.py:36] Received request cmpl-338c6dc41fb74f599088d0dd7c4c4db1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:37 async_llm_engine.py:173] Added request cmpl-338c6dc41fb74f599088d0dd7c4c4db1-0.
INFO 02-05 12:25:37 logger.py:36] Received request cmpl-571ff59e05c04c01ae7cf89c664fdcfe-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:37 async_llm_engine.py:173] Added request cmpl-571ff59e05c04c01ae7cf89c664fdcfe-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:38 async_llm_engine.py:140] Finished request cmpl-3d57b1c9ab234b75b4159c7701eeb860-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:38 async_llm_engine.py:140] Finished request cmpl-e21d04c1f70046e1b85cc362a3308ff3-0.
INFO 02-05 12:25:38 async_llm_engine.py:140] Finished request cmpl-26d570b99d3447ae834bbe5e6bf7d41f-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:38 async_llm_engine.py:140] Finished request cmpl-d684ac6ae2cb45bdbb0656e4bd330793-0.
INFO 02-05 12:25:38 async_llm_engine.py:140] Finished request cmpl-568b1861c4574787bec142b50e47f202-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:38 logger.py:36] Received request cmpl-61f42088ad474f8db4a150e627cdc414-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:38 async_llm_engine.py:173] Added request cmpl-61f42088ad474f8db4a150e627cdc414-0.
INFO 02-05 12:25:38 async_llm_engine.py:140] Finished request cmpl-fccc8fb7615448fcbb4063e4d4e78ca7-0.
INFO 02-05 12:25:38 async_llm_engine.py:140] Finished request cmpl-b73640dc31934cd48bf72c6434ae8aad-0.
INFO 02-05 12:25:38 async_llm_engine.py:140] Finished request cmpl-dde4a9f63f6d49e0afb8759392821d7b-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:38 async_llm_engine.py:140] Finished request cmpl-338c6dc41fb74f599088d0dd7c4c4db1-0.
INFO 02-05 12:25:38 async_llm_engine.py:140] Finished request cmpl-571ff59e05c04c01ae7cf89c664fdcfe-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:38 logger.py:36] Received request cmpl-cf46bae8943649e2974464af451571d2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:38 async_llm_engine.py:173] Added request cmpl-cf46bae8943649e2974464af451571d2-0.
INFO 02-05 12:25:38 logger.py:36] Received request cmpl-f487a1e4cb9d4a2f918fa8d389729daa-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:38 async_llm_engine.py:173] Added request cmpl-f487a1e4cb9d4a2f918fa8d389729daa-0.
INFO 02-05 12:25:38 logger.py:36] Received request cmpl-ade33356bb6248698805219464bc8664-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:38 async_llm_engine.py:173] Added request cmpl-ade33356bb6248698805219464bc8664-0.
INFO 02-05 12:25:38 logger.py:36] Received request cmpl-f6881e89bdde48649519e664daace007-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:38 async_llm_engine.py:173] Added request cmpl-f6881e89bdde48649519e664daace007-0.
INFO 02-05 12:25:38 metrics.py:396] Avg prompt throughput: 57.6 tokens/s, Avg generation throughput: 498.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:25:38 logger.py:36] Received request cmpl-336e81b22fd34d47b11dbab41b918978-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:38 async_llm_engine.py:173] Added request cmpl-336e81b22fd34d47b11dbab41b918978-0.
INFO 02-05 12:25:38 logger.py:36] Received request cmpl-42b701378dac48d68c3cfed231c386b3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:38 async_llm_engine.py:173] Added request cmpl-42b701378dac48d68c3cfed231c386b3-0.
INFO 02-05 12:25:38 logger.py:36] Received request cmpl-6f1b566bac10496c9db48de6f6e0b51f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:38 async_llm_engine.py:173] Added request cmpl-6f1b566bac10496c9db48de6f6e0b51f-0.
INFO 02-05 12:25:38 logger.py:36] Received request cmpl-28c696a83b3e4b6092fc9fd170bb6732-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:38 async_llm_engine.py:173] Added request cmpl-28c696a83b3e4b6092fc9fd170bb6732-0.
INFO 02-05 12:25:38 logger.py:36] Received request cmpl-b8f2a1261342410a80b1694f44ed5291-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:38 async_llm_engine.py:173] Added request cmpl-b8f2a1261342410a80b1694f44ed5291-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:39 async_llm_engine.py:140] Finished request cmpl-61f42088ad474f8db4a150e627cdc414-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:39 async_llm_engine.py:140] Finished request cmpl-cf46bae8943649e2974464af451571d2-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:39 async_llm_engine.py:140] Finished request cmpl-f487a1e4cb9d4a2f918fa8d389729daa-0.
INFO 02-05 12:25:39 async_llm_engine.py:140] Finished request cmpl-ade33356bb6248698805219464bc8664-0.
INFO 02-05 12:25:39 async_llm_engine.py:140] Finished request cmpl-f6881e89bdde48649519e664daace007-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:39 async_llm_engine.py:140] Finished request cmpl-336e81b22fd34d47b11dbab41b918978-0.
INFO 02-05 12:25:39 async_llm_engine.py:140] Finished request cmpl-42b701378dac48d68c3cfed231c386b3-0.
INFO 02-05 12:25:39 async_llm_engine.py:140] Finished request cmpl-6f1b566bac10496c9db48de6f6e0b51f-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:39 logger.py:36] Received request cmpl-4d8de17029034788b566692cd673c740-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:39 async_llm_engine.py:173] Added request cmpl-4d8de17029034788b566692cd673c740-0.
INFO 02-05 12:25:39 async_llm_engine.py:140] Finished request cmpl-28c696a83b3e4b6092fc9fd170bb6732-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:39 async_llm_engine.py:140] Finished request cmpl-b8f2a1261342410a80b1694f44ed5291-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:39 logger.py:36] Received request cmpl-82aacca7731c4923893f2fc27a8604aa-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:39 async_llm_engine.py:173] Added request cmpl-82aacca7731c4923893f2fc27a8604aa-0.
INFO 02-05 12:25:39 logger.py:36] Received request cmpl-99d8568733db4774bce673ef26c3c957-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:39 async_llm_engine.py:173] Added request cmpl-99d8568733db4774bce673ef26c3c957-0.
INFO 02-05 12:25:39 logger.py:36] Received request cmpl-57f3d2bb1bcd4f0f86eb7c6770d82b99-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:39 async_llm_engine.py:173] Added request cmpl-57f3d2bb1bcd4f0f86eb7c6770d82b99-0.
INFO 02-05 12:25:39 logger.py:36] Received request cmpl-926d9982567743d4bde2c26cdfb51c24-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:39 async_llm_engine.py:173] Added request cmpl-926d9982567743d4bde2c26cdfb51c24-0.
INFO 02-05 12:25:39 logger.py:36] Received request cmpl-3c2c68cad7be44368f08f586510dee67-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:39 logger.py:36] Received request cmpl-2360dc69b1674031962e67535a68202f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:39 async_llm_engine.py:173] Added request cmpl-3c2c68cad7be44368f08f586510dee67-0.
INFO 02-05 12:25:39 async_llm_engine.py:173] Added request cmpl-2360dc69b1674031962e67535a68202f-0.
INFO 02-05 12:25:39 logger.py:36] Received request cmpl-19c1a77c3a3743b88404de1eadd8b726-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:39 async_llm_engine.py:173] Added request cmpl-19c1a77c3a3743b88404de1eadd8b726-0.
INFO 02-05 12:25:39 logger.py:36] Received request cmpl-7ad37b276415460cb9693693b11584bc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:39 async_llm_engine.py:173] Added request cmpl-7ad37b276415460cb9693693b11584bc-0.
INFO 02-05 12:25:39 logger.py:36] Received request cmpl-c061c18506fe4ee69af0dc1517e1a2fd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:39 async_llm_engine.py:173] Added request cmpl-c061c18506fe4ee69af0dc1517e1a2fd-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:40 async_llm_engine.py:140] Finished request cmpl-4d8de17029034788b566692cd673c740-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:40 async_llm_engine.py:140] Finished request cmpl-82aacca7731c4923893f2fc27a8604aa-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:40 async_llm_engine.py:140] Finished request cmpl-99d8568733db4774bce673ef26c3c957-0.
INFO 02-05 12:25:40 async_llm_engine.py:140] Finished request cmpl-57f3d2bb1bcd4f0f86eb7c6770d82b99-0.
INFO 02-05 12:25:40 async_llm_engine.py:140] Finished request cmpl-926d9982567743d4bde2c26cdfb51c24-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:40 async_llm_engine.py:140] Finished request cmpl-3c2c68cad7be44368f08f586510dee67-0.
INFO 02-05 12:25:40 async_llm_engine.py:140] Finished request cmpl-2360dc69b1674031962e67535a68202f-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:40 async_llm_engine.py:140] Finished request cmpl-19c1a77c3a3743b88404de1eadd8b726-0.
INFO 02-05 12:25:40 async_llm_engine.py:140] Finished request cmpl-7ad37b276415460cb9693693b11584bc-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:41 async_llm_engine.py:140] Finished request cmpl-c061c18506fe4ee69af0dc1517e1a2fd-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:41 logger.py:36] Received request cmpl-3462330650b040a98aeb75d38ed90050-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:41 async_llm_engine.py:173] Added request cmpl-3462330650b040a98aeb75d38ed90050-0.
INFO 02-05 12:25:41 logger.py:36] Received request cmpl-e9b7c890fcb942df813289f802257e1c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:41 async_llm_engine.py:173] Added request cmpl-e9b7c890fcb942df813289f802257e1c-0.
INFO 02-05 12:25:41 logger.py:36] Received request cmpl-d1101d7f92e548099ba269c67a3dbd64-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:41 async_llm_engine.py:173] Added request cmpl-d1101d7f92e548099ba269c67a3dbd64-0.
INFO 02-05 12:25:41 logger.py:36] Received request cmpl-fd7237cb21d34a6bb5e2e7cc0b283484-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:41 async_llm_engine.py:173] Added request cmpl-fd7237cb21d34a6bb5e2e7cc0b283484-0.
INFO 02-05 12:25:41 logger.py:36] Received request cmpl-673b5a22c4084fffbee689846eef0c59-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:41 async_llm_engine.py:173] Added request cmpl-673b5a22c4084fffbee689846eef0c59-0.
INFO 02-05 12:25:41 logger.py:36] Received request cmpl-33e99fcdd2b04ec3b2560813410e20a5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:41 async_llm_engine.py:173] Added request cmpl-33e99fcdd2b04ec3b2560813410e20a5-0.
INFO 02-05 12:25:41 logger.py:36] Received request cmpl-eddc5af4332040c1af18f680ef660edc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:41 async_llm_engine.py:173] Added request cmpl-eddc5af4332040c1af18f680ef660edc-0.
INFO 02-05 12:25:41 logger.py:36] Received request cmpl-259ba7027a0b4f9aafb5e993d135a059-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:41 async_llm_engine.py:173] Added request cmpl-259ba7027a0b4f9aafb5e993d135a059-0.
INFO 02-05 12:25:41 logger.py:36] Received request cmpl-98de7510c86a4529aa57e9f2d609c852-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:41 async_llm_engine.py:173] Added request cmpl-98de7510c86a4529aa57e9f2d609c852-0.
INFO 02-05 12:25:41 logger.py:36] Received request cmpl-c93053388b56441ea3b63eaccdc548ae-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:41 async_llm_engine.py:173] Added request cmpl-c93053388b56441ea3b63eaccdc548ae-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:42 async_llm_engine.py:140] Finished request cmpl-3462330650b040a98aeb75d38ed90050-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:42 async_llm_engine.py:140] Finished request cmpl-e9b7c890fcb942df813289f802257e1c-0.
INFO 02-05 12:25:42 async_llm_engine.py:140] Finished request cmpl-d1101d7f92e548099ba269c67a3dbd64-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:42 async_llm_engine.py:140] Finished request cmpl-fd7237cb21d34a6bb5e2e7cc0b283484-0.
INFO 02-05 12:25:42 async_llm_engine.py:140] Finished request cmpl-673b5a22c4084fffbee689846eef0c59-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:42 async_llm_engine.py:140] Finished request cmpl-33e99fcdd2b04ec3b2560813410e20a5-0.
INFO 02-05 12:25:42 async_llm_engine.py:140] Finished request cmpl-eddc5af4332040c1af18f680ef660edc-0.
INFO 02-05 12:25:42 async_llm_engine.py:140] Finished request cmpl-259ba7027a0b4f9aafb5e993d135a059-0.
INFO 02-05 12:25:42 async_llm_engine.py:140] Finished request cmpl-98de7510c86a4529aa57e9f2d609c852-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:42 async_llm_engine.py:140] Finished request cmpl-c93053388b56441ea3b63eaccdc548ae-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:42 logger.py:36] Received request cmpl-88204073bbbe414683f94cbc7b86a895-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:42 async_llm_engine.py:173] Added request cmpl-88204073bbbe414683f94cbc7b86a895-0.
INFO 02-05 12:25:42 logger.py:36] Received request cmpl-0898e6ae23e3471ab979a3860a87ee4a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:42 async_llm_engine.py:173] Added request cmpl-0898e6ae23e3471ab979a3860a87ee4a-0.
INFO 02-05 12:25:42 logger.py:36] Received request cmpl-62e161f87b3046879147c34470928750-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:42 async_llm_engine.py:173] Added request cmpl-62e161f87b3046879147c34470928750-0.
INFO 02-05 12:25:42 logger.py:36] Received request cmpl-84b08935b0c34896b01c1cf7f6d673c2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:42 async_llm_engine.py:173] Added request cmpl-84b08935b0c34896b01c1cf7f6d673c2-0.
INFO 02-05 12:25:42 logger.py:36] Received request cmpl-94ded3991b804499b9bb56509c52606f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:42 async_llm_engine.py:173] Added request cmpl-94ded3991b804499b9bb56509c52606f-0.
INFO 02-05 12:25:42 logger.py:36] Received request cmpl-be2dc7442a8d4c29b7af5d4dd45e132a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:42 async_llm_engine.py:173] Added request cmpl-be2dc7442a8d4c29b7af5d4dd45e132a-0.
INFO 02-05 12:25:42 logger.py:36] Received request cmpl-97ae32e3477340cabba181d44e9c69de-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:42 logger.py:36] Received request cmpl-c6b48fc6276342e1b5cdec4b239ca68d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:42 async_llm_engine.py:173] Added request cmpl-97ae32e3477340cabba181d44e9c69de-0.
INFO 02-05 12:25:42 async_llm_engine.py:173] Added request cmpl-c6b48fc6276342e1b5cdec4b239ca68d-0.
INFO 02-05 12:25:42 logger.py:36] Received request cmpl-98cd5f4a71fa4765be00b9cc35229d65-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:42 async_llm_engine.py:173] Added request cmpl-98cd5f4a71fa4765be00b9cc35229d65-0.
INFO 02-05 12:25:42 logger.py:36] Received request cmpl-c2d1b872c8bc4550be47de0ed70e2895-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:42 async_llm_engine.py:173] Added request cmpl-c2d1b872c8bc4550be47de0ed70e2895-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:43 async_llm_engine.py:140] Finished request cmpl-88204073bbbe414683f94cbc7b86a895-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:43 async_llm_engine.py:140] Finished request cmpl-0898e6ae23e3471ab979a3860a87ee4a-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:43 async_llm_engine.py:140] Finished request cmpl-62e161f87b3046879147c34470928750-0.
INFO 02-05 12:25:43 async_llm_engine.py:140] Finished request cmpl-84b08935b0c34896b01c1cf7f6d673c2-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:43 async_llm_engine.py:140] Finished request cmpl-94ded3991b804499b9bb56509c52606f-0.
INFO 02-05 12:25:43 async_llm_engine.py:140] Finished request cmpl-be2dc7442a8d4c29b7af5d4dd45e132a-0.
INFO 02-05 12:25:43 async_llm_engine.py:140] Finished request cmpl-97ae32e3477340cabba181d44e9c69de-0.
INFO 02-05 12:25:43 async_llm_engine.py:140] Finished request cmpl-c6b48fc6276342e1b5cdec4b239ca68d-0.
INFO 02-05 12:25:43 async_llm_engine.py:140] Finished request cmpl-98cd5f4a71fa4765be00b9cc35229d65-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:43 async_llm_engine.py:140] Finished request cmpl-c2d1b872c8bc4550be47de0ed70e2895-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:43 logger.py:36] Received request cmpl-78ba992a9b8f4c1ea45589423d20f9e2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:43 async_llm_engine.py:173] Added request cmpl-78ba992a9b8f4c1ea45589423d20f9e2-0.
INFO 02-05 12:25:43 logger.py:36] Received request cmpl-7b2bc5fd73c949de8f716e5419d0352d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:43 async_llm_engine.py:173] Added request cmpl-7b2bc5fd73c949de8f716e5419d0352d-0.
INFO 02-05 12:25:43 logger.py:36] Received request cmpl-69ae4a0f58094691adfbf3fa423a1147-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:43 async_llm_engine.py:173] Added request cmpl-69ae4a0f58094691adfbf3fa423a1147-0.
INFO 02-05 12:25:43 logger.py:36] Received request cmpl-e4dfcf87d9374afb911a89909fe187a5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:43 async_llm_engine.py:173] Added request cmpl-e4dfcf87d9374afb911a89909fe187a5-0.
INFO 02-05 12:25:43 logger.py:36] Received request cmpl-572326b134a04c179b0cf58d20e7e28c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:43 async_llm_engine.py:173] Added request cmpl-572326b134a04c179b0cf58d20e7e28c-0.
INFO 02-05 12:25:43 logger.py:36] Received request cmpl-75a30d63dad64eb99d4df9c7370cad36-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:43 async_llm_engine.py:173] Added request cmpl-75a30d63dad64eb99d4df9c7370cad36-0.
INFO 02-05 12:25:43 logger.py:36] Received request cmpl-2c12bcf1fce1418da7192a7e963b58ac-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:43 async_llm_engine.py:173] Added request cmpl-2c12bcf1fce1418da7192a7e963b58ac-0.
INFO 02-05 12:25:43 logger.py:36] Received request cmpl-f78132ba296b47d084bb9aac18033f29-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:43 async_llm_engine.py:173] Added request cmpl-f78132ba296b47d084bb9aac18033f29-0.
INFO 02-05 12:25:43 logger.py:36] Received request cmpl-30b91b97e7504c16945421d397b6359d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:43 async_llm_engine.py:173] Added request cmpl-30b91b97e7504c16945421d397b6359d-0.
INFO 02-05 12:25:43 logger.py:36] Received request cmpl-7c4cf315a91f412ea5c3ac515a61808e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:43 async_llm_engine.py:173] Added request cmpl-7c4cf315a91f412ea5c3ac515a61808e-0.
INFO 02-05 12:25:43 metrics.py:396] Avg prompt throughput: 82.6 tokens/s, Avg generation throughput: 514.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:44 async_llm_engine.py:140] Finished request cmpl-78ba992a9b8f4c1ea45589423d20f9e2-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:44 async_llm_engine.py:140] Finished request cmpl-7b2bc5fd73c949de8f716e5419d0352d-0.
INFO 02-05 12:25:44 async_llm_engine.py:140] Finished request cmpl-69ae4a0f58094691adfbf3fa423a1147-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:44 async_llm_engine.py:140] Finished request cmpl-e4dfcf87d9374afb911a89909fe187a5-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:44 async_llm_engine.py:140] Finished request cmpl-572326b134a04c179b0cf58d20e7e28c-0.
INFO 02-05 12:25:44 async_llm_engine.py:140] Finished request cmpl-75a30d63dad64eb99d4df9c7370cad36-0.
INFO 02-05 12:25:44 async_llm_engine.py:140] Finished request cmpl-2c12bcf1fce1418da7192a7e963b58ac-0.
INFO 02-05 12:25:44 async_llm_engine.py:140] Finished request cmpl-f78132ba296b47d084bb9aac18033f29-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:44 async_llm_engine.py:140] Finished request cmpl-30b91b97e7504c16945421d397b6359d-0.
INFO 02-05 12:25:44 async_llm_engine.py:140] Finished request cmpl-7c4cf315a91f412ea5c3ac515a61808e-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:44 logger.py:36] Received request cmpl-d79ae0689bf849efab16aded60cb1898-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:44 async_llm_engine.py:173] Added request cmpl-d79ae0689bf849efab16aded60cb1898-0.
INFO 02-05 12:25:44 logger.py:36] Received request cmpl-320e0ce5730b4d0b89ffe585b2ad623e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:44 async_llm_engine.py:173] Added request cmpl-320e0ce5730b4d0b89ffe585b2ad623e-0.
INFO 02-05 12:25:44 logger.py:36] Received request cmpl-50fb1af5eb264f4c85c8fc9a4c1918ec-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:44 async_llm_engine.py:173] Added request cmpl-50fb1af5eb264f4c85c8fc9a4c1918ec-0.
INFO 02-05 12:25:44 logger.py:36] Received request cmpl-87a8601a940745b5908fd637e7b5fc52-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:44 async_llm_engine.py:173] Added request cmpl-87a8601a940745b5908fd637e7b5fc52-0.
INFO 02-05 12:25:44 logger.py:36] Received request cmpl-3383153cf04e4f799bcfe8c598cc570e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:44 async_llm_engine.py:173] Added request cmpl-3383153cf04e4f799bcfe8c598cc570e-0.
INFO 02-05 12:25:44 logger.py:36] Received request cmpl-ef1b2927c3dc44b6b3c5e3d446503e2c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:44 async_llm_engine.py:173] Added request cmpl-ef1b2927c3dc44b6b3c5e3d446503e2c-0.
INFO 02-05 12:25:44 logger.py:36] Received request cmpl-c21bbb9a96ae441c921d3b7dd9dc4101-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:44 async_llm_engine.py:173] Added request cmpl-c21bbb9a96ae441c921d3b7dd9dc4101-0.
INFO 02-05 12:25:44 logger.py:36] Received request cmpl-425d74f71efa4e93aead94dcfcad26c9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:44 async_llm_engine.py:173] Added request cmpl-425d74f71efa4e93aead94dcfcad26c9-0.
INFO 02-05 12:25:44 logger.py:36] Received request cmpl-180ae1e7e54b4e0b8d618729a98ff3b6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:44 async_llm_engine.py:173] Added request cmpl-180ae1e7e54b4e0b8d618729a98ff3b6-0.
INFO 02-05 12:25:44 logger.py:36] Received request cmpl-bbc9c8dcfca74161b5efd799c4c19c00-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:44 async_llm_engine.py:173] Added request cmpl-bbc9c8dcfca74161b5efd799c4c19c00-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:45 async_llm_engine.py:140] Finished request cmpl-d79ae0689bf849efab16aded60cb1898-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:45 async_llm_engine.py:140] Finished request cmpl-320e0ce5730b4d0b89ffe585b2ad623e-0.
INFO 02-05 12:25:45 async_llm_engine.py:140] Finished request cmpl-50fb1af5eb264f4c85c8fc9a4c1918ec-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:45 async_llm_engine.py:140] Finished request cmpl-87a8601a940745b5908fd637e7b5fc52-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:45 async_llm_engine.py:140] Finished request cmpl-3383153cf04e4f799bcfe8c598cc570e-0.
INFO 02-05 12:25:45 async_llm_engine.py:140] Finished request cmpl-ef1b2927c3dc44b6b3c5e3d446503e2c-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:45 async_llm_engine.py:140] Finished request cmpl-c21bbb9a96ae441c921d3b7dd9dc4101-0.
INFO 02-05 12:25:45 async_llm_engine.py:140] Finished request cmpl-425d74f71efa4e93aead94dcfcad26c9-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:45 async_llm_engine.py:140] Finished request cmpl-180ae1e7e54b4e0b8d618729a98ff3b6-0.
INFO 02-05 12:25:45 async_llm_engine.py:140] Finished request cmpl-bbc9c8dcfca74161b5efd799c4c19c00-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     89.105.200.105:58746 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:58758 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:25:46 logger.py:36] Received request cmpl-17ee176082744ed09b08bad79f8e4fef-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:46 async_llm_engine.py:173] Added request cmpl-17ee176082744ed09b08bad79f8e4fef-0.
INFO 02-05 12:25:46 logger.py:36] Received request cmpl-871682627e53496b83fa88bb53ce25f3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:46 async_llm_engine.py:173] Added request cmpl-871682627e53496b83fa88bb53ce25f3-0.
INFO 02-05 12:25:46 logger.py:36] Received request cmpl-72842a8129ad43188e1b49a5ce4495c2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:46 async_llm_engine.py:173] Added request cmpl-72842a8129ad43188e1b49a5ce4495c2-0.
INFO 02-05 12:25:46 logger.py:36] Received request cmpl-84eab9ed8cd74e009491bb0a229ec7b4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:46 async_llm_engine.py:173] Added request cmpl-84eab9ed8cd74e009491bb0a229ec7b4-0.
INFO 02-05 12:25:46 logger.py:36] Received request cmpl-79fe3a07dc45465eb0f00daede22fa14-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:46 async_llm_engine.py:173] Added request cmpl-79fe3a07dc45465eb0f00daede22fa14-0.
INFO 02-05 12:25:46 logger.py:36] Received request cmpl-0e216adcc9904f1a8111d255ca126512-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:46 async_llm_engine.py:173] Added request cmpl-0e216adcc9904f1a8111d255ca126512-0.
INFO 02-05 12:25:46 logger.py:36] Received request cmpl-7b10126b97544764b02d54734eef09ff-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:46 async_llm_engine.py:173] Added request cmpl-7b10126b97544764b02d54734eef09ff-0.
INFO 02-05 12:25:46 logger.py:36] Received request cmpl-58cce518b8624bf6b3f69b0e24c6c288-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:46 async_llm_engine.py:173] Added request cmpl-58cce518b8624bf6b3f69b0e24c6c288-0.
INFO 02-05 12:25:46 logger.py:36] Received request cmpl-27a2fe2abee84d349262b4788b84d4ce-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:46 logger.py:36] Received request cmpl-c4ebbed8594f49b5ac5b3bf58121953c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:46 async_llm_engine.py:173] Added request cmpl-27a2fe2abee84d349262b4788b84d4ce-0.
INFO 02-05 12:25:46 async_llm_engine.py:173] Added request cmpl-c4ebbed8594f49b5ac5b3bf58121953c-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:47 async_llm_engine.py:140] Finished request cmpl-17ee176082744ed09b08bad79f8e4fef-0.
INFO 02-05 12:25:47 async_llm_engine.py:140] Finished request cmpl-871682627e53496b83fa88bb53ce25f3-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:47 async_llm_engine.py:140] Finished request cmpl-72842a8129ad43188e1b49a5ce4495c2-0.
INFO 02-05 12:25:47 async_llm_engine.py:140] Finished request cmpl-84eab9ed8cd74e009491bb0a229ec7b4-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:47 async_llm_engine.py:140] Finished request cmpl-79fe3a07dc45465eb0f00daede22fa14-0.
INFO 02-05 12:25:47 async_llm_engine.py:140] Finished request cmpl-0e216adcc9904f1a8111d255ca126512-0.
INFO 02-05 12:25:47 async_llm_engine.py:140] Finished request cmpl-7b10126b97544764b02d54734eef09ff-0.
INFO 02-05 12:25:47 async_llm_engine.py:140] Finished request cmpl-58cce518b8624bf6b3f69b0e24c6c288-0.
INFO 02-05 12:25:47 async_llm_engine.py:140] Finished request cmpl-27a2fe2abee84d349262b4788b84d4ce-0.
INFO 02-05 12:25:47 async_llm_engine.py:140] Finished request cmpl-c4ebbed8594f49b5ac5b3bf58121953c-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:47 logger.py:36] Received request cmpl-2c717d134e954d3f965366b13b3d41d8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:47 async_llm_engine.py:173] Added request cmpl-2c717d134e954d3f965366b13b3d41d8-0.
INFO 02-05 12:25:47 logger.py:36] Received request cmpl-34fae0dc4793486a8cf5333f39fc0714-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:47 async_llm_engine.py:173] Added request cmpl-34fae0dc4793486a8cf5333f39fc0714-0.
INFO 02-05 12:25:47 logger.py:36] Received request cmpl-12dc48e5ce4e4675b3a9993bd7ecc1ee-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:47 async_llm_engine.py:173] Added request cmpl-12dc48e5ce4e4675b3a9993bd7ecc1ee-0.
INFO:     192.168.200.241:45920 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:47 logger.py:36] Received request cmpl-69cf6c1baa9f403399a774663d912e72-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:47 async_llm_engine.py:173] Added request cmpl-69cf6c1baa9f403399a774663d912e72-0.
INFO 02-05 12:25:47 logger.py:36] Received request cmpl-6792f00e0e934c7697fdc3c2383f0abc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:47 async_llm_engine.py:173] Added request cmpl-6792f00e0e934c7697fdc3c2383f0abc-0.
INFO 02-05 12:25:47 logger.py:36] Received request cmpl-435fcdaf9c9047b2a02d69a2a488e400-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:47 async_llm_engine.py:173] Added request cmpl-435fcdaf9c9047b2a02d69a2a488e400-0.
INFO 02-05 12:25:47 logger.py:36] Received request cmpl-69c6fa05d8a74c179fbd05f5a6ee1116-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:47 async_llm_engine.py:173] Added request cmpl-69c6fa05d8a74c179fbd05f5a6ee1116-0.
INFO 02-05 12:25:47 logger.py:36] Received request cmpl-8fc3d85a50d14b5b9e3845a7885e0bab-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:47 async_llm_engine.py:173] Added request cmpl-8fc3d85a50d14b5b9e3845a7885e0bab-0.
INFO 02-05 12:25:47 logger.py:36] Received request cmpl-80793b4b91894f3db317f884f4cbbea3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:47 async_llm_engine.py:173] Added request cmpl-80793b4b91894f3db317f884f4cbbea3-0.
INFO 02-05 12:25:47 logger.py:36] Received request cmpl-7a21c0771ef44a519715c45da3550cc7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:47 async_llm_engine.py:173] Added request cmpl-7a21c0771ef44a519715c45da3550cc7-0.
INFO 02-05 12:25:48 async_llm_engine.py:140] Finished request cmpl-2c717d134e954d3f965366b13b3d41d8-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:48 async_llm_engine.py:140] Finished request cmpl-34fae0dc4793486a8cf5333f39fc0714-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:48 async_llm_engine.py:140] Finished request cmpl-12dc48e5ce4e4675b3a9993bd7ecc1ee-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:48 async_llm_engine.py:140] Finished request cmpl-69cf6c1baa9f403399a774663d912e72-0.
INFO 02-05 12:25:48 async_llm_engine.py:140] Finished request cmpl-6792f00e0e934c7697fdc3c2383f0abc-0.
INFO 02-05 12:25:48 async_llm_engine.py:140] Finished request cmpl-435fcdaf9c9047b2a02d69a2a488e400-0.
INFO 02-05 12:25:48 async_llm_engine.py:140] Finished request cmpl-69c6fa05d8a74c179fbd05f5a6ee1116-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:48 async_llm_engine.py:140] Finished request cmpl-8fc3d85a50d14b5b9e3845a7885e0bab-0.
INFO 02-05 12:25:48 async_llm_engine.py:140] Finished request cmpl-80793b4b91894f3db317f884f4cbbea3-0.
INFO 02-05 12:25:48 async_llm_engine.py:140] Finished request cmpl-7a21c0771ef44a519715c45da3550cc7-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:48 logger.py:36] Received request cmpl-256455117360465f95bb09a315189b13-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:48 async_llm_engine.py:173] Added request cmpl-256455117360465f95bb09a315189b13-0.
INFO 02-05 12:25:48 logger.py:36] Received request cmpl-018b4bb34ad0412b80fbe4b52589800d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:48 async_llm_engine.py:173] Added request cmpl-018b4bb34ad0412b80fbe4b52589800d-0.
INFO 02-05 12:25:48 logger.py:36] Received request cmpl-4f8eaf7b8ad34f289b9d7af9b0b04f81-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:48 async_llm_engine.py:173] Added request cmpl-4f8eaf7b8ad34f289b9d7af9b0b04f81-0.
INFO 02-05 12:25:48 logger.py:36] Received request cmpl-4b61d4549d1b48f2ad90c461283e64f2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:48 async_llm_engine.py:173] Added request cmpl-4b61d4549d1b48f2ad90c461283e64f2-0.
INFO 02-05 12:25:48 logger.py:36] Received request cmpl-c85b635bbeeb48adbcbb7203b2c62590-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:48 async_llm_engine.py:173] Added request cmpl-c85b635bbeeb48adbcbb7203b2c62590-0.
INFO 02-05 12:25:48 logger.py:36] Received request cmpl-0e3ed9d95d3c4dada96431d294b503bf-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:48 async_llm_engine.py:173] Added request cmpl-0e3ed9d95d3c4dada96431d294b503bf-0.
INFO 02-05 12:25:48 logger.py:36] Received request cmpl-19bc91ec33f64d87806428edb632313a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:48 async_llm_engine.py:173] Added request cmpl-19bc91ec33f64d87806428edb632313a-0.
INFO 02-05 12:25:48 metrics.py:396] Avg prompt throughput: 61.1 tokens/s, Avg generation throughput: 507.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:25:48 logger.py:36] Received request cmpl-7d293e72d62c41b9975d7e182201023c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:48 async_llm_engine.py:173] Added request cmpl-7d293e72d62c41b9975d7e182201023c-0.
INFO 02-05 12:25:48 logger.py:36] Received request cmpl-e663bbfe5c094c81a0f3fe7467594064-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:48 async_llm_engine.py:173] Added request cmpl-e663bbfe5c094c81a0f3fe7467594064-0.
INFO 02-05 12:25:48 logger.py:36] Received request cmpl-94ee0bc2a418498a91077d0c8655d2c9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:48 async_llm_engine.py:173] Added request cmpl-94ee0bc2a418498a91077d0c8655d2c9-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:49 async_llm_engine.py:140] Finished request cmpl-256455117360465f95bb09a315189b13-0.
INFO 02-05 12:25:49 async_llm_engine.py:140] Finished request cmpl-018b4bb34ad0412b80fbe4b52589800d-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:49 async_llm_engine.py:140] Finished request cmpl-4f8eaf7b8ad34f289b9d7af9b0b04f81-0.
INFO 02-05 12:25:49 async_llm_engine.py:140] Finished request cmpl-4b61d4549d1b48f2ad90c461283e64f2-0.
INFO 02-05 12:25:49 async_llm_engine.py:140] Finished request cmpl-c85b635bbeeb48adbcbb7203b2c62590-0.
INFO 02-05 12:25:49 async_llm_engine.py:140] Finished request cmpl-0e3ed9d95d3c4dada96431d294b503bf-0.
INFO 02-05 12:25:49 async_llm_engine.py:140] Finished request cmpl-19bc91ec33f64d87806428edb632313a-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:49 async_llm_engine.py:140] Finished request cmpl-7d293e72d62c41b9975d7e182201023c-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:49 async_llm_engine.py:140] Finished request cmpl-e663bbfe5c094c81a0f3fe7467594064-0.
INFO 02-05 12:25:49 async_llm_engine.py:140] Finished request cmpl-94ee0bc2a418498a91077d0c8655d2c9-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:49 logger.py:36] Received request cmpl-e4aa15a7e6d141bd9b59bb86c07ec74c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:49 async_llm_engine.py:173] Added request cmpl-e4aa15a7e6d141bd9b59bb86c07ec74c-0.
INFO 02-05 12:25:49 logger.py:36] Received request cmpl-bd71ffd848674f1b88a53eb52ad9a1a3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:49 async_llm_engine.py:173] Added request cmpl-bd71ffd848674f1b88a53eb52ad9a1a3-0.
INFO 02-05 12:25:49 logger.py:36] Received request cmpl-60e69eadaf6a473384ebf6915300cae0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:49 async_llm_engine.py:173] Added request cmpl-60e69eadaf6a473384ebf6915300cae0-0.
INFO 02-05 12:25:49 logger.py:36] Received request cmpl-9de8a16ab7384e269d6c7c683970dc5a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:49 async_llm_engine.py:173] Added request cmpl-9de8a16ab7384e269d6c7c683970dc5a-0.
INFO 02-05 12:25:49 logger.py:36] Received request cmpl-3c50f0b0a2ad4cd68ba79d9104309ec4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:49 async_llm_engine.py:173] Added request cmpl-3c50f0b0a2ad4cd68ba79d9104309ec4-0.
INFO 02-05 12:25:49 logger.py:36] Received request cmpl-80076deea19746fda8b0bae172393d84-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:49 async_llm_engine.py:173] Added request cmpl-80076deea19746fda8b0bae172393d84-0.
INFO 02-05 12:25:49 logger.py:36] Received request cmpl-561cc7f87684428eac83898b44ecc85d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:49 async_llm_engine.py:173] Added request cmpl-561cc7f87684428eac83898b44ecc85d-0.
INFO 02-05 12:25:49 logger.py:36] Received request cmpl-c4f8ffef6a0e4a11b8f9f204dac3bf6b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:49 async_llm_engine.py:173] Added request cmpl-c4f8ffef6a0e4a11b8f9f204dac3bf6b-0.
INFO 02-05 12:25:49 logger.py:36] Received request cmpl-7477739358ef47c8a95aefb33faf9782-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:49 async_llm_engine.py:173] Added request cmpl-7477739358ef47c8a95aefb33faf9782-0.
INFO 02-05 12:25:49 logger.py:36] Received request cmpl-b4121a5aa6c54ddc9e70939d36c834c9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:49 async_llm_engine.py:173] Added request cmpl-b4121a5aa6c54ddc9e70939d36c834c9-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:50 async_llm_engine.py:140] Finished request cmpl-e4aa15a7e6d141bd9b59bb86c07ec74c-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:50 async_llm_engine.py:140] Finished request cmpl-bd71ffd848674f1b88a53eb52ad9a1a3-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:50 async_llm_engine.py:140] Finished request cmpl-60e69eadaf6a473384ebf6915300cae0-0.
INFO 02-05 12:25:50 async_llm_engine.py:140] Finished request cmpl-9de8a16ab7384e269d6c7c683970dc5a-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:50 async_llm_engine.py:140] Finished request cmpl-3c50f0b0a2ad4cd68ba79d9104309ec4-0.
INFO 02-05 12:25:50 async_llm_engine.py:140] Finished request cmpl-80076deea19746fda8b0bae172393d84-0.
INFO 02-05 12:25:50 async_llm_engine.py:140] Finished request cmpl-561cc7f87684428eac83898b44ecc85d-0.
INFO 02-05 12:25:50 async_llm_engine.py:140] Finished request cmpl-c4f8ffef6a0e4a11b8f9f204dac3bf6b-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:50 async_llm_engine.py:140] Finished request cmpl-7477739358ef47c8a95aefb33faf9782-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:50 async_llm_engine.py:140] Finished request cmpl-b4121a5aa6c54ddc9e70939d36c834c9-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:51 logger.py:36] Received request cmpl-f617b8ea37354eaf84af03da589fc4af-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:51 async_llm_engine.py:173] Added request cmpl-f617b8ea37354eaf84af03da589fc4af-0.
INFO 02-05 12:25:51 logger.py:36] Received request cmpl-ed4079acbd314afea8dbc56a31d1e50b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:51 async_llm_engine.py:173] Added request cmpl-ed4079acbd314afea8dbc56a31d1e50b-0.
INFO 02-05 12:25:51 logger.py:36] Received request cmpl-fcb1d56a8fea4f249093a1b6c3969b76-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:51 async_llm_engine.py:173] Added request cmpl-fcb1d56a8fea4f249093a1b6c3969b76-0.
INFO 02-05 12:25:51 logger.py:36] Received request cmpl-88215973174e4862acf838340929e332-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:51 async_llm_engine.py:173] Added request cmpl-88215973174e4862acf838340929e332-0.
INFO 02-05 12:25:51 logger.py:36] Received request cmpl-ae2cd642bb824c54ad07c95db136b67b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:51 logger.py:36] Received request cmpl-8fe4fad5d49d405baed0ac49efb6991e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:51 async_llm_engine.py:173] Added request cmpl-ae2cd642bb824c54ad07c95db136b67b-0.
INFO 02-05 12:25:51 async_llm_engine.py:173] Added request cmpl-8fe4fad5d49d405baed0ac49efb6991e-0.
INFO 02-05 12:25:51 logger.py:36] Received request cmpl-dff974862b89473dbeccfda2c15776f2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:51 async_llm_engine.py:173] Added request cmpl-dff974862b89473dbeccfda2c15776f2-0.
INFO 02-05 12:25:51 logger.py:36] Received request cmpl-6d9766ac26e743a99518dd0144f9a17c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:51 async_llm_engine.py:173] Added request cmpl-6d9766ac26e743a99518dd0144f9a17c-0.
INFO 02-05 12:25:51 logger.py:36] Received request cmpl-62034dbd03344d31b31e93149ebff628-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:51 async_llm_engine.py:173] Added request cmpl-62034dbd03344d31b31e93149ebff628-0.
INFO 02-05 12:25:51 logger.py:36] Received request cmpl-b9345800129b44c190f336a48212f88d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:51 async_llm_engine.py:173] Added request cmpl-b9345800129b44c190f336a48212f88d-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:52 async_llm_engine.py:140] Finished request cmpl-f617b8ea37354eaf84af03da589fc4af-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:52 async_llm_engine.py:140] Finished request cmpl-ed4079acbd314afea8dbc56a31d1e50b-0.
INFO 02-05 12:25:52 async_llm_engine.py:140] Finished request cmpl-fcb1d56a8fea4f249093a1b6c3969b76-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:52 async_llm_engine.py:140] Finished request cmpl-88215973174e4862acf838340929e332-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:52 async_llm_engine.py:140] Finished request cmpl-ae2cd642bb824c54ad07c95db136b67b-0.
INFO 02-05 12:25:52 async_llm_engine.py:140] Finished request cmpl-8fe4fad5d49d405baed0ac49efb6991e-0.
INFO 02-05 12:25:52 async_llm_engine.py:140] Finished request cmpl-dff974862b89473dbeccfda2c15776f2-0.
INFO 02-05 12:25:52 async_llm_engine.py:140] Finished request cmpl-6d9766ac26e743a99518dd0144f9a17c-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:52 async_llm_engine.py:140] Finished request cmpl-62034dbd03344d31b31e93149ebff628-0.
INFO 02-05 12:25:52 async_llm_engine.py:140] Finished request cmpl-b9345800129b44c190f336a48212f88d-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:52 logger.py:36] Received request cmpl-813cb6536f1c4bf48b1fa5c6eb1f263b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:52 async_llm_engine.py:173] Added request cmpl-813cb6536f1c4bf48b1fa5c6eb1f263b-0.
INFO 02-05 12:25:52 logger.py:36] Received request cmpl-bca7ed9242c74121af2a268a947c395c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:52 logger.py:36] Received request cmpl-4901444a584543ee834c9f3c6c233a99-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:52 async_llm_engine.py:173] Added request cmpl-bca7ed9242c74121af2a268a947c395c-0.
INFO 02-05 12:25:52 async_llm_engine.py:173] Added request cmpl-4901444a584543ee834c9f3c6c233a99-0.
INFO 02-05 12:25:52 logger.py:36] Received request cmpl-527b4baba01e44a395f0f7cc8dc30533-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:52 async_llm_engine.py:173] Added request cmpl-527b4baba01e44a395f0f7cc8dc30533-0.
INFO 02-05 12:25:52 logger.py:36] Received request cmpl-8e29bf3279c94b47ad5831ae10589a32-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:52 async_llm_engine.py:173] Added request cmpl-8e29bf3279c94b47ad5831ae10589a32-0.
INFO 02-05 12:25:52 logger.py:36] Received request cmpl-546d7aeb60444f4eb63059e330c1f2e8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:52 logger.py:36] Received request cmpl-af0cdf20f92648ce9108d4e5278e1132-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:52 async_llm_engine.py:173] Added request cmpl-546d7aeb60444f4eb63059e330c1f2e8-0.
INFO 02-05 12:25:52 async_llm_engine.py:173] Added request cmpl-af0cdf20f92648ce9108d4e5278e1132-0.
INFO 02-05 12:25:52 logger.py:36] Received request cmpl-3f589ad77fc24396a0da19ef46ef4c54-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:52 logger.py:36] Received request cmpl-93ddf40e8e154109a143f9cbb0093059-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:52 logger.py:36] Received request cmpl-635484c84f574c18bd9977359512d981-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:52 async_llm_engine.py:173] Added request cmpl-3f589ad77fc24396a0da19ef46ef4c54-0.
INFO 02-05 12:25:52 async_llm_engine.py:173] Added request cmpl-93ddf40e8e154109a143f9cbb0093059-0.
INFO 02-05 12:25:52 async_llm_engine.py:173] Added request cmpl-635484c84f574c18bd9977359512d981-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:53 async_llm_engine.py:140] Finished request cmpl-813cb6536f1c4bf48b1fa5c6eb1f263b-0.
INFO 02-05 12:25:53 async_llm_engine.py:140] Finished request cmpl-bca7ed9242c74121af2a268a947c395c-0.
INFO 02-05 12:25:53 async_llm_engine.py:140] Finished request cmpl-4901444a584543ee834c9f3c6c233a99-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:53 async_llm_engine.py:140] Finished request cmpl-527b4baba01e44a395f0f7cc8dc30533-0.
INFO 02-05 12:25:53 async_llm_engine.py:140] Finished request cmpl-8e29bf3279c94b47ad5831ae10589a32-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:53 async_llm_engine.py:140] Finished request cmpl-546d7aeb60444f4eb63059e330c1f2e8-0.
INFO 02-05 12:25:53 async_llm_engine.py:140] Finished request cmpl-af0cdf20f92648ce9108d4e5278e1132-0.
INFO 02-05 12:25:53 async_llm_engine.py:140] Finished request cmpl-3f589ad77fc24396a0da19ef46ef4c54-0.
INFO 02-05 12:25:53 async_llm_engine.py:140] Finished request cmpl-93ddf40e8e154109a143f9cbb0093059-0.
INFO 02-05 12:25:53 async_llm_engine.py:140] Finished request cmpl-635484c84f574c18bd9977359512d981-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:53 logger.py:36] Received request cmpl-28285681082d4178816fc75ac6962cb2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:53 async_llm_engine.py:173] Added request cmpl-28285681082d4178816fc75ac6962cb2-0.
INFO 02-05 12:25:53 logger.py:36] Received request cmpl-7a1213a616c74e529c2d7a706d879813-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:53 async_llm_engine.py:173] Added request cmpl-7a1213a616c74e529c2d7a706d879813-0.
INFO 02-05 12:25:53 logger.py:36] Received request cmpl-fa6d0f1cd22842b08de60abe19b10e48-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:53 logger.py:36] Received request cmpl-6e52f54049884eb796805308deebf55d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:53 async_llm_engine.py:173] Added request cmpl-fa6d0f1cd22842b08de60abe19b10e48-0.
INFO 02-05 12:25:53 async_llm_engine.py:173] Added request cmpl-6e52f54049884eb796805308deebf55d-0.
INFO 02-05 12:25:53 logger.py:36] Received request cmpl-491d4d13ec6b46f2b3a590b13c977d87-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:53 async_llm_engine.py:173] Added request cmpl-491d4d13ec6b46f2b3a590b13c977d87-0.
INFO 02-05 12:25:53 logger.py:36] Received request cmpl-061c8d21e5fc41f48584b403034afe18-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:53 async_llm_engine.py:173] Added request cmpl-061c8d21e5fc41f48584b403034afe18-0.
INFO 02-05 12:25:53 logger.py:36] Received request cmpl-cc29019000844e499c25bf36650c918f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:53 async_llm_engine.py:173] Added request cmpl-cc29019000844e499c25bf36650c918f-0.
INFO 02-05 12:25:53 logger.py:36] Received request cmpl-acb5f50e291240e98f9ce1852bd402b5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:53 async_llm_engine.py:173] Added request cmpl-acb5f50e291240e98f9ce1852bd402b5-0.
INFO 02-05 12:25:53 metrics.py:396] Avg prompt throughput: 79.1 tokens/s, Avg generation throughput: 511.6 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:25:53 logger.py:36] Received request cmpl-0582693b908d400d9ee41be935523cf5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:53 async_llm_engine.py:173] Added request cmpl-0582693b908d400d9ee41be935523cf5-0.
INFO 02-05 12:25:53 logger.py:36] Received request cmpl-1174aa37cadb41ff8f029f1c3cb9be79-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:53 async_llm_engine.py:173] Added request cmpl-1174aa37cadb41ff8f029f1c3cb9be79-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:54 async_llm_engine.py:140] Finished request cmpl-28285681082d4178816fc75ac6962cb2-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:54 async_llm_engine.py:140] Finished request cmpl-7a1213a616c74e529c2d7a706d879813-0.
INFO 02-05 12:25:54 async_llm_engine.py:140] Finished request cmpl-fa6d0f1cd22842b08de60abe19b10e48-0.
INFO 02-05 12:25:54 async_llm_engine.py:140] Finished request cmpl-6e52f54049884eb796805308deebf55d-0.
INFO 02-05 12:25:54 async_llm_engine.py:140] Finished request cmpl-491d4d13ec6b46f2b3a590b13c977d87-0.
INFO 02-05 12:25:54 async_llm_engine.py:140] Finished request cmpl-061c8d21e5fc41f48584b403034afe18-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:54 async_llm_engine.py:140] Finished request cmpl-cc29019000844e499c25bf36650c918f-0.
INFO 02-05 12:25:54 async_llm_engine.py:140] Finished request cmpl-acb5f50e291240e98f9ce1852bd402b5-0.
INFO 02-05 12:25:54 async_llm_engine.py:140] Finished request cmpl-0582693b908d400d9ee41be935523cf5-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:54 async_llm_engine.py:140] Finished request cmpl-1174aa37cadb41ff8f029f1c3cb9be79-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:54 logger.py:36] Received request cmpl-ce17affaf83e4435b1bf6f96f275c24c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:54 async_llm_engine.py:173] Added request cmpl-ce17affaf83e4435b1bf6f96f275c24c-0.
INFO 02-05 12:25:54 logger.py:36] Received request cmpl-a889ac84c8ef4e48af9e0342de34409b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:54 logger.py:36] Received request cmpl-e388279415974b74bdbc7064c889e904-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:54 logger.py:36] Received request cmpl-c0653a549ec6409e9c219fa83158f805-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:54 logger.py:36] Received request cmpl-cebb1fabf67f46969f7003d8ef17c02c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:54 logger.py:36] Received request cmpl-c7fdc56786bb4a3fb972c1f14ca777fe-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:54 async_llm_engine.py:173] Added request cmpl-a889ac84c8ef4e48af9e0342de34409b-0.
INFO 02-05 12:25:54 async_llm_engine.py:173] Added request cmpl-e388279415974b74bdbc7064c889e904-0.
INFO 02-05 12:25:54 async_llm_engine.py:173] Added request cmpl-c0653a549ec6409e9c219fa83158f805-0.
INFO 02-05 12:25:54 async_llm_engine.py:173] Added request cmpl-cebb1fabf67f46969f7003d8ef17c02c-0.
INFO 02-05 12:25:54 async_llm_engine.py:173] Added request cmpl-c7fdc56786bb4a3fb972c1f14ca777fe-0.
INFO 02-05 12:25:54 logger.py:36] Received request cmpl-846d7a24632c40c79bc8ca2eead100aa-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:54 async_llm_engine.py:173] Added request cmpl-846d7a24632c40c79bc8ca2eead100aa-0.
INFO 02-05 12:25:54 logger.py:36] Received request cmpl-aae4d62b51944841bef0a4d476c34988-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:54 async_llm_engine.py:173] Added request cmpl-aae4d62b51944841bef0a4d476c34988-0.
INFO 02-05 12:25:54 logger.py:36] Received request cmpl-7d74f171f2464b53bb4336b334ec1894-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:54 async_llm_engine.py:173] Added request cmpl-7d74f171f2464b53bb4336b334ec1894-0.
INFO 02-05 12:25:54 logger.py:36] Received request cmpl-8119a835d35d43fba2ba5618232b7efb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:54 async_llm_engine.py:173] Added request cmpl-8119a835d35d43fba2ba5618232b7efb-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:36744 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:36756 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:25:56 async_llm_engine.py:140] Finished request cmpl-ce17affaf83e4435b1bf6f96f275c24c-0.
INFO 02-05 12:25:56 async_llm_engine.py:140] Finished request cmpl-a889ac84c8ef4e48af9e0342de34409b-0.
INFO 02-05 12:25:56 async_llm_engine.py:140] Finished request cmpl-e388279415974b74bdbc7064c889e904-0.
INFO 02-05 12:25:56 async_llm_engine.py:140] Finished request cmpl-c0653a549ec6409e9c219fa83158f805-0.
INFO 02-05 12:25:56 async_llm_engine.py:140] Finished request cmpl-cebb1fabf67f46969f7003d8ef17c02c-0.
INFO 02-05 12:25:56 async_llm_engine.py:140] Finished request cmpl-c7fdc56786bb4a3fb972c1f14ca777fe-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:56 async_llm_engine.py:140] Finished request cmpl-846d7a24632c40c79bc8ca2eead100aa-0.
INFO 02-05 12:25:56 async_llm_engine.py:140] Finished request cmpl-aae4d62b51944841bef0a4d476c34988-0.
INFO 02-05 12:25:56 async_llm_engine.py:140] Finished request cmpl-7d74f171f2464b53bb4336b334ec1894-0.
INFO 02-05 12:25:56 async_llm_engine.py:140] Finished request cmpl-8119a835d35d43fba2ba5618232b7efb-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:56 logger.py:36] Received request cmpl-31c994b9ebff457797e2289207ac24c1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:56 async_llm_engine.py:173] Added request cmpl-31c994b9ebff457797e2289207ac24c1-0.
INFO 02-05 12:25:56 logger.py:36] Received request cmpl-423d60ab06fe4369931b6f920f2a6654-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:56 async_llm_engine.py:173] Added request cmpl-423d60ab06fe4369931b6f920f2a6654-0.
INFO 02-05 12:25:56 logger.py:36] Received request cmpl-128b3c1b39ad4b6f9cf8b8e01799c4b0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:56 async_llm_engine.py:173] Added request cmpl-128b3c1b39ad4b6f9cf8b8e01799c4b0-0.
INFO 02-05 12:25:56 logger.py:36] Received request cmpl-b0160a60cf9f40a68170d570831dd825-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:56 async_llm_engine.py:173] Added request cmpl-b0160a60cf9f40a68170d570831dd825-0.
INFO 02-05 12:25:56 logger.py:36] Received request cmpl-17a2f55fe92b4628ab317be02ac7a177-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:56 async_llm_engine.py:173] Added request cmpl-17a2f55fe92b4628ab317be02ac7a177-0.
INFO 02-05 12:25:56 logger.py:36] Received request cmpl-173a8b84534043529319664d046b5920-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:56 async_llm_engine.py:173] Added request cmpl-173a8b84534043529319664d046b5920-0.
INFO 02-05 12:25:56 logger.py:36] Received request cmpl-f8c4d4ecafed43e1aa44f1d6bc5cdb56-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:56 logger.py:36] Received request cmpl-679438e87597483bbd73f24ca0711b10-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:56 logger.py:36] Received request cmpl-91c2a072c2354fe3a5ee40b7f51e0474-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:56 logger.py:36] Received request cmpl-73a9be350a2143a3a6a28f3502db9828-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:56 async_llm_engine.py:173] Added request cmpl-f8c4d4ecafed43e1aa44f1d6bc5cdb56-0.
INFO 02-05 12:25:56 async_llm_engine.py:173] Added request cmpl-679438e87597483bbd73f24ca0711b10-0.
INFO 02-05 12:25:56 async_llm_engine.py:173] Added request cmpl-91c2a072c2354fe3a5ee40b7f51e0474-0.
INFO 02-05 12:25:56 async_llm_engine.py:173] Added request cmpl-73a9be350a2143a3a6a28f3502db9828-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:57 async_llm_engine.py:140] Finished request cmpl-31c994b9ebff457797e2289207ac24c1-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:57 async_llm_engine.py:140] Finished request cmpl-423d60ab06fe4369931b6f920f2a6654-0.
INFO 02-05 12:25:57 async_llm_engine.py:140] Finished request cmpl-128b3c1b39ad4b6f9cf8b8e01799c4b0-0.
INFO 02-05 12:25:57 async_llm_engine.py:140] Finished request cmpl-b0160a60cf9f40a68170d570831dd825-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:57 async_llm_engine.py:140] Finished request cmpl-17a2f55fe92b4628ab317be02ac7a177-0.
INFO 02-05 12:25:57 async_llm_engine.py:140] Finished request cmpl-173a8b84534043529319664d046b5920-0.
INFO 02-05 12:25:57 async_llm_engine.py:140] Finished request cmpl-f8c4d4ecafed43e1aa44f1d6bc5cdb56-0.
INFO 02-05 12:25:57 async_llm_engine.py:140] Finished request cmpl-679438e87597483bbd73f24ca0711b10-0.
INFO 02-05 12:25:57 async_llm_engine.py:140] Finished request cmpl-91c2a072c2354fe3a5ee40b7f51e0474-0.
INFO 02-05 12:25:57 async_llm_engine.py:140] Finished request cmpl-73a9be350a2143a3a6a28f3502db9828-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:57 logger.py:36] Received request cmpl-cdefe6d99b9d4415955eb0f114056f0d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:57 async_llm_engine.py:173] Added request cmpl-cdefe6d99b9d4415955eb0f114056f0d-0.
INFO 02-05 12:25:57 logger.py:36] Received request cmpl-50d8effbb8b6407f95bdee03f9346ead-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:57 async_llm_engine.py:173] Added request cmpl-50d8effbb8b6407f95bdee03f9346ead-0.
INFO 02-05 12:25:57 logger.py:36] Received request cmpl-fe0cbee3357b4768b8d9f8fe82706c3c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:57 async_llm_engine.py:173] Added request cmpl-fe0cbee3357b4768b8d9f8fe82706c3c-0.
INFO 02-05 12:25:57 logger.py:36] Received request cmpl-d3918189a8284b9a985195dd04354b1e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:57 async_llm_engine.py:173] Added request cmpl-d3918189a8284b9a985195dd04354b1e-0.
INFO 02-05 12:25:57 logger.py:36] Received request cmpl-b23fc321eb61429cbe2eaa769ecbe96e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:57 async_llm_engine.py:173] Added request cmpl-b23fc321eb61429cbe2eaa769ecbe96e-0.
INFO 02-05 12:25:57 logger.py:36] Received request cmpl-2d0d0e33564b42e5850f9361629e87d5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:57 async_llm_engine.py:173] Added request cmpl-2d0d0e33564b42e5850f9361629e87d5-0.
INFO 02-05 12:25:57 logger.py:36] Received request cmpl-31039bf04d754ecdb0ae3660bbb68db4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:57 async_llm_engine.py:173] Added request cmpl-31039bf04d754ecdb0ae3660bbb68db4-0.
INFO 02-05 12:25:57 logger.py:36] Received request cmpl-11e61912ee87426d9ff6f89931e3331a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:57 async_llm_engine.py:173] Added request cmpl-11e61912ee87426d9ff6f89931e3331a-0.
INFO 02-05 12:25:57 logger.py:36] Received request cmpl-a2ef24e9f771497d8dd057b1bf6e4569-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:57 async_llm_engine.py:173] Added request cmpl-a2ef24e9f771497d8dd057b1bf6e4569-0.
INFO 02-05 12:25:57 logger.py:36] Received request cmpl-afaeb551e5ea455b8a999d3ddf53b4e1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:57 async_llm_engine.py:173] Added request cmpl-afaeb551e5ea455b8a999d3ddf53b4e1-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:58 async_llm_engine.py:140] Finished request cmpl-cdefe6d99b9d4415955eb0f114056f0d-0.
INFO 02-05 12:25:58 async_llm_engine.py:140] Finished request cmpl-50d8effbb8b6407f95bdee03f9346ead-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:58 async_llm_engine.py:140] Finished request cmpl-fe0cbee3357b4768b8d9f8fe82706c3c-0.
INFO 02-05 12:25:58 async_llm_engine.py:140] Finished request cmpl-d3918189a8284b9a985195dd04354b1e-0.
INFO 02-05 12:25:58 async_llm_engine.py:140] Finished request cmpl-b23fc321eb61429cbe2eaa769ecbe96e-0.
INFO 02-05 12:25:58 async_llm_engine.py:140] Finished request cmpl-2d0d0e33564b42e5850f9361629e87d5-0.
INFO 02-05 12:25:58 async_llm_engine.py:140] Finished request cmpl-31039bf04d754ecdb0ae3660bbb68db4-0.
INFO 02-05 12:25:58 async_llm_engine.py:140] Finished request cmpl-11e61912ee87426d9ff6f89931e3331a-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:58 async_llm_engine.py:140] Finished request cmpl-a2ef24e9f771497d8dd057b1bf6e4569-0.
INFO 02-05 12:25:58 async_llm_engine.py:140] Finished request cmpl-afaeb551e5ea455b8a999d3ddf53b4e1-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:58 logger.py:36] Received request cmpl-c58b5b03b8b54b739a13656115517e1a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:58 async_llm_engine.py:173] Added request cmpl-c58b5b03b8b54b739a13656115517e1a-0.
INFO 02-05 12:25:58 logger.py:36] Received request cmpl-3ac8feddb49f4f9ca0da509f0060df87-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:58 async_llm_engine.py:173] Added request cmpl-3ac8feddb49f4f9ca0da509f0060df87-0.
INFO 02-05 12:25:58 logger.py:36] Received request cmpl-910a3197952740089d0a5eae77d16119-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:58 async_llm_engine.py:173] Added request cmpl-910a3197952740089d0a5eae77d16119-0.
INFO 02-05 12:25:58 metrics.py:396] Avg prompt throughput: 62.3 tokens/s, Avg generation throughput: 504.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:25:58 logger.py:36] Received request cmpl-bd13cc670514400ca6fc694205c5abee-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:58 async_llm_engine.py:173] Added request cmpl-bd13cc670514400ca6fc694205c5abee-0.
INFO 02-05 12:25:58 logger.py:36] Received request cmpl-532a746512364d24b3bfb4fdbd6ab813-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:58 async_llm_engine.py:173] Added request cmpl-532a746512364d24b3bfb4fdbd6ab813-0.
INFO 02-05 12:25:58 logger.py:36] Received request cmpl-47f3497e25b84bbb9b979cf208a1a0f2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:58 async_llm_engine.py:173] Added request cmpl-47f3497e25b84bbb9b979cf208a1a0f2-0.
INFO 02-05 12:25:58 logger.py:36] Received request cmpl-7a1b93f4ed5945b582b1f5687d9d1cad-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:58 async_llm_engine.py:173] Added request cmpl-7a1b93f4ed5945b582b1f5687d9d1cad-0.
INFO 02-05 12:25:58 logger.py:36] Received request cmpl-5fde9db2b15f4eb99ee83f8b159449bd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:58 async_llm_engine.py:173] Added request cmpl-5fde9db2b15f4eb99ee83f8b159449bd-0.
INFO 02-05 12:25:58 logger.py:36] Received request cmpl-741072354cef422fad8feea328786e8f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:58 async_llm_engine.py:173] Added request cmpl-741072354cef422fad8feea328786e8f-0.
INFO 02-05 12:25:58 logger.py:36] Received request cmpl-e726a250e44945e0ae3cbe7008b2b671-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:58 async_llm_engine.py:173] Added request cmpl-e726a250e44945e0ae3cbe7008b2b671-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:59 async_llm_engine.py:140] Finished request cmpl-c58b5b03b8b54b739a13656115517e1a-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:59 async_llm_engine.py:140] Finished request cmpl-3ac8feddb49f4f9ca0da509f0060df87-0.
INFO 02-05 12:25:59 async_llm_engine.py:140] Finished request cmpl-910a3197952740089d0a5eae77d16119-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:59 async_llm_engine.py:140] Finished request cmpl-bd13cc670514400ca6fc694205c5abee-0.
INFO 02-05 12:25:59 async_llm_engine.py:140] Finished request cmpl-532a746512364d24b3bfb4fdbd6ab813-0.
INFO 02-05 12:25:59 async_llm_engine.py:140] Finished request cmpl-47f3497e25b84bbb9b979cf208a1a0f2-0.
INFO 02-05 12:25:59 async_llm_engine.py:140] Finished request cmpl-7a1b93f4ed5945b582b1f5687d9d1cad-0.
INFO 02-05 12:25:59 async_llm_engine.py:140] Finished request cmpl-5fde9db2b15f4eb99ee83f8b159449bd-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:25:59 async_llm_engine.py:140] Finished request cmpl-741072354cef422fad8feea328786e8f-0.
INFO 02-05 12:25:59 async_llm_engine.py:140] Finished request cmpl-e726a250e44945e0ae3cbe7008b2b671-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:25:59 logger.py:36] Received request cmpl-3497872a844647dab5549dfd5e90059e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:59 async_llm_engine.py:173] Added request cmpl-3497872a844647dab5549dfd5e90059e-0.
INFO 02-05 12:25:59 logger.py:36] Received request cmpl-4d889cf7950e4b5eb42b83d7696f3e11-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:59 async_llm_engine.py:173] Added request cmpl-4d889cf7950e4b5eb42b83d7696f3e11-0.
INFO 02-05 12:25:59 logger.py:36] Received request cmpl-b32514a070044a0eb170f4caba43abf8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:59 async_llm_engine.py:173] Added request cmpl-b32514a070044a0eb170f4caba43abf8-0.
INFO 02-05 12:25:59 logger.py:36] Received request cmpl-29eaaf7e0df340239471d1800aaf9aca-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:59 async_llm_engine.py:173] Added request cmpl-29eaaf7e0df340239471d1800aaf9aca-0.
INFO 02-05 12:25:59 logger.py:36] Received request cmpl-f893e033a0214f109bdc4a65a8f08643-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:59 async_llm_engine.py:173] Added request cmpl-f893e033a0214f109bdc4a65a8f08643-0.
INFO 02-05 12:25:59 logger.py:36] Received request cmpl-f1bf194f77324f57a5347db2c783bc45-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:59 async_llm_engine.py:173] Added request cmpl-f1bf194f77324f57a5347db2c783bc45-0.
INFO 02-05 12:25:59 logger.py:36] Received request cmpl-f077546452484e56a762845cc6d5d14e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:59 async_llm_engine.py:173] Added request cmpl-f077546452484e56a762845cc6d5d14e-0.
INFO 02-05 12:25:59 logger.py:36] Received request cmpl-f5ef749640074dac9d4850428b64f4ce-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:59 logger.py:36] Received request cmpl-62521f849b9c49568f238eb0a4d9b52e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:59 async_llm_engine.py:173] Added request cmpl-f5ef749640074dac9d4850428b64f4ce-0.
INFO 02-05 12:25:59 async_llm_engine.py:173] Added request cmpl-62521f849b9c49568f238eb0a4d9b52e-0.
INFO 02-05 12:25:59 logger.py:36] Received request cmpl-2c03eff9a6c146fd9d3a920044e45d41-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:25:59 async_llm_engine.py:173] Added request cmpl-2c03eff9a6c146fd9d3a920044e45d41-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:01 async_llm_engine.py:140] Finished request cmpl-3497872a844647dab5549dfd5e90059e-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:01 async_llm_engine.py:140] Finished request cmpl-4d889cf7950e4b5eb42b83d7696f3e11-0.
INFO 02-05 12:26:01 async_llm_engine.py:140] Finished request cmpl-b32514a070044a0eb170f4caba43abf8-0.
INFO 02-05 12:26:01 async_llm_engine.py:140] Finished request cmpl-29eaaf7e0df340239471d1800aaf9aca-0.
INFO 02-05 12:26:01 async_llm_engine.py:140] Finished request cmpl-f893e033a0214f109bdc4a65a8f08643-0.
INFO 02-05 12:26:01 async_llm_engine.py:140] Finished request cmpl-f1bf194f77324f57a5347db2c783bc45-0.
INFO 02-05 12:26:01 async_llm_engine.py:140] Finished request cmpl-f077546452484e56a762845cc6d5d14e-0.
INFO 02-05 12:26:01 async_llm_engine.py:140] Finished request cmpl-f5ef749640074dac9d4850428b64f4ce-0.
INFO 02-05 12:26:01 async_llm_engine.py:140] Finished request cmpl-62521f849b9c49568f238eb0a4d9b52e-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:01 async_llm_engine.py:140] Finished request cmpl-2c03eff9a6c146fd9d3a920044e45d41-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:01 logger.py:36] Received request cmpl-7c48532fa2c6400cb0fbe372d71c25fe-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:01 async_llm_engine.py:173] Added request cmpl-7c48532fa2c6400cb0fbe372d71c25fe-0.
INFO 02-05 12:26:01 logger.py:36] Received request cmpl-528d73ec85dd4be09a307655482a3d80-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:01 async_llm_engine.py:173] Added request cmpl-528d73ec85dd4be09a307655482a3d80-0.
INFO 02-05 12:26:01 logger.py:36] Received request cmpl-d7b47d504d374f65ab351f235de5ff6e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:01 async_llm_engine.py:173] Added request cmpl-d7b47d504d374f65ab351f235de5ff6e-0.
INFO 02-05 12:26:01 logger.py:36] Received request cmpl-8e13692e6390422d83f98c1eb702d724-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:01 async_llm_engine.py:173] Added request cmpl-8e13692e6390422d83f98c1eb702d724-0.
INFO 02-05 12:26:01 logger.py:36] Received request cmpl-272f61d5b1d74ee6868865fe2d5a88f0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:01 logger.py:36] Received request cmpl-9f383a4e11b04fc5b0e3eb9a1587100f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:01 async_llm_engine.py:173] Added request cmpl-272f61d5b1d74ee6868865fe2d5a88f0-0.
INFO 02-05 12:26:01 async_llm_engine.py:173] Added request cmpl-9f383a4e11b04fc5b0e3eb9a1587100f-0.
INFO 02-05 12:26:01 logger.py:36] Received request cmpl-aaea8bdd408642ca855e1a0ad45c45e5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:01 async_llm_engine.py:173] Added request cmpl-aaea8bdd408642ca855e1a0ad45c45e5-0.
INFO 02-05 12:26:01 logger.py:36] Received request cmpl-73d02c45b7ce4c55b66d738d70bb81c7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:01 async_llm_engine.py:173] Added request cmpl-73d02c45b7ce4c55b66d738d70bb81c7-0.
INFO 02-05 12:26:01 logger.py:36] Received request cmpl-67a39c408a99460b8cf84a55c9e7f717-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:01 async_llm_engine.py:173] Added request cmpl-67a39c408a99460b8cf84a55c9e7f717-0.
INFO 02-05 12:26:01 logger.py:36] Received request cmpl-ab4f22e5b78a4e2f83425a1de11b7e94-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:01 async_llm_engine.py:173] Added request cmpl-ab4f22e5b78a4e2f83425a1de11b7e94-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:02 async_llm_engine.py:140] Finished request cmpl-7c48532fa2c6400cb0fbe372d71c25fe-0.
INFO 02-05 12:26:02 async_llm_engine.py:140] Finished request cmpl-528d73ec85dd4be09a307655482a3d80-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:02 async_llm_engine.py:140] Finished request cmpl-d7b47d504d374f65ab351f235de5ff6e-0.
INFO 02-05 12:26:02 async_llm_engine.py:140] Finished request cmpl-8e13692e6390422d83f98c1eb702d724-0.
INFO 02-05 12:26:02 async_llm_engine.py:140] Finished request cmpl-272f61d5b1d74ee6868865fe2d5a88f0-0.
INFO 02-05 12:26:02 async_llm_engine.py:140] Finished request cmpl-9f383a4e11b04fc5b0e3eb9a1587100f-0.
INFO 02-05 12:26:02 async_llm_engine.py:140] Finished request cmpl-aaea8bdd408642ca855e1a0ad45c45e5-0.
INFO 02-05 12:26:02 async_llm_engine.py:140] Finished request cmpl-73d02c45b7ce4c55b66d738d70bb81c7-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:02 async_llm_engine.py:140] Finished request cmpl-67a39c408a99460b8cf84a55c9e7f717-0.
INFO 02-05 12:26:02 async_llm_engine.py:140] Finished request cmpl-ab4f22e5b78a4e2f83425a1de11b7e94-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:02 logger.py:36] Received request cmpl-9b23afed61514d9b9613d84d4185e07e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:02 async_llm_engine.py:173] Added request cmpl-9b23afed61514d9b9613d84d4185e07e-0.
INFO 02-05 12:26:02 logger.py:36] Received request cmpl-bae3d7c813f74e4299a088724af672f5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:02 async_llm_engine.py:173] Added request cmpl-bae3d7c813f74e4299a088724af672f5-0.
INFO 02-05 12:26:02 logger.py:36] Received request cmpl-57f3a72b767a42479577f11f80f8cbea-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:02 async_llm_engine.py:173] Added request cmpl-57f3a72b767a42479577f11f80f8cbea-0.
INFO 02-05 12:26:02 logger.py:36] Received request cmpl-36306a9c2e7843bfa2b909d9c7f96c36-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:02 async_llm_engine.py:173] Added request cmpl-36306a9c2e7843bfa2b909d9c7f96c36-0.
INFO 02-05 12:26:02 logger.py:36] Received request cmpl-9b076da8534243709129c943c52e7d3e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:02 async_llm_engine.py:173] Added request cmpl-9b076da8534243709129c943c52e7d3e-0.
INFO 02-05 12:26:02 logger.py:36] Received request cmpl-54cd3007ba64471e9689a4675163b790-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:02 logger.py:36] Received request cmpl-bde080274433496ab5d7a291053c0c95-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:02 async_llm_engine.py:173] Added request cmpl-54cd3007ba64471e9689a4675163b790-0.
INFO 02-05 12:26:02 async_llm_engine.py:173] Added request cmpl-bde080274433496ab5d7a291053c0c95-0.
INFO 02-05 12:26:02 logger.py:36] Received request cmpl-5aa57859f0304ff39c0bcb67759ee265-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:02 async_llm_engine.py:173] Added request cmpl-5aa57859f0304ff39c0bcb67759ee265-0.
INFO 02-05 12:26:02 logger.py:36] Received request cmpl-bdc048c43a354633bcb756bc0af47a28-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:02 async_llm_engine.py:173] Added request cmpl-bdc048c43a354633bcb756bc0af47a28-0.
INFO 02-05 12:26:02 logger.py:36] Received request cmpl-d4af0332591049ad99ba2a47f6483f47-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:02 async_llm_engine.py:173] Added request cmpl-d4af0332591049ad99ba2a47f6483f47-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:03 async_llm_engine.py:140] Finished request cmpl-9b23afed61514d9b9613d84d4185e07e-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:03 async_llm_engine.py:140] Finished request cmpl-bae3d7c813f74e4299a088724af672f5-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:03 async_llm_engine.py:140] Finished request cmpl-57f3a72b767a42479577f11f80f8cbea-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:03 async_llm_engine.py:140] Finished request cmpl-36306a9c2e7843bfa2b909d9c7f96c36-0.
INFO 02-05 12:26:03 async_llm_engine.py:140] Finished request cmpl-9b076da8534243709129c943c52e7d3e-0.
INFO 02-05 12:26:03 async_llm_engine.py:140] Finished request cmpl-54cd3007ba64471e9689a4675163b790-0.
INFO 02-05 12:26:03 async_llm_engine.py:140] Finished request cmpl-bde080274433496ab5d7a291053c0c95-0.
INFO 02-05 12:26:03 async_llm_engine.py:140] Finished request cmpl-5aa57859f0304ff39c0bcb67759ee265-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:03 async_llm_engine.py:140] Finished request cmpl-bdc048c43a354633bcb756bc0af47a28-0.
INFO 02-05 12:26:03 async_llm_engine.py:140] Finished request cmpl-d4af0332591049ad99ba2a47f6483f47-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:03 logger.py:36] Received request cmpl-bcc2f6e862f44312a463bdaefe920b78-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:03 async_llm_engine.py:173] Added request cmpl-bcc2f6e862f44312a463bdaefe920b78-0.
INFO 02-05 12:26:03 logger.py:36] Received request cmpl-1c5a5823ee2f4afc861934c60f6fb937-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:03 async_llm_engine.py:173] Added request cmpl-1c5a5823ee2f4afc861934c60f6fb937-0.
INFO 02-05 12:26:03 metrics.py:396] Avg prompt throughput: 71.9 tokens/s, Avg generation throughput: 511.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:26:03 logger.py:36] Received request cmpl-aded74c9a6a44716b20c2bf93eaf8e89-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:03 async_llm_engine.py:173] Added request cmpl-aded74c9a6a44716b20c2bf93eaf8e89-0.
INFO 02-05 12:26:03 logger.py:36] Received request cmpl-21d940f48f4249d783a1a4a53721fee5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:03 async_llm_engine.py:173] Added request cmpl-21d940f48f4249d783a1a4a53721fee5-0.
INFO 02-05 12:26:03 logger.py:36] Received request cmpl-1d7f9cce35044980b8d429478ff9b8a3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:03 logger.py:36] Received request cmpl-36c3b6216c8240a09454ff868140fa9a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:03 async_llm_engine.py:173] Added request cmpl-1d7f9cce35044980b8d429478ff9b8a3-0.
INFO 02-05 12:26:03 async_llm_engine.py:173] Added request cmpl-36c3b6216c8240a09454ff868140fa9a-0.
INFO 02-05 12:26:03 logger.py:36] Received request cmpl-81caa28a4c644343a80cf1a0df8d1328-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:03 async_llm_engine.py:173] Added request cmpl-81caa28a4c644343a80cf1a0df8d1328-0.
INFO 02-05 12:26:03 logger.py:36] Received request cmpl-bcf406217d314cb8a467ea868cd86771-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:03 async_llm_engine.py:173] Added request cmpl-bcf406217d314cb8a467ea868cd86771-0.
INFO 02-05 12:26:03 logger.py:36] Received request cmpl-148787b029cb4a3db9a3bb6e0a1d0eee-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:03 async_llm_engine.py:173] Added request cmpl-148787b029cb4a3db9a3bb6e0a1d0eee-0.
INFO 02-05 12:26:03 logger.py:36] Received request cmpl-e82e7a48074d40b79aa60953057c10b5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:03 async_llm_engine.py:173] Added request cmpl-e82e7a48074d40b79aa60953057c10b5-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:04 async_llm_engine.py:140] Finished request cmpl-bcc2f6e862f44312a463bdaefe920b78-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:04 async_llm_engine.py:140] Finished request cmpl-1c5a5823ee2f4afc861934c60f6fb937-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:04 async_llm_engine.py:140] Finished request cmpl-aded74c9a6a44716b20c2bf93eaf8e89-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:04 async_llm_engine.py:140] Finished request cmpl-21d940f48f4249d783a1a4a53721fee5-0.
INFO 02-05 12:26:04 async_llm_engine.py:140] Finished request cmpl-1d7f9cce35044980b8d429478ff9b8a3-0.
INFO 02-05 12:26:04 async_llm_engine.py:140] Finished request cmpl-36c3b6216c8240a09454ff868140fa9a-0.
INFO 02-05 12:26:04 async_llm_engine.py:140] Finished request cmpl-81caa28a4c644343a80cf1a0df8d1328-0.
INFO 02-05 12:26:04 async_llm_engine.py:140] Finished request cmpl-bcf406217d314cb8a467ea868cd86771-0.
INFO 02-05 12:26:04 async_llm_engine.py:140] Finished request cmpl-148787b029cb4a3db9a3bb6e0a1d0eee-0.
INFO 02-05 12:26:04 async_llm_engine.py:140] Finished request cmpl-e82e7a48074d40b79aa60953057c10b5-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:04 logger.py:36] Received request cmpl-669d8f5b6e7447d883e12826ce3015aa-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:04 async_llm_engine.py:173] Added request cmpl-669d8f5b6e7447d883e12826ce3015aa-0.
INFO 02-05 12:26:04 logger.py:36] Received request cmpl-bd24d34cd2214b488908c2e7b3625081-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:04 async_llm_engine.py:173] Added request cmpl-bd24d34cd2214b488908c2e7b3625081-0.
INFO 02-05 12:26:04 logger.py:36] Received request cmpl-a9b361fa2ef3497e8f43e7d267d4c2a4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:04 async_llm_engine.py:173] Added request cmpl-a9b361fa2ef3497e8f43e7d267d4c2a4-0.
INFO 02-05 12:26:04 logger.py:36] Received request cmpl-8685ca8fac6246fe83b352e970523e32-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:04 async_llm_engine.py:173] Added request cmpl-8685ca8fac6246fe83b352e970523e32-0.
INFO 02-05 12:26:04 logger.py:36] Received request cmpl-ca9f5c7c57874b819f12681de08dcbee-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:04 async_llm_engine.py:173] Added request cmpl-ca9f5c7c57874b819f12681de08dcbee-0.
INFO 02-05 12:26:04 logger.py:36] Received request cmpl-73c84706b76a4346a494930ea51b7660-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:04 async_llm_engine.py:173] Added request cmpl-73c84706b76a4346a494930ea51b7660-0.
INFO 02-05 12:26:04 logger.py:36] Received request cmpl-7d5c1ffb81f24e44af5796681e42902f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:04 async_llm_engine.py:173] Added request cmpl-7d5c1ffb81f24e44af5796681e42902f-0.
INFO 02-05 12:26:04 logger.py:36] Received request cmpl-fbcaa938f7f14d3ebf1c08c9156e975e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:04 logger.py:36] Received request cmpl-2c1eaf56adf6401085aff7b25cafa580-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:04 logger.py:36] Received request cmpl-285ae41db47745d6bba450fd988ccc9e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:04 async_llm_engine.py:173] Added request cmpl-fbcaa938f7f14d3ebf1c08c9156e975e-0.
INFO 02-05 12:26:04 async_llm_engine.py:173] Added request cmpl-2c1eaf56adf6401085aff7b25cafa580-0.
INFO 02-05 12:26:04 async_llm_engine.py:173] Added request cmpl-285ae41db47745d6bba450fd988ccc9e-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:60772 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:60780 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:26:06 async_llm_engine.py:140] Finished request cmpl-669d8f5b6e7447d883e12826ce3015aa-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:06 async_llm_engine.py:140] Finished request cmpl-bd24d34cd2214b488908c2e7b3625081-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:06 async_llm_engine.py:140] Finished request cmpl-a9b361fa2ef3497e8f43e7d267d4c2a4-0.
INFO 02-05 12:26:06 async_llm_engine.py:140] Finished request cmpl-8685ca8fac6246fe83b352e970523e32-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:06 async_llm_engine.py:140] Finished request cmpl-ca9f5c7c57874b819f12681de08dcbee-0.
INFO 02-05 12:26:06 async_llm_engine.py:140] Finished request cmpl-73c84706b76a4346a494930ea51b7660-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:06 async_llm_engine.py:140] Finished request cmpl-7d5c1ffb81f24e44af5796681e42902f-0.
INFO 02-05 12:26:06 async_llm_engine.py:140] Finished request cmpl-fbcaa938f7f14d3ebf1c08c9156e975e-0.
INFO 02-05 12:26:06 async_llm_engine.py:140] Finished request cmpl-2c1eaf56adf6401085aff7b25cafa580-0.
INFO 02-05 12:26:06 async_llm_engine.py:140] Finished request cmpl-285ae41db47745d6bba450fd988ccc9e-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:06 logger.py:36] Received request cmpl-d4481d17341f48f883b2580b55189ce4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:06 async_llm_engine.py:173] Added request cmpl-d4481d17341f48f883b2580b55189ce4-0.
INFO 02-05 12:26:06 logger.py:36] Received request cmpl-7ef33e33eef349869324868a0756c3e0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:06 async_llm_engine.py:173] Added request cmpl-7ef33e33eef349869324868a0756c3e0-0.
INFO 02-05 12:26:06 logger.py:36] Received request cmpl-5432e629c18743fd9ffeb5b154dc2eb6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:06 async_llm_engine.py:173] Added request cmpl-5432e629c18743fd9ffeb5b154dc2eb6-0.
INFO 02-05 12:26:06 logger.py:36] Received request cmpl-2a1112f3a63844d994c08ead5d7e7759-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:06 async_llm_engine.py:173] Added request cmpl-2a1112f3a63844d994c08ead5d7e7759-0.
INFO 02-05 12:26:06 logger.py:36] Received request cmpl-a2ee398b07644ee281c698034582d796-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:06 async_llm_engine.py:173] Added request cmpl-a2ee398b07644ee281c698034582d796-0.
INFO 02-05 12:26:06 logger.py:36] Received request cmpl-4cfa5a289e6945188ed55725e076ca02-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:06 logger.py:36] Received request cmpl-fb5ba1ce63d04efaa71eea106855dbde-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:06 async_llm_engine.py:173] Added request cmpl-4cfa5a289e6945188ed55725e076ca02-0.
INFO 02-05 12:26:06 async_llm_engine.py:173] Added request cmpl-fb5ba1ce63d04efaa71eea106855dbde-0.
INFO 02-05 12:26:06 logger.py:36] Received request cmpl-6f8b201bee8d4401b93d07619ef9a9bf-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:06 async_llm_engine.py:173] Added request cmpl-6f8b201bee8d4401b93d07619ef9a9bf-0.
INFO 02-05 12:26:06 logger.py:36] Received request cmpl-1d139b8455d74d029f45f430d5858478-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:06 async_llm_engine.py:173] Added request cmpl-1d139b8455d74d029f45f430d5858478-0.
INFO 02-05 12:26:06 logger.py:36] Received request cmpl-9039f9503905455991657dcbbeb4eb2a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:06 async_llm_engine.py:173] Added request cmpl-9039f9503905455991657dcbbeb4eb2a-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:07 async_llm_engine.py:140] Finished request cmpl-d4481d17341f48f883b2580b55189ce4-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:07 async_llm_engine.py:140] Finished request cmpl-7ef33e33eef349869324868a0756c3e0-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:07 async_llm_engine.py:140] Finished request cmpl-5432e629c18743fd9ffeb5b154dc2eb6-0.
INFO 02-05 12:26:07 async_llm_engine.py:140] Finished request cmpl-2a1112f3a63844d994c08ead5d7e7759-0.
INFO 02-05 12:26:07 async_llm_engine.py:140] Finished request cmpl-a2ee398b07644ee281c698034582d796-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:07 async_llm_engine.py:140] Finished request cmpl-4cfa5a289e6945188ed55725e076ca02-0.
INFO 02-05 12:26:07 async_llm_engine.py:140] Finished request cmpl-fb5ba1ce63d04efaa71eea106855dbde-0.
INFO 02-05 12:26:07 async_llm_engine.py:140] Finished request cmpl-6f8b201bee8d4401b93d07619ef9a9bf-0.
INFO 02-05 12:26:07 async_llm_engine.py:140] Finished request cmpl-1d139b8455d74d029f45f430d5858478-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:07 async_llm_engine.py:140] Finished request cmpl-9039f9503905455991657dcbbeb4eb2a-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:07 logger.py:36] Received request cmpl-87e90ab3311641b3b4c0dcba6fbb8b3b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:07 async_llm_engine.py:173] Added request cmpl-87e90ab3311641b3b4c0dcba6fbb8b3b-0.
INFO 02-05 12:26:07 logger.py:36] Received request cmpl-7514230b49f54fcd94a5709bdb1e4848-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:07 async_llm_engine.py:173] Added request cmpl-7514230b49f54fcd94a5709bdb1e4848-0.
INFO 02-05 12:26:07 logger.py:36] Received request cmpl-c4b28a12bbf44b83aa509feaffd0517c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:07 async_llm_engine.py:173] Added request cmpl-c4b28a12bbf44b83aa509feaffd0517c-0.
INFO 02-05 12:26:07 logger.py:36] Received request cmpl-4a87e6a87558442da54db73148f11bcb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:07 async_llm_engine.py:173] Added request cmpl-4a87e6a87558442da54db73148f11bcb-0.
INFO 02-05 12:26:07 logger.py:36] Received request cmpl-690be89c891b4eb1be1115fa7bee1e5b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:07 async_llm_engine.py:173] Added request cmpl-690be89c891b4eb1be1115fa7bee1e5b-0.
INFO 02-05 12:26:07 logger.py:36] Received request cmpl-5e1312ddd4db412181b2ef667fcd5b42-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:07 async_llm_engine.py:173] Added request cmpl-5e1312ddd4db412181b2ef667fcd5b42-0.
INFO 02-05 12:26:07 logger.py:36] Received request cmpl-78a0737677ca408a8b7921fcb4a74e81-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:07 async_llm_engine.py:173] Added request cmpl-78a0737677ca408a8b7921fcb4a74e81-0.
INFO 02-05 12:26:07 logger.py:36] Received request cmpl-df9b45eb53fe41319462a8cbeb070484-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:07 async_llm_engine.py:173] Added request cmpl-df9b45eb53fe41319462a8cbeb070484-0.
INFO 02-05 12:26:07 logger.py:36] Received request cmpl-3a5d039b29ae4346a1b58314b265d1f9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:07 async_llm_engine.py:173] Added request cmpl-3a5d039b29ae4346a1b58314b265d1f9-0.
INFO 02-05 12:26:07 logger.py:36] Received request cmpl-2d4b83614568418fb04548bc10def8da-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:07 async_llm_engine.py:173] Added request cmpl-2d4b83614568418fb04548bc10def8da-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:08 metrics.py:396] Avg prompt throughput: 70.0 tokens/s, Avg generation throughput: 500.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-05 12:26:08 async_llm_engine.py:140] Finished request cmpl-87e90ab3311641b3b4c0dcba6fbb8b3b-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:08 async_llm_engine.py:140] Finished request cmpl-7514230b49f54fcd94a5709bdb1e4848-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:08 async_llm_engine.py:140] Finished request cmpl-c4b28a12bbf44b83aa509feaffd0517c-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:08 async_llm_engine.py:140] Finished request cmpl-4a87e6a87558442da54db73148f11bcb-0.
INFO 02-05 12:26:08 async_llm_engine.py:140] Finished request cmpl-690be89c891b4eb1be1115fa7bee1e5b-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:08 async_llm_engine.py:140] Finished request cmpl-5e1312ddd4db412181b2ef667fcd5b42-0.
INFO 02-05 12:26:08 async_llm_engine.py:140] Finished request cmpl-78a0737677ca408a8b7921fcb4a74e81-0.
INFO 02-05 12:26:08 async_llm_engine.py:140] Finished request cmpl-df9b45eb53fe41319462a8cbeb070484-0.
INFO 02-05 12:26:08 async_llm_engine.py:140] Finished request cmpl-3a5d039b29ae4346a1b58314b265d1f9-0.
INFO 02-05 12:26:08 async_llm_engine.py:140] Finished request cmpl-2d4b83614568418fb04548bc10def8da-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:08 logger.py:36] Received request cmpl-e8ec6a9780384381b0da4a024234f747-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:08 async_llm_engine.py:173] Added request cmpl-e8ec6a9780384381b0da4a024234f747-0.
INFO 02-05 12:26:08 logger.py:36] Received request cmpl-f99419a5e42146d6a4f82e460975fc7b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:08 async_llm_engine.py:173] Added request cmpl-f99419a5e42146d6a4f82e460975fc7b-0.
INFO 02-05 12:26:08 logger.py:36] Received request cmpl-e7484ebd732d4c16adbb1afc499cabcb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:08 async_llm_engine.py:173] Added request cmpl-e7484ebd732d4c16adbb1afc499cabcb-0.
INFO 02-05 12:26:08 logger.py:36] Received request cmpl-9c2df8374cdf4a9988cd95fa6224056f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:08 async_llm_engine.py:173] Added request cmpl-9c2df8374cdf4a9988cd95fa6224056f-0.
INFO 02-05 12:26:08 logger.py:36] Received request cmpl-c59875dd1e7f46e38a89dba92665f15e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:08 async_llm_engine.py:173] Added request cmpl-c59875dd1e7f46e38a89dba92665f15e-0.
INFO 02-05 12:26:08 logger.py:36] Received request cmpl-ac02317927914a88b38ad9099da386e1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:08 async_llm_engine.py:173] Added request cmpl-ac02317927914a88b38ad9099da386e1-0.
INFO 02-05 12:26:08 logger.py:36] Received request cmpl-2e827e2409234e94b3c626361e6ad03c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:08 async_llm_engine.py:173] Added request cmpl-2e827e2409234e94b3c626361e6ad03c-0.
INFO 02-05 12:26:08 logger.py:36] Received request cmpl-87c4dbf50cd14e1ab7f387f6c5e4edee-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:08 async_llm_engine.py:173] Added request cmpl-87c4dbf50cd14e1ab7f387f6c5e4edee-0.
INFO 02-05 12:26:09 logger.py:36] Received request cmpl-3236289931aa44bc8916003f23057cf0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:09 logger.py:36] Received request cmpl-2b406152b6064ebeb1c8efb4a91c3639-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:09 async_llm_engine.py:173] Added request cmpl-3236289931aa44bc8916003f23057cf0-0.
INFO 02-05 12:26:09 async_llm_engine.py:173] Added request cmpl-2b406152b6064ebeb1c8efb4a91c3639-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:09 async_llm_engine.py:140] Finished request cmpl-e8ec6a9780384381b0da4a024234f747-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:09 async_llm_engine.py:140] Finished request cmpl-f99419a5e42146d6a4f82e460975fc7b-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:09 async_llm_engine.py:140] Finished request cmpl-e7484ebd732d4c16adbb1afc499cabcb-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:10 async_llm_engine.py:140] Finished request cmpl-9c2df8374cdf4a9988cd95fa6224056f-0.
INFO 02-05 12:26:10 async_llm_engine.py:140] Finished request cmpl-c59875dd1e7f46e38a89dba92665f15e-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:10 async_llm_engine.py:140] Finished request cmpl-ac02317927914a88b38ad9099da386e1-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:10 async_llm_engine.py:140] Finished request cmpl-2e827e2409234e94b3c626361e6ad03c-0.
INFO 02-05 12:26:10 async_llm_engine.py:140] Finished request cmpl-87c4dbf50cd14e1ab7f387f6c5e4edee-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:10 logger.py:36] Received request cmpl-93dd5918cadb4adb8fea8012453799a1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:10 async_llm_engine.py:173] Added request cmpl-93dd5918cadb4adb8fea8012453799a1-0.
INFO 02-05 12:26:10 async_llm_engine.py:140] Finished request cmpl-3236289931aa44bc8916003f23057cf0-0.
INFO 02-05 12:26:10 async_llm_engine.py:140] Finished request cmpl-2b406152b6064ebeb1c8efb4a91c3639-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:10 logger.py:36] Received request cmpl-950b70d7817a4fa585f863d69bb272b2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:10 async_llm_engine.py:173] Added request cmpl-950b70d7817a4fa585f863d69bb272b2-0.
INFO 02-05 12:26:10 logger.py:36] Received request cmpl-80f94f55b36b48fe839ca79aca54650a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:10 async_llm_engine.py:173] Added request cmpl-80f94f55b36b48fe839ca79aca54650a-0.
INFO 02-05 12:26:10 logger.py:36] Received request cmpl-5535ee63dfd44c88a6ec4bf7a571f8d8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:10 async_llm_engine.py:173] Added request cmpl-5535ee63dfd44c88a6ec4bf7a571f8d8-0.
INFO 02-05 12:26:10 logger.py:36] Received request cmpl-193dd8340c6546e780003faddbdddba2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:10 async_llm_engine.py:173] Added request cmpl-193dd8340c6546e780003faddbdddba2-0.
INFO 02-05 12:26:10 logger.py:36] Received request cmpl-d790fcabcb24432786c7a98b0dbffb15-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:10 async_llm_engine.py:173] Added request cmpl-d790fcabcb24432786c7a98b0dbffb15-0.
INFO 02-05 12:26:10 logger.py:36] Received request cmpl-3b44a40f99924f6fb7486a0ff0db903a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:10 async_llm_engine.py:173] Added request cmpl-3b44a40f99924f6fb7486a0ff0db903a-0.
INFO 02-05 12:26:10 logger.py:36] Received request cmpl-660e3c7ec826413ebc3c0193efcaa57f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:10 async_llm_engine.py:173] Added request cmpl-660e3c7ec826413ebc3c0193efcaa57f-0.
INFO 02-05 12:26:10 logger.py:36] Received request cmpl-194fd19c2439446d9f264937dd344005-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:10 async_llm_engine.py:173] Added request cmpl-194fd19c2439446d9f264937dd344005-0.
INFO 02-05 12:26:10 logger.py:36] Received request cmpl-167606b7e7ec4923a2b3516513fcf2ad-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:10 async_llm_engine.py:173] Added request cmpl-167606b7e7ec4923a2b3516513fcf2ad-0.
INFO:     192.168.200.241:45950 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:11 async_llm_engine.py:140] Finished request cmpl-93dd5918cadb4adb8fea8012453799a1-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:11 async_llm_engine.py:140] Finished request cmpl-950b70d7817a4fa585f863d69bb272b2-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:11 async_llm_engine.py:140] Finished request cmpl-80f94f55b36b48fe839ca79aca54650a-0.
INFO 02-05 12:26:11 async_llm_engine.py:140] Finished request cmpl-5535ee63dfd44c88a6ec4bf7a571f8d8-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:11 async_llm_engine.py:140] Finished request cmpl-193dd8340c6546e780003faddbdddba2-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:11 async_llm_engine.py:140] Finished request cmpl-d790fcabcb24432786c7a98b0dbffb15-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:11 async_llm_engine.py:140] Finished request cmpl-3b44a40f99924f6fb7486a0ff0db903a-0.
INFO 02-05 12:26:11 async_llm_engine.py:140] Finished request cmpl-660e3c7ec826413ebc3c0193efcaa57f-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:11 async_llm_engine.py:140] Finished request cmpl-194fd19c2439446d9f264937dd344005-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:11 async_llm_engine.py:140] Finished request cmpl-167606b7e7ec4923a2b3516513fcf2ad-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:11 logger.py:36] Received request cmpl-2ee8cb785049419db1896991175eb918-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:11 async_llm_engine.py:173] Added request cmpl-2ee8cb785049419db1896991175eb918-0.
INFO 02-05 12:26:11 logger.py:36] Received request cmpl-dccfa4890cea4fdaa17bf2fb0f013242-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:11 async_llm_engine.py:173] Added request cmpl-dccfa4890cea4fdaa17bf2fb0f013242-0.
INFO 02-05 12:26:11 logger.py:36] Received request cmpl-002c0de89e8e496b8e7163488cdcc6d5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:11 async_llm_engine.py:173] Added request cmpl-002c0de89e8e496b8e7163488cdcc6d5-0.
INFO 02-05 12:26:11 logger.py:36] Received request cmpl-d3457e79688c40edadac4109f4de771b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:11 async_llm_engine.py:173] Added request cmpl-d3457e79688c40edadac4109f4de771b-0.
INFO 02-05 12:26:11 logger.py:36] Received request cmpl-a0156911ba0049cda8e015456bac8387-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:11 async_llm_engine.py:173] Added request cmpl-a0156911ba0049cda8e015456bac8387-0.
INFO 02-05 12:26:11 logger.py:36] Received request cmpl-78f0e611661445b991a64261a8314198-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO:     192.168.200.241:45936 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:11 async_llm_engine.py:173] Added request cmpl-78f0e611661445b991a64261a8314198-0.
INFO 02-05 12:26:11 logger.py:36] Received request cmpl-dda4364c96074c929096096ea652cb90-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:11 async_llm_engine.py:173] Added request cmpl-dda4364c96074c929096096ea652cb90-0.
INFO 02-05 12:26:11 logger.py:36] Received request cmpl-1a135112907e4ed992d902e7c39dcf47-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:11 async_llm_engine.py:173] Added request cmpl-1a135112907e4ed992d902e7c39dcf47-0.
INFO 02-05 12:26:11 logger.py:36] Received request cmpl-8d70971a4be34b4e94763a9bf692f258-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:11 async_llm_engine.py:173] Added request cmpl-8d70971a4be34b4e94763a9bf692f258-0.
INFO 02-05 12:26:11 logger.py:36] Received request cmpl-e4b006798d324be2bece096077b52249-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:11 async_llm_engine.py:173] Added request cmpl-e4b006798d324be2bece096077b52249-0.
INFO 02-05 12:26:12 async_llm_engine.py:140] Finished request cmpl-2ee8cb785049419db1896991175eb918-0.
INFO 02-05 12:26:12 async_llm_engine.py:140] Finished request cmpl-dccfa4890cea4fdaa17bf2fb0f013242-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:12 async_llm_engine.py:140] Finished request cmpl-002c0de89e8e496b8e7163488cdcc6d5-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:12 async_llm_engine.py:140] Finished request cmpl-d3457e79688c40edadac4109f4de771b-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:12 async_llm_engine.py:140] Finished request cmpl-a0156911ba0049cda8e015456bac8387-0.
INFO 02-05 12:26:12 async_llm_engine.py:140] Finished request cmpl-78f0e611661445b991a64261a8314198-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:12 async_llm_engine.py:140] Finished request cmpl-dda4364c96074c929096096ea652cb90-0.
INFO 02-05 12:26:12 async_llm_engine.py:140] Finished request cmpl-1a135112907e4ed992d902e7c39dcf47-0.
INFO 02-05 12:26:12 async_llm_engine.py:140] Finished request cmpl-8d70971a4be34b4e94763a9bf692f258-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:12 async_llm_engine.py:140] Finished request cmpl-e4b006798d324be2bece096077b52249-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:12 logger.py:36] Received request cmpl-ff4f5a39362b4367a5461831d6a2535a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:12 async_llm_engine.py:173] Added request cmpl-ff4f5a39362b4367a5461831d6a2535a-0.
INFO 02-05 12:26:12 logger.py:36] Received request cmpl-78cef186b06f47f8b693d0ed422053e0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO:     192.168.200.241:45884 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:12 async_llm_engine.py:173] Added request cmpl-78cef186b06f47f8b693d0ed422053e0-0.
INFO 02-05 12:26:12 logger.py:36] Received request cmpl-c0b9051414be4139ad8f3717279f411b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:12 async_llm_engine.py:173] Added request cmpl-c0b9051414be4139ad8f3717279f411b-0.
INFO 02-05 12:26:12 logger.py:36] Received request cmpl-fe035612347e4a4eb5e02e08550d22bc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:12 async_llm_engine.py:173] Added request cmpl-fe035612347e4a4eb5e02e08550d22bc-0.
INFO 02-05 12:26:12 logger.py:36] Received request cmpl-fbebfda09d634989b2377a4464289b29-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:12 async_llm_engine.py:173] Added request cmpl-fbebfda09d634989b2377a4464289b29-0.
INFO 02-05 12:26:12 logger.py:36] Received request cmpl-1d9315d311024885b6e08e7d65ed34d5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:12 async_llm_engine.py:173] Added request cmpl-1d9315d311024885b6e08e7d65ed34d5-0.
INFO 02-05 12:26:12 logger.py:36] Received request cmpl-0c27952442ad4b7a82c7cb59d00d1efc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:12 async_llm_engine.py:173] Added request cmpl-0c27952442ad4b7a82c7cb59d00d1efc-0.
INFO 02-05 12:26:12 logger.py:36] Received request cmpl-717e21df7ffa4e6d80af959193e98b6c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:12 async_llm_engine.py:173] Added request cmpl-717e21df7ffa4e6d80af959193e98b6c-0.
INFO 02-05 12:26:12 logger.py:36] Received request cmpl-11a01f1c8e2243d39b5abb088b11d080-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:12 async_llm_engine.py:173] Added request cmpl-11a01f1c8e2243d39b5abb088b11d080-0.
INFO 02-05 12:26:12 logger.py:36] Received request cmpl-18eb60ea070345c5a014d5ec11d17bd0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:12 async_llm_engine.py:173] Added request cmpl-18eb60ea070345c5a014d5ec11d17bd0-0.
INFO 02-05 12:26:13 async_llm_engine.py:140] Finished request cmpl-ff4f5a39362b4367a5461831d6a2535a-0.
INFO 02-05 12:26:13 async_llm_engine.py:140] Finished request cmpl-78cef186b06f47f8b693d0ed422053e0-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:13 async_llm_engine.py:140] Finished request cmpl-c0b9051414be4139ad8f3717279f411b-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:13 async_llm_engine.py:140] Finished request cmpl-fe035612347e4a4eb5e02e08550d22bc-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:13 metrics.py:396] Avg prompt throughput: 71.9 tokens/s, Avg generation throughput: 520.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-05 12:26:13 async_llm_engine.py:140] Finished request cmpl-fbebfda09d634989b2377a4464289b29-0.
INFO 02-05 12:26:13 async_llm_engine.py:140] Finished request cmpl-1d9315d311024885b6e08e7d65ed34d5-0.
INFO 02-05 12:26:13 async_llm_engine.py:140] Finished request cmpl-0c27952442ad4b7a82c7cb59d00d1efc-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:13 async_llm_engine.py:140] Finished request cmpl-717e21df7ffa4e6d80af959193e98b6c-0.
INFO 02-05 12:26:13 async_llm_engine.py:140] Finished request cmpl-11a01f1c8e2243d39b5abb088b11d080-0.
INFO 02-05 12:26:13 async_llm_engine.py:140] Finished request cmpl-18eb60ea070345c5a014d5ec11d17bd0-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:13 logger.py:36] Received request cmpl-1d98df77f2be44ea84f4290ca4b1c0fd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:13 async_llm_engine.py:173] Added request cmpl-1d98df77f2be44ea84f4290ca4b1c0fd-0.
INFO 02-05 12:26:13 logger.py:36] Received request cmpl-3e87c22d5b8a499eb271088b937d3f24-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:13 async_llm_engine.py:173] Added request cmpl-3e87c22d5b8a499eb271088b937d3f24-0.
INFO 02-05 12:26:13 logger.py:36] Received request cmpl-fb796e232c9a4be38e993ed504a9bded-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:13 async_llm_engine.py:173] Added request cmpl-fb796e232c9a4be38e993ed504a9bded-0.
INFO 02-05 12:26:13 logger.py:36] Received request cmpl-874969f11bd24a5085fb31634fd35c4c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:13 logger.py:36] Received request cmpl-4afcbf5265884b939ee4d551ef234cf0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:13 async_llm_engine.py:173] Added request cmpl-874969f11bd24a5085fb31634fd35c4c-0.
INFO 02-05 12:26:13 async_llm_engine.py:173] Added request cmpl-4afcbf5265884b939ee4d551ef234cf0-0.
INFO 02-05 12:26:13 logger.py:36] Received request cmpl-d78f53d3a2154d2cb21ce3ccead593dc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:13 async_llm_engine.py:173] Added request cmpl-d78f53d3a2154d2cb21ce3ccead593dc-0.
INFO 02-05 12:26:13 logger.py:36] Received request cmpl-84a5c7fae2c749ff834976fc8ccd0568-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:13 async_llm_engine.py:173] Added request cmpl-84a5c7fae2c749ff834976fc8ccd0568-0.
INFO 02-05 12:26:13 logger.py:36] Received request cmpl-81f6fbbe75614c2eb33e746a66536091-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:13 async_llm_engine.py:173] Added request cmpl-81f6fbbe75614c2eb33e746a66536091-0.
INFO 02-05 12:26:13 logger.py:36] Received request cmpl-0ca98928d29142c1a3414632cb3cd55c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:13 async_llm_engine.py:173] Added request cmpl-0ca98928d29142c1a3414632cb3cd55c-0.
INFO 02-05 12:26:13 logger.py:36] Received request cmpl-5eef7e6b61e146f6936efe2f8d0888c6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:13 async_llm_engine.py:173] Added request cmpl-5eef7e6b61e146f6936efe2f8d0888c6-0.
INFO 02-05 12:26:14 async_llm_engine.py:140] Finished request cmpl-1d98df77f2be44ea84f4290ca4b1c0fd-0.
INFO 02-05 12:26:14 async_llm_engine.py:140] Finished request cmpl-3e87c22d5b8a499eb271088b937d3f24-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:14 async_llm_engine.py:140] Finished request cmpl-fb796e232c9a4be38e993ed504a9bded-0.
INFO:     192.168.200.241:45944 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:14 async_llm_engine.py:140] Finished request cmpl-874969f11bd24a5085fb31634fd35c4c-0.
INFO 02-05 12:26:14 async_llm_engine.py:140] Finished request cmpl-4afcbf5265884b939ee4d551ef234cf0-0.
INFO 02-05 12:26:14 async_llm_engine.py:140] Finished request cmpl-d78f53d3a2154d2cb21ce3ccead593dc-0.
INFO 02-05 12:26:14 async_llm_engine.py:140] Finished request cmpl-84a5c7fae2c749ff834976fc8ccd0568-0.
INFO 02-05 12:26:14 async_llm_engine.py:140] Finished request cmpl-81f6fbbe75614c2eb33e746a66536091-0.
INFO 02-05 12:26:14 async_llm_engine.py:140] Finished request cmpl-0ca98928d29142c1a3414632cb3cd55c-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:14 async_llm_engine.py:140] Finished request cmpl-5eef7e6b61e146f6936efe2f8d0888c6-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:15 logger.py:36] Received request cmpl-f4ec3b1305ce42eb8a4dddbaddf32eae-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:15 async_llm_engine.py:173] Added request cmpl-f4ec3b1305ce42eb8a4dddbaddf32eae-0.
INFO 02-05 12:26:15 logger.py:36] Received request cmpl-6f7cbb0a84d34f819552f6547fe2316d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:15 async_llm_engine.py:173] Added request cmpl-6f7cbb0a84d34f819552f6547fe2316d-0.
INFO 02-05 12:26:15 logger.py:36] Received request cmpl-4c41710031fe477a956bf1f775c80689-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:15 async_llm_engine.py:173] Added request cmpl-4c41710031fe477a956bf1f775c80689-0.
INFO 02-05 12:26:15 logger.py:36] Received request cmpl-13ceacb456e54774b6f8b3e6ff48b534-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:15 async_llm_engine.py:173] Added request cmpl-13ceacb456e54774b6f8b3e6ff48b534-0.
INFO 02-05 12:26:15 logger.py:36] Received request cmpl-cd4e54c6196b49be8cd5f78216b590de-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:15 async_llm_engine.py:173] Added request cmpl-cd4e54c6196b49be8cd5f78216b590de-0.
INFO 02-05 12:26:15 logger.py:36] Received request cmpl-ad3d588bf6c743eb9e06034d67dccd6f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:15 async_llm_engine.py:173] Added request cmpl-ad3d588bf6c743eb9e06034d67dccd6f-0.
INFO 02-05 12:26:15 logger.py:36] Received request cmpl-f4d4f173906a4e0b96df3b3eda70379c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:15 async_llm_engine.py:173] Added request cmpl-f4d4f173906a4e0b96df3b3eda70379c-0.
INFO 02-05 12:26:15 logger.py:36] Received request cmpl-47e9e44a1dc848adb0f9becea5baf41a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:15 async_llm_engine.py:173] Added request cmpl-47e9e44a1dc848adb0f9becea5baf41a-0.
INFO 02-05 12:26:15 logger.py:36] Received request cmpl-97da3950ad024717bc805469ec0d13f7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:15 async_llm_engine.py:173] Added request cmpl-97da3950ad024717bc805469ec0d13f7-0.
INFO 02-05 12:26:15 logger.py:36] Received request cmpl-d15b858266974ff5ae61839cf02d40ac-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:15 async_llm_engine.py:173] Added request cmpl-d15b858266974ff5ae61839cf02d40ac-0.
INFO:     89.105.200.105:59214 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:59216 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:16 async_llm_engine.py:140] Finished request cmpl-f4ec3b1305ce42eb8a4dddbaddf32eae-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:16 async_llm_engine.py:140] Finished request cmpl-6f7cbb0a84d34f819552f6547fe2316d-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:16 async_llm_engine.py:140] Finished request cmpl-4c41710031fe477a956bf1f775c80689-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:16 async_llm_engine.py:140] Finished request cmpl-13ceacb456e54774b6f8b3e6ff48b534-0.
INFO 02-05 12:26:16 async_llm_engine.py:140] Finished request cmpl-cd4e54c6196b49be8cd5f78216b590de-0.
INFO 02-05 12:26:16 async_llm_engine.py:140] Finished request cmpl-ad3d588bf6c743eb9e06034d67dccd6f-0.
INFO 02-05 12:26:16 async_llm_engine.py:140] Finished request cmpl-f4d4f173906a4e0b96df3b3eda70379c-0.
INFO 02-05 12:26:16 async_llm_engine.py:140] Finished request cmpl-47e9e44a1dc848adb0f9becea5baf41a-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:16 async_llm_engine.py:140] Finished request cmpl-97da3950ad024717bc805469ec0d13f7-0.
INFO 02-05 12:26:16 async_llm_engine.py:140] Finished request cmpl-d15b858266974ff5ae61839cf02d40ac-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:16 logger.py:36] Received request cmpl-6cf55e8fc1c34073a7ab474d3c671a4d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:16 async_llm_engine.py:173] Added request cmpl-6cf55e8fc1c34073a7ab474d3c671a4d-0.
INFO 02-05 12:26:16 logger.py:36] Received request cmpl-1d76498d909a43429b2d43acabc70d43-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:16 async_llm_engine.py:173] Added request cmpl-1d76498d909a43429b2d43acabc70d43-0.
INFO 02-05 12:26:16 logger.py:36] Received request cmpl-eb12c568f922470484002271f2ec1fc8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:16 async_llm_engine.py:173] Added request cmpl-eb12c568f922470484002271f2ec1fc8-0.
INFO 02-05 12:26:16 logger.py:36] Received request cmpl-049749c10671454fa22a7471945eabae-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:16 logger.py:36] Received request cmpl-8f1901d17da94a8c835be5e46c3d5dfa-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:16 async_llm_engine.py:173] Added request cmpl-049749c10671454fa22a7471945eabae-0.
INFO 02-05 12:26:16 async_llm_engine.py:173] Added request cmpl-8f1901d17da94a8c835be5e46c3d5dfa-0.
INFO 02-05 12:26:16 logger.py:36] Received request cmpl-31b59dbb435d4578a95d822420b67e8d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:16 async_llm_engine.py:173] Added request cmpl-31b59dbb435d4578a95d822420b67e8d-0.
INFO 02-05 12:26:16 logger.py:36] Received request cmpl-ce13f1e8ebb54f77bc7320753bcff81f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:16 async_llm_engine.py:173] Added request cmpl-ce13f1e8ebb54f77bc7320753bcff81f-0.
INFO 02-05 12:26:16 logger.py:36] Received request cmpl-9678b01a8da04366bd4dff452fe294fc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:16 async_llm_engine.py:173] Added request cmpl-9678b01a8da04366bd4dff452fe294fc-0.
INFO 02-05 12:26:16 logger.py:36] Received request cmpl-08c722b177e743459faa6134435cb2e6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:16 async_llm_engine.py:173] Added request cmpl-08c722b177e743459faa6134435cb2e6-0.
INFO 02-05 12:26:16 logger.py:36] Received request cmpl-3c90ede4e6184b6c8be7bf64cc0d04df-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:16 async_llm_engine.py:173] Added request cmpl-3c90ede4e6184b6c8be7bf64cc0d04df-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:17 async_llm_engine.py:140] Finished request cmpl-6cf55e8fc1c34073a7ab474d3c671a4d-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:17 async_llm_engine.py:140] Finished request cmpl-1d76498d909a43429b2d43acabc70d43-0.
INFO 02-05 12:26:17 async_llm_engine.py:140] Finished request cmpl-eb12c568f922470484002271f2ec1fc8-0.
INFO 02-05 12:26:17 async_llm_engine.py:140] Finished request cmpl-049749c10671454fa22a7471945eabae-0.
INFO 02-05 12:26:17 async_llm_engine.py:140] Finished request cmpl-8f1901d17da94a8c835be5e46c3d5dfa-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:17 async_llm_engine.py:140] Finished request cmpl-31b59dbb435d4578a95d822420b67e8d-0.
INFO 02-05 12:26:17 async_llm_engine.py:140] Finished request cmpl-ce13f1e8ebb54f77bc7320753bcff81f-0.
INFO 02-05 12:26:17 async_llm_engine.py:140] Finished request cmpl-9678b01a8da04366bd4dff452fe294fc-0.
INFO 02-05 12:26:17 async_llm_engine.py:140] Finished request cmpl-08c722b177e743459faa6134435cb2e6-0.
INFO 02-05 12:26:17 async_llm_engine.py:140] Finished request cmpl-3c90ede4e6184b6c8be7bf64cc0d04df-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:17 logger.py:36] Received request cmpl-9762f3f2857c43d4b248f42ae0bbfe47-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:17 async_llm_engine.py:173] Added request cmpl-9762f3f2857c43d4b248f42ae0bbfe47-0.
INFO 02-05 12:26:17 logger.py:36] Received request cmpl-cc6495cec3424f85a049ce0d3312abb9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:17 async_llm_engine.py:173] Added request cmpl-cc6495cec3424f85a049ce0d3312abb9-0.
INFO 02-05 12:26:17 logger.py:36] Received request cmpl-0c28aac5f0d14228803743ba7480190f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:17 async_llm_engine.py:173] Added request cmpl-0c28aac5f0d14228803743ba7480190f-0.
INFO 02-05 12:26:17 logger.py:36] Received request cmpl-668d6ef77e9141388f62bcfe6f1ace8e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:17 async_llm_engine.py:173] Added request cmpl-668d6ef77e9141388f62bcfe6f1ace8e-0.
INFO 02-05 12:26:17 logger.py:36] Received request cmpl-194b3666ce234e46afdb00854d3994c7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:17 async_llm_engine.py:173] Added request cmpl-194b3666ce234e46afdb00854d3994c7-0.
INFO 02-05 12:26:17 logger.py:36] Received request cmpl-480ab6877d2c4bb983298be4f64ca6d9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:17 async_llm_engine.py:173] Added request cmpl-480ab6877d2c4bb983298be4f64ca6d9-0.
INFO 02-05 12:26:17 logger.py:36] Received request cmpl-c5681b79a1034d8592cf2474fcd6a13e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:17 async_llm_engine.py:173] Added request cmpl-c5681b79a1034d8592cf2474fcd6a13e-0.
INFO 02-05 12:26:17 logger.py:36] Received request cmpl-8d09ac4352df4e48ba959d53c0ec2f83-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:17 async_llm_engine.py:173] Added request cmpl-8d09ac4352df4e48ba959d53c0ec2f83-0.
INFO 02-05 12:26:17 logger.py:36] Received request cmpl-d60ebd199dfa487bb3039e1d22fbe87c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:17 async_llm_engine.py:173] Added request cmpl-d60ebd199dfa487bb3039e1d22fbe87c-0.
INFO 02-05 12:26:17 logger.py:36] Received request cmpl-4718cee488cb4ad9a36402c23bc9ab2b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:17 async_llm_engine.py:173] Added request cmpl-4718cee488cb4ad9a36402c23bc9ab2b-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:18 async_llm_engine.py:140] Finished request cmpl-9762f3f2857c43d4b248f42ae0bbfe47-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:18 async_llm_engine.py:140] Finished request cmpl-cc6495cec3424f85a049ce0d3312abb9-0.
INFO 02-05 12:26:18 async_llm_engine.py:140] Finished request cmpl-0c28aac5f0d14228803743ba7480190f-0.
INFO 02-05 12:26:18 async_llm_engine.py:140] Finished request cmpl-668d6ef77e9141388f62bcfe6f1ace8e-0.
INFO 02-05 12:26:18 async_llm_engine.py:140] Finished request cmpl-194b3666ce234e46afdb00854d3994c7-0.
INFO 02-05 12:26:18 async_llm_engine.py:140] Finished request cmpl-480ab6877d2c4bb983298be4f64ca6d9-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:18 async_llm_engine.py:140] Finished request cmpl-c5681b79a1034d8592cf2474fcd6a13e-0.
INFO 02-05 12:26:18 async_llm_engine.py:140] Finished request cmpl-8d09ac4352df4e48ba959d53c0ec2f83-0.
INFO 02-05 12:26:18 async_llm_engine.py:140] Finished request cmpl-d60ebd199dfa487bb3039e1d22fbe87c-0.
INFO 02-05 12:26:18 async_llm_engine.py:140] Finished request cmpl-4718cee488cb4ad9a36402c23bc9ab2b-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:18 logger.py:36] Received request cmpl-46d74c146d40451d9b5c55c1eb6bf9b8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:18 async_llm_engine.py:173] Added request cmpl-46d74c146d40451d9b5c55c1eb6bf9b8-0.
INFO 02-05 12:26:18 logger.py:36] Received request cmpl-3c3179f19d63459387ce42d61fd5bce5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:18 async_llm_engine.py:173] Added request cmpl-3c3179f19d63459387ce42d61fd5bce5-0.
INFO 02-05 12:26:18 logger.py:36] Received request cmpl-24faec1f6e1647e084a480066f139d1f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:18 async_llm_engine.py:173] Added request cmpl-24faec1f6e1647e084a480066f139d1f-0.
INFO 02-05 12:26:18 logger.py:36] Received request cmpl-5d58f35f266240c0afaa4881e1c622c8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:18 async_llm_engine.py:173] Added request cmpl-5d58f35f266240c0afaa4881e1c622c8-0.
INFO 02-05 12:26:18 logger.py:36] Received request cmpl-82543abc5b154124a48e484c83e97a6a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:18 async_llm_engine.py:173] Added request cmpl-82543abc5b154124a48e484c83e97a6a-0.
INFO 02-05 12:26:18 metrics.py:396] Avg prompt throughput: 72.5 tokens/s, Avg generation throughput: 503.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:26:18 logger.py:36] Received request cmpl-bd3f2f227c834c7abc81573cb5f4f7d7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:18 async_llm_engine.py:173] Added request cmpl-bd3f2f227c834c7abc81573cb5f4f7d7-0.
INFO 02-05 12:26:18 logger.py:36] Received request cmpl-41353e722f9242f3be5c646afc1bc397-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:18 async_llm_engine.py:173] Added request cmpl-41353e722f9242f3be5c646afc1bc397-0.
INFO 02-05 12:26:18 logger.py:36] Received request cmpl-50d380d5de4b4305b32dce048a08f5aa-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:18 async_llm_engine.py:173] Added request cmpl-50d380d5de4b4305b32dce048a08f5aa-0.
INFO 02-05 12:26:18 logger.py:36] Received request cmpl-d9a4ca965ef04d81990060b50a4a6b33-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:18 async_llm_engine.py:173] Added request cmpl-d9a4ca965ef04d81990060b50a4a6b33-0.
INFO 02-05 12:26:18 logger.py:36] Received request cmpl-683532903ee84c8cb4a207fbebf523cc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:18 async_llm_engine.py:173] Added request cmpl-683532903ee84c8cb4a207fbebf523cc-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:19 async_llm_engine.py:140] Finished request cmpl-46d74c146d40451d9b5c55c1eb6bf9b8-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:19 async_llm_engine.py:140] Finished request cmpl-3c3179f19d63459387ce42d61fd5bce5-0.
INFO 02-05 12:26:19 async_llm_engine.py:140] Finished request cmpl-24faec1f6e1647e084a480066f139d1f-0.
INFO 02-05 12:26:19 async_llm_engine.py:140] Finished request cmpl-5d58f35f266240c0afaa4881e1c622c8-0.
INFO 02-05 12:26:19 async_llm_engine.py:140] Finished request cmpl-82543abc5b154124a48e484c83e97a6a-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:19 async_llm_engine.py:140] Finished request cmpl-bd3f2f227c834c7abc81573cb5f4f7d7-0.
INFO 02-05 12:26:19 async_llm_engine.py:140] Finished request cmpl-41353e722f9242f3be5c646afc1bc397-0.
INFO 02-05 12:26:19 async_llm_engine.py:140] Finished request cmpl-50d380d5de4b4305b32dce048a08f5aa-0.
INFO 02-05 12:26:19 async_llm_engine.py:140] Finished request cmpl-d9a4ca965ef04d81990060b50a4a6b33-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:19 async_llm_engine.py:140] Finished request cmpl-683532903ee84c8cb4a207fbebf523cc-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:20 logger.py:36] Received request cmpl-385f5f5baba64b6189a4a4715a1c3a13-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:20 async_llm_engine.py:173] Added request cmpl-385f5f5baba64b6189a4a4715a1c3a13-0.
INFO 02-05 12:26:20 logger.py:36] Received request cmpl-e9a34d929ae74ead9e1546bb83e6c88d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:20 async_llm_engine.py:173] Added request cmpl-e9a34d929ae74ead9e1546bb83e6c88d-0.
INFO 02-05 12:26:20 logger.py:36] Received request cmpl-b75c52769d35458aa42b9e00e698fdd7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:20 async_llm_engine.py:173] Added request cmpl-b75c52769d35458aa42b9e00e698fdd7-0.
INFO 02-05 12:26:20 logger.py:36] Received request cmpl-7b7d8f67f73c492e8cd7e20ca6e8fcc7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:20 async_llm_engine.py:173] Added request cmpl-7b7d8f67f73c492e8cd7e20ca6e8fcc7-0.
INFO 02-05 12:26:20 logger.py:36] Received request cmpl-2a9d5cff472d4847ba7494f4225c05f6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:20 async_llm_engine.py:173] Added request cmpl-2a9d5cff472d4847ba7494f4225c05f6-0.
INFO 02-05 12:26:20 logger.py:36] Received request cmpl-66d3574160ac4f77b58ceece0bdba7ee-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:20 async_llm_engine.py:173] Added request cmpl-66d3574160ac4f77b58ceece0bdba7ee-0.
INFO 02-05 12:26:20 logger.py:36] Received request cmpl-b079d0ec440c4b88b78b0c36fd8f7044-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:20 async_llm_engine.py:173] Added request cmpl-b079d0ec440c4b88b78b0c36fd8f7044-0.
INFO 02-05 12:26:20 logger.py:36] Received request cmpl-d327aec500bf45c59ae33c2e9d959a71-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:20 async_llm_engine.py:173] Added request cmpl-d327aec500bf45c59ae33c2e9d959a71-0.
INFO 02-05 12:26:20 logger.py:36] Received request cmpl-66997910ed544302b5e9e76a6d5d1ad2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:20 async_llm_engine.py:173] Added request cmpl-66997910ed544302b5e9e76a6d5d1ad2-0.
INFO 02-05 12:26:20 logger.py:36] Received request cmpl-ba9490b6cc87479e8f358cb9c7f10f0e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:20 async_llm_engine.py:173] Added request cmpl-ba9490b6cc87479e8f358cb9c7f10f0e-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:21 async_llm_engine.py:140] Finished request cmpl-385f5f5baba64b6189a4a4715a1c3a13-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:21 async_llm_engine.py:140] Finished request cmpl-e9a34d929ae74ead9e1546bb83e6c88d-0.
INFO 02-05 12:26:21 async_llm_engine.py:140] Finished request cmpl-b75c52769d35458aa42b9e00e698fdd7-0.
INFO 02-05 12:26:21 async_llm_engine.py:140] Finished request cmpl-7b7d8f67f73c492e8cd7e20ca6e8fcc7-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:21 async_llm_engine.py:140] Finished request cmpl-2a9d5cff472d4847ba7494f4225c05f6-0.
INFO 02-05 12:26:21 async_llm_engine.py:140] Finished request cmpl-66d3574160ac4f77b58ceece0bdba7ee-0.
INFO 02-05 12:26:21 async_llm_engine.py:140] Finished request cmpl-b079d0ec440c4b88b78b0c36fd8f7044-0.
INFO 02-05 12:26:21 async_llm_engine.py:140] Finished request cmpl-d327aec500bf45c59ae33c2e9d959a71-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:21 async_llm_engine.py:140] Finished request cmpl-66997910ed544302b5e9e76a6d5d1ad2-0.
INFO 02-05 12:26:21 async_llm_engine.py:140] Finished request cmpl-ba9490b6cc87479e8f358cb9c7f10f0e-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:21 logger.py:36] Received request cmpl-46251552310847a99038edd2dc2574f3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:21 async_llm_engine.py:173] Added request cmpl-46251552310847a99038edd2dc2574f3-0.
INFO 02-05 12:26:21 logger.py:36] Received request cmpl-c32f6cd297184206b759e2cf27a0204f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:21 async_llm_engine.py:173] Added request cmpl-c32f6cd297184206b759e2cf27a0204f-0.
INFO 02-05 12:26:21 logger.py:36] Received request cmpl-add896025b9e42908fef5119c547acef-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:21 async_llm_engine.py:173] Added request cmpl-add896025b9e42908fef5119c547acef-0.
INFO 02-05 12:26:21 logger.py:36] Received request cmpl-ff9e7a3ae85545f2bb922f271f54248d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:21 async_llm_engine.py:173] Added request cmpl-ff9e7a3ae85545f2bb922f271f54248d-0.
INFO 02-05 12:26:21 logger.py:36] Received request cmpl-5e2e5a260b744d5fb4bf31b485c2bd3e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:21 async_llm_engine.py:173] Added request cmpl-5e2e5a260b744d5fb4bf31b485c2bd3e-0.
INFO 02-05 12:26:21 logger.py:36] Received request cmpl-fbfd1560531f4900bc9430aa9db93b5c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:21 async_llm_engine.py:173] Added request cmpl-fbfd1560531f4900bc9430aa9db93b5c-0.
INFO 02-05 12:26:21 logger.py:36] Received request cmpl-b0dd2d1b933f4bab87ea833e5205be87-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:21 logger.py:36] Received request cmpl-25f62d1db2bd4dcaaa10fcbc5ce472cf-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:21 async_llm_engine.py:173] Added request cmpl-b0dd2d1b933f4bab87ea833e5205be87-0.
INFO 02-05 12:26:21 async_llm_engine.py:173] Added request cmpl-25f62d1db2bd4dcaaa10fcbc5ce472cf-0.
INFO 02-05 12:26:21 logger.py:36] Received request cmpl-61b660feece14cd69588b9083fd5cd80-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:21 async_llm_engine.py:173] Added request cmpl-61b660feece14cd69588b9083fd5cd80-0.
INFO 02-05 12:26:21 logger.py:36] Received request cmpl-98a990b290ec404294a5c7f25b7c0321-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:21 async_llm_engine.py:173] Added request cmpl-98a990b290ec404294a5c7f25b7c0321-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:22 async_llm_engine.py:140] Finished request cmpl-46251552310847a99038edd2dc2574f3-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:22 async_llm_engine.py:140] Finished request cmpl-c32f6cd297184206b759e2cf27a0204f-0.
INFO 02-05 12:26:22 async_llm_engine.py:140] Finished request cmpl-add896025b9e42908fef5119c547acef-0.
INFO 02-05 12:26:22 async_llm_engine.py:140] Finished request cmpl-ff9e7a3ae85545f2bb922f271f54248d-0.
INFO 02-05 12:26:22 async_llm_engine.py:140] Finished request cmpl-5e2e5a260b744d5fb4bf31b485c2bd3e-0.
INFO 02-05 12:26:22 async_llm_engine.py:140] Finished request cmpl-fbfd1560531f4900bc9430aa9db93b5c-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:22 async_llm_engine.py:140] Finished request cmpl-b0dd2d1b933f4bab87ea833e5205be87-0.
INFO 02-05 12:26:22 async_llm_engine.py:140] Finished request cmpl-25f62d1db2bd4dcaaa10fcbc5ce472cf-0.
INFO 02-05 12:26:22 async_llm_engine.py:140] Finished request cmpl-61b660feece14cd69588b9083fd5cd80-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:22 async_llm_engine.py:140] Finished request cmpl-98a990b290ec404294a5c7f25b7c0321-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:22 logger.py:36] Received request cmpl-2ac368dee7fb4e5e8729b0fe271cc301-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:22 async_llm_engine.py:173] Added request cmpl-2ac368dee7fb4e5e8729b0fe271cc301-0.
INFO 02-05 12:26:22 logger.py:36] Received request cmpl-8b94eed90e5646208113c47ca541967e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:22 async_llm_engine.py:173] Added request cmpl-8b94eed90e5646208113c47ca541967e-0.
INFO 02-05 12:26:22 logger.py:36] Received request cmpl-c14eb8cd4fe74932afcdb53c0c6db8c6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:22 async_llm_engine.py:173] Added request cmpl-c14eb8cd4fe74932afcdb53c0c6db8c6-0.
INFO 02-05 12:26:22 logger.py:36] Received request cmpl-0de5ce28da8f4736b8bd1a5dda2aed13-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:22 async_llm_engine.py:173] Added request cmpl-0de5ce28da8f4736b8bd1a5dda2aed13-0.
INFO 02-05 12:26:22 logger.py:36] Received request cmpl-02ca68e901ba470bbd10d55e7a8040a7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:22 async_llm_engine.py:173] Added request cmpl-02ca68e901ba470bbd10d55e7a8040a7-0.
INFO 02-05 12:26:22 logger.py:36] Received request cmpl-e5aa64ba027548d59b5865fe7612c18e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:22 logger.py:36] Received request cmpl-4ae16f0fd814426ba8742bcdc0bb4f21-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:22 async_llm_engine.py:173] Added request cmpl-e5aa64ba027548d59b5865fe7612c18e-0.
INFO 02-05 12:26:22 async_llm_engine.py:173] Added request cmpl-4ae16f0fd814426ba8742bcdc0bb4f21-0.
INFO 02-05 12:26:22 logger.py:36] Received request cmpl-2a5ab64c868e493e9522d710a916a880-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:22 async_llm_engine.py:173] Added request cmpl-2a5ab64c868e493e9522d710a916a880-0.
INFO 02-05 12:26:22 logger.py:36] Received request cmpl-9740372ff8a140ae904740e384b4b199-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:22 async_llm_engine.py:173] Added request cmpl-9740372ff8a140ae904740e384b4b199-0.
INFO 02-05 12:26:22 logger.py:36] Received request cmpl-8a4af00114204666b5a02d78831d7b0d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:22 async_llm_engine.py:173] Added request cmpl-8a4af00114204666b5a02d78831d7b0d-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:23 async_llm_engine.py:140] Finished request cmpl-2ac368dee7fb4e5e8729b0fe271cc301-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:23 async_llm_engine.py:140] Finished request cmpl-8b94eed90e5646208113c47ca541967e-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:23 async_llm_engine.py:140] Finished request cmpl-c14eb8cd4fe74932afcdb53c0c6db8c6-0.
INFO 02-05 12:26:23 async_llm_engine.py:140] Finished request cmpl-0de5ce28da8f4736b8bd1a5dda2aed13-0.
INFO 02-05 12:26:23 async_llm_engine.py:140] Finished request cmpl-02ca68e901ba470bbd10d55e7a8040a7-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:23 async_llm_engine.py:140] Finished request cmpl-e5aa64ba027548d59b5865fe7612c18e-0.
INFO 02-05 12:26:23 async_llm_engine.py:140] Finished request cmpl-4ae16f0fd814426ba8742bcdc0bb4f21-0.
INFO 02-05 12:26:23 async_llm_engine.py:140] Finished request cmpl-2a5ab64c868e493e9522d710a916a880-0.
INFO 02-05 12:26:23 async_llm_engine.py:140] Finished request cmpl-9740372ff8a140ae904740e384b4b199-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:23 logger.py:36] Received request cmpl-b05cc6afb9cc4571ba9046c643e823d3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:23 async_llm_engine.py:173] Added request cmpl-b05cc6afb9cc4571ba9046c643e823d3-0.
INFO 02-05 12:26:23 async_llm_engine.py:140] Finished request cmpl-8a4af00114204666b5a02d78831d7b0d-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:23 logger.py:36] Received request cmpl-75fca2d4e1d14fd2a1f18b6aa699cf34-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:23 async_llm_engine.py:173] Added request cmpl-75fca2d4e1d14fd2a1f18b6aa699cf34-0.
INFO 02-05 12:26:23 logger.py:36] Received request cmpl-1d3b040b8b7349c9bac46a8fc19262dd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:23 async_llm_engine.py:173] Added request cmpl-1d3b040b8b7349c9bac46a8fc19262dd-0.
INFO 02-05 12:26:23 logger.py:36] Received request cmpl-c48fac294f2c483db306c6b773d83244-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:23 async_llm_engine.py:173] Added request cmpl-c48fac294f2c483db306c6b773d83244-0.
INFO 02-05 12:26:23 logger.py:36] Received request cmpl-24f569e132a34de1b2e6a033f7dc404b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:23 async_llm_engine.py:173] Added request cmpl-24f569e132a34de1b2e6a033f7dc404b-0.
INFO 02-05 12:26:23 logger.py:36] Received request cmpl-a3bfa3e857e6438ea9fd90ff3f4fe033-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:23 async_llm_engine.py:173] Added request cmpl-a3bfa3e857e6438ea9fd90ff3f4fe033-0.
INFO 02-05 12:26:23 logger.py:36] Received request cmpl-361b092c9b1a4885acdcbe8b98adecdc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:23 async_llm_engine.py:173] Added request cmpl-361b092c9b1a4885acdcbe8b98adecdc-0.
INFO 02-05 12:26:23 logger.py:36] Received request cmpl-4629d6b554dc4932a6f271404610712f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:23 async_llm_engine.py:173] Added request cmpl-4629d6b554dc4932a6f271404610712f-0.
INFO 02-05 12:26:23 logger.py:36] Received request cmpl-be56fafe9632466ba37cb43e3648b7a1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:23 async_llm_engine.py:173] Added request cmpl-be56fafe9632466ba37cb43e3648b7a1-0.
INFO 02-05 12:26:23 metrics.py:396] Avg prompt throughput: 86.4 tokens/s, Avg generation throughput: 515.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:26:23 logger.py:36] Received request cmpl-e53dc8ea521243afa61da63122fc45fc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:23 async_llm_engine.py:173] Added request cmpl-e53dc8ea521243afa61da63122fc45fc-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:24 async_llm_engine.py:140] Finished request cmpl-b05cc6afb9cc4571ba9046c643e823d3-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:24 async_llm_engine.py:140] Finished request cmpl-75fca2d4e1d14fd2a1f18b6aa699cf34-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:24 async_llm_engine.py:140] Finished request cmpl-1d3b040b8b7349c9bac46a8fc19262dd-0.
INFO 02-05 12:26:24 async_llm_engine.py:140] Finished request cmpl-c48fac294f2c483db306c6b773d83244-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:24 async_llm_engine.py:140] Finished request cmpl-24f569e132a34de1b2e6a033f7dc404b-0.
INFO 02-05 12:26:24 async_llm_engine.py:140] Finished request cmpl-a3bfa3e857e6438ea9fd90ff3f4fe033-0.
INFO 02-05 12:26:24 async_llm_engine.py:140] Finished request cmpl-361b092c9b1a4885acdcbe8b98adecdc-0.
INFO 02-05 12:26:24 async_llm_engine.py:140] Finished request cmpl-4629d6b554dc4932a6f271404610712f-0.
INFO 02-05 12:26:24 async_llm_engine.py:140] Finished request cmpl-be56fafe9632466ba37cb43e3648b7a1-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:24 async_llm_engine.py:140] Finished request cmpl-e53dc8ea521243afa61da63122fc45fc-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:24 logger.py:36] Received request cmpl-313e4d70c77545eaa7f13677ebea9f7d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:24 async_llm_engine.py:173] Added request cmpl-313e4d70c77545eaa7f13677ebea9f7d-0.
INFO 02-05 12:26:24 logger.py:36] Received request cmpl-3bd4d5a8c6564ac19387f7b5ecd16b77-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:24 async_llm_engine.py:173] Added request cmpl-3bd4d5a8c6564ac19387f7b5ecd16b77-0.
INFO 02-05 12:26:24 logger.py:36] Received request cmpl-4e4f7286d6c048d0b390c2fcf3b3a2a1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:24 async_llm_engine.py:173] Added request cmpl-4e4f7286d6c048d0b390c2fcf3b3a2a1-0.
INFO 02-05 12:26:24 logger.py:36] Received request cmpl-32ed474cb5de4155a81b274729d19b40-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:24 async_llm_engine.py:173] Added request cmpl-32ed474cb5de4155a81b274729d19b40-0.
INFO 02-05 12:26:25 logger.py:36] Received request cmpl-4efeeb6db4634186b4d4c84d01bb4724-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:25 async_llm_engine.py:173] Added request cmpl-4efeeb6db4634186b4d4c84d01bb4724-0.
INFO 02-05 12:26:25 logger.py:36] Received request cmpl-deb28778105b40609e681c88ffe0d194-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:25 async_llm_engine.py:173] Added request cmpl-deb28778105b40609e681c88ffe0d194-0.
INFO 02-05 12:26:25 logger.py:36] Received request cmpl-0f6450ea603347b7a4ce8e16a5385b86-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:25 async_llm_engine.py:173] Added request cmpl-0f6450ea603347b7a4ce8e16a5385b86-0.
INFO 02-05 12:26:25 logger.py:36] Received request cmpl-81d99263340445f5a2031d5897234bfc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:25 async_llm_engine.py:173] Added request cmpl-81d99263340445f5a2031d5897234bfc-0.
INFO 02-05 12:26:25 logger.py:36] Received request cmpl-9b2b96c1978f44279b028edb41e14f3f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:25 async_llm_engine.py:173] Added request cmpl-9b2b96c1978f44279b028edb41e14f3f-0.
INFO 02-05 12:26:25 logger.py:36] Received request cmpl-82c7230673284ea2bd79128e80343be8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:25 async_llm_engine.py:173] Added request cmpl-82c7230673284ea2bd79128e80343be8-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:53776 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:53778 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:26:26 async_llm_engine.py:140] Finished request cmpl-313e4d70c77545eaa7f13677ebea9f7d-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:26 async_llm_engine.py:140] Finished request cmpl-3bd4d5a8c6564ac19387f7b5ecd16b77-0.
INFO 02-05 12:26:26 async_llm_engine.py:140] Finished request cmpl-4e4f7286d6c048d0b390c2fcf3b3a2a1-0.
INFO 02-05 12:26:26 async_llm_engine.py:140] Finished request cmpl-32ed474cb5de4155a81b274729d19b40-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:26 async_llm_engine.py:140] Finished request cmpl-4efeeb6db4634186b4d4c84d01bb4724-0.
INFO 02-05 12:26:26 async_llm_engine.py:140] Finished request cmpl-deb28778105b40609e681c88ffe0d194-0.
INFO 02-05 12:26:26 async_llm_engine.py:140] Finished request cmpl-0f6450ea603347b7a4ce8e16a5385b86-0.
INFO 02-05 12:26:26 async_llm_engine.py:140] Finished request cmpl-81d99263340445f5a2031d5897234bfc-0.
INFO 02-05 12:26:26 async_llm_engine.py:140] Finished request cmpl-9b2b96c1978f44279b028edb41e14f3f-0.
INFO 02-05 12:26:26 async_llm_engine.py:140] Finished request cmpl-82c7230673284ea2bd79128e80343be8-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:26 logger.py:36] Received request cmpl-e41487161e1b40e796566990fa2bbf39-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:26 logger.py:36] Received request cmpl-780a036071b94dfeb5cb0566d19c89d1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:26 async_llm_engine.py:173] Added request cmpl-e41487161e1b40e796566990fa2bbf39-0.
INFO 02-05 12:26:26 async_llm_engine.py:173] Added request cmpl-780a036071b94dfeb5cb0566d19c89d1-0.
INFO 02-05 12:26:26 logger.py:36] Received request cmpl-e7fb8bc094dd4941ad3845fd476b1511-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:26 async_llm_engine.py:173] Added request cmpl-e7fb8bc094dd4941ad3845fd476b1511-0.
INFO 02-05 12:26:26 logger.py:36] Received request cmpl-3287d4e8c41d4ed89175f6ce83f8d666-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:26 async_llm_engine.py:173] Added request cmpl-3287d4e8c41d4ed89175f6ce83f8d666-0.
INFO 02-05 12:26:26 logger.py:36] Received request cmpl-bfbad3b4b44840e6a7e8a4d9f54943a3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:26 async_llm_engine.py:173] Added request cmpl-bfbad3b4b44840e6a7e8a4d9f54943a3-0.
INFO 02-05 12:26:26 logger.py:36] Received request cmpl-674c7f57138f48cbb362434ae7262d38-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:26 async_llm_engine.py:173] Added request cmpl-674c7f57138f48cbb362434ae7262d38-0.
INFO 02-05 12:26:26 logger.py:36] Received request cmpl-0a3c95fa3688427ba094c20b0b3810a2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:26 async_llm_engine.py:173] Added request cmpl-0a3c95fa3688427ba094c20b0b3810a2-0.
INFO 02-05 12:26:26 logger.py:36] Received request cmpl-3f063bee40494092b4ba1c6082eb569f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:26 async_llm_engine.py:173] Added request cmpl-3f063bee40494092b4ba1c6082eb569f-0.
INFO 02-05 12:26:26 logger.py:36] Received request cmpl-b313e75587c6403d9c73c7038c754078-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:26 async_llm_engine.py:173] Added request cmpl-b313e75587c6403d9c73c7038c754078-0.
INFO 02-05 12:26:26 logger.py:36] Received request cmpl-664aa326dc724162b2638670ef7be3fc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:26 async_llm_engine.py:173] Added request cmpl-664aa326dc724162b2638670ef7be3fc-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:27 async_llm_engine.py:140] Finished request cmpl-e41487161e1b40e796566990fa2bbf39-0.
INFO 02-05 12:26:27 async_llm_engine.py:140] Finished request cmpl-780a036071b94dfeb5cb0566d19c89d1-0.
INFO 02-05 12:26:27 async_llm_engine.py:140] Finished request cmpl-e7fb8bc094dd4941ad3845fd476b1511-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:27 async_llm_engine.py:140] Finished request cmpl-3287d4e8c41d4ed89175f6ce83f8d666-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:27 async_llm_engine.py:140] Finished request cmpl-bfbad3b4b44840e6a7e8a4d9f54943a3-0.
INFO 02-05 12:26:27 async_llm_engine.py:140] Finished request cmpl-674c7f57138f48cbb362434ae7262d38-0.
INFO 02-05 12:26:27 async_llm_engine.py:140] Finished request cmpl-0a3c95fa3688427ba094c20b0b3810a2-0.
INFO 02-05 12:26:27 async_llm_engine.py:140] Finished request cmpl-3f063bee40494092b4ba1c6082eb569f-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:27 async_llm_engine.py:140] Finished request cmpl-b313e75587c6403d9c73c7038c754078-0.
INFO 02-05 12:26:27 async_llm_engine.py:140] Finished request cmpl-664aa326dc724162b2638670ef7be3fc-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:27 logger.py:36] Received request cmpl-77fab10e21a94f77ad5b214963e352c9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:27 async_llm_engine.py:173] Added request cmpl-77fab10e21a94f77ad5b214963e352c9-0.
INFO 02-05 12:26:27 logger.py:36] Received request cmpl-4e9239379802490c8f854be20c825f8b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:27 async_llm_engine.py:173] Added request cmpl-4e9239379802490c8f854be20c825f8b-0.
INFO 02-05 12:26:27 logger.py:36] Received request cmpl-d868fd4c866d468686e18f5d7e046244-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:27 async_llm_engine.py:173] Added request cmpl-d868fd4c866d468686e18f5d7e046244-0.
INFO 02-05 12:26:27 logger.py:36] Received request cmpl-1f77924b5ef34b0fafe6cc5c4ac5ad63-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:27 logger.py:36] Received request cmpl-effeb264833c42aa9269d9a8e1a06835-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:27 async_llm_engine.py:173] Added request cmpl-1f77924b5ef34b0fafe6cc5c4ac5ad63-0.
INFO 02-05 12:26:27 async_llm_engine.py:173] Added request cmpl-effeb264833c42aa9269d9a8e1a06835-0.
INFO 02-05 12:26:27 logger.py:36] Received request cmpl-00a5f4be4b9c4209a1a7c01fdafadea2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:27 async_llm_engine.py:173] Added request cmpl-00a5f4be4b9c4209a1a7c01fdafadea2-0.
INFO 02-05 12:26:27 logger.py:36] Received request cmpl-05587e070433496bb9638c0832e3b22c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:27 async_llm_engine.py:173] Added request cmpl-05587e070433496bb9638c0832e3b22c-0.
INFO 02-05 12:26:27 logger.py:36] Received request cmpl-25bdabf8baac4acf83c339be108b42e2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:27 async_llm_engine.py:173] Added request cmpl-25bdabf8baac4acf83c339be108b42e2-0.
INFO 02-05 12:26:27 logger.py:36] Received request cmpl-d789c089283b4e52be625387203c2645-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:27 async_llm_engine.py:173] Added request cmpl-d789c089283b4e52be625387203c2645-0.
INFO 02-05 12:26:27 logger.py:36] Received request cmpl-d934fe204d34465c8a07ef7986877cb8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:27 async_llm_engine.py:173] Added request cmpl-d934fe204d34465c8a07ef7986877cb8-0.
INFO 02-05 12:26:28 async_llm_engine.py:140] Finished request cmpl-77fab10e21a94f77ad5b214963e352c9-0.
INFO 02-05 12:26:28 async_llm_engine.py:140] Finished request cmpl-4e9239379802490c8f854be20c825f8b-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:28 async_llm_engine.py:140] Finished request cmpl-d868fd4c866d468686e18f5d7e046244-0.
INFO 02-05 12:26:28 async_llm_engine.py:140] Finished request cmpl-1f77924b5ef34b0fafe6cc5c4ac5ad63-0.
INFO 02-05 12:26:28 async_llm_engine.py:140] Finished request cmpl-effeb264833c42aa9269d9a8e1a06835-0.
INFO 02-05 12:26:28 async_llm_engine.py:140] Finished request cmpl-00a5f4be4b9c4209a1a7c01fdafadea2-0.
INFO 02-05 12:26:28 async_llm_engine.py:140] Finished request cmpl-05587e070433496bb9638c0832e3b22c-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:28 async_llm_engine.py:140] Finished request cmpl-25bdabf8baac4acf83c339be108b42e2-0.
INFO 02-05 12:26:28 async_llm_engine.py:140] Finished request cmpl-d789c089283b4e52be625387203c2645-0.
INFO 02-05 12:26:28 async_llm_engine.py:140] Finished request cmpl-d934fe204d34465c8a07ef7986877cb8-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:28 logger.py:36] Received request cmpl-ed23be58a5e04149b4e3e8ff93057f3b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:28 async_llm_engine.py:173] Added request cmpl-ed23be58a5e04149b4e3e8ff93057f3b-0.
INFO 02-05 12:26:28 logger.py:36] Received request cmpl-3c5a37db092e47869d68142b2b19e49e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:28 logger.py:36] Received request cmpl-50c7804468fa40a2acf7de41250ce320-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:28 async_llm_engine.py:173] Added request cmpl-3c5a37db092e47869d68142b2b19e49e-0.
INFO 02-05 12:26:28 async_llm_engine.py:173] Added request cmpl-50c7804468fa40a2acf7de41250ce320-0.
INFO 02-05 12:26:28 logger.py:36] Received request cmpl-376a6bb590d34fa9825ab63792672ae6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:28 async_llm_engine.py:173] Added request cmpl-376a6bb590d34fa9825ab63792672ae6-0.
INFO 02-05 12:26:28 logger.py:36] Received request cmpl-b3e6f3fe14c34f969751e42efda619e3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:28 async_llm_engine.py:173] Added request cmpl-b3e6f3fe14c34f969751e42efda619e3-0.
INFO 02-05 12:26:28 logger.py:36] Received request cmpl-d5ce62328c524b71984d0797b3b69156-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:28 async_llm_engine.py:173] Added request cmpl-d5ce62328c524b71984d0797b3b69156-0.
INFO 02-05 12:26:28 logger.py:36] Received request cmpl-64ca519bd6f742188e6f5fa49f131406-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:28 async_llm_engine.py:173] Added request cmpl-64ca519bd6f742188e6f5fa49f131406-0.
INFO 02-05 12:26:28 logger.py:36] Received request cmpl-e2f166c45d024762a3237da860af4204-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:28 async_llm_engine.py:173] Added request cmpl-e2f166c45d024762a3237da860af4204-0.
INFO 02-05 12:26:28 metrics.py:396] Avg prompt throughput: 66.5 tokens/s, Avg generation throughput: 509.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:26:28 logger.py:36] Received request cmpl-e2fe1dff157e4cba98788fbf01b84e2c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:28 async_llm_engine.py:173] Added request cmpl-e2fe1dff157e4cba98788fbf01b84e2c-0.
INFO 02-05 12:26:28 logger.py:36] Received request cmpl-9f3ec140dd3246b4815b9f0d89fe0046-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:28 async_llm_engine.py:173] Added request cmpl-9f3ec140dd3246b4815b9f0d89fe0046-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:29 async_llm_engine.py:140] Finished request cmpl-ed23be58a5e04149b4e3e8ff93057f3b-0.
INFO 02-05 12:26:29 async_llm_engine.py:140] Finished request cmpl-3c5a37db092e47869d68142b2b19e49e-0.
INFO 02-05 12:26:29 async_llm_engine.py:140] Finished request cmpl-50c7804468fa40a2acf7de41250ce320-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:29 async_llm_engine.py:140] Finished request cmpl-376a6bb590d34fa9825ab63792672ae6-0.
INFO 02-05 12:26:29 async_llm_engine.py:140] Finished request cmpl-b3e6f3fe14c34f969751e42efda619e3-0.
INFO 02-05 12:26:29 async_llm_engine.py:140] Finished request cmpl-d5ce62328c524b71984d0797b3b69156-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:29 async_llm_engine.py:140] Finished request cmpl-64ca519bd6f742188e6f5fa49f131406-0.
INFO 02-05 12:26:29 async_llm_engine.py:140] Finished request cmpl-e2f166c45d024762a3237da860af4204-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:29 async_llm_engine.py:140] Finished request cmpl-e2fe1dff157e4cba98788fbf01b84e2c-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:29 async_llm_engine.py:140] Finished request cmpl-9f3ec140dd3246b4815b9f0d89fe0046-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:29 logger.py:36] Received request cmpl-e8664ed5cfe749b3ada383653833a503-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:29 async_llm_engine.py:173] Added request cmpl-e8664ed5cfe749b3ada383653833a503-0.
INFO 02-05 12:26:29 logger.py:36] Received request cmpl-92ca2a383a934053aa74136fab1e5cca-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:29 async_llm_engine.py:173] Added request cmpl-92ca2a383a934053aa74136fab1e5cca-0.
INFO 02-05 12:26:30 logger.py:36] Received request cmpl-93214cd9c3244af29a8d2c5c8fef2942-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:30 async_llm_engine.py:173] Added request cmpl-93214cd9c3244af29a8d2c5c8fef2942-0.
INFO 02-05 12:26:30 logger.py:36] Received request cmpl-0c6588cb911c465aa10f1fe06afbf25d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:30 async_llm_engine.py:173] Added request cmpl-0c6588cb911c465aa10f1fe06afbf25d-0.
INFO 02-05 12:26:30 logger.py:36] Received request cmpl-d76646e861ed45e5baeaddc4420d51d9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:30 async_llm_engine.py:173] Added request cmpl-d76646e861ed45e5baeaddc4420d51d9-0.
INFO 02-05 12:26:30 logger.py:36] Received request cmpl-9ceb183a39fb4c4a8c0dffafe7838f17-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:30 async_llm_engine.py:173] Added request cmpl-9ceb183a39fb4c4a8c0dffafe7838f17-0.
INFO 02-05 12:26:30 logger.py:36] Received request cmpl-ba9cb02f470e4f97baa8bbc8a436d916-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:30 async_llm_engine.py:173] Added request cmpl-ba9cb02f470e4f97baa8bbc8a436d916-0.
INFO 02-05 12:26:30 logger.py:36] Received request cmpl-4966fd947c56402383a7cbca6df71876-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:30 async_llm_engine.py:173] Added request cmpl-4966fd947c56402383a7cbca6df71876-0.
INFO 02-05 12:26:30 logger.py:36] Received request cmpl-6e18999acf84484192d3da750720dd63-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:30 async_llm_engine.py:173] Added request cmpl-6e18999acf84484192d3da750720dd63-0.
INFO 02-05 12:26:30 logger.py:36] Received request cmpl-72351e217b3a44898490465e1bbf3878-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:30 async_llm_engine.py:173] Added request cmpl-72351e217b3a44898490465e1bbf3878-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:31 async_llm_engine.py:140] Finished request cmpl-e8664ed5cfe749b3ada383653833a503-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:31 async_llm_engine.py:140] Finished request cmpl-92ca2a383a934053aa74136fab1e5cca-0.
INFO 02-05 12:26:31 async_llm_engine.py:140] Finished request cmpl-93214cd9c3244af29a8d2c5c8fef2942-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:31 async_llm_engine.py:140] Finished request cmpl-0c6588cb911c465aa10f1fe06afbf25d-0.
INFO 02-05 12:26:31 async_llm_engine.py:140] Finished request cmpl-d76646e861ed45e5baeaddc4420d51d9-0.
INFO 02-05 12:26:31 async_llm_engine.py:140] Finished request cmpl-9ceb183a39fb4c4a8c0dffafe7838f17-0.
INFO 02-05 12:26:31 async_llm_engine.py:140] Finished request cmpl-ba9cb02f470e4f97baa8bbc8a436d916-0.
INFO 02-05 12:26:31 async_llm_engine.py:140] Finished request cmpl-4966fd947c56402383a7cbca6df71876-0.
INFO 02-05 12:26:31 async_llm_engine.py:140] Finished request cmpl-6e18999acf84484192d3da750720dd63-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:31 async_llm_engine.py:140] Finished request cmpl-72351e217b3a44898490465e1bbf3878-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:31 logger.py:36] Received request cmpl-6e01c357fa5c435fb71f45310ce0811d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:31 async_llm_engine.py:173] Added request cmpl-6e01c357fa5c435fb71f45310ce0811d-0.
INFO 02-05 12:26:31 logger.py:36] Received request cmpl-b76b58122c60491da72af68a1efe1711-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:31 async_llm_engine.py:173] Added request cmpl-b76b58122c60491da72af68a1efe1711-0.
INFO 02-05 12:26:31 logger.py:36] Received request cmpl-8144f6accb854edc9330fff7d8e623a9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:31 async_llm_engine.py:173] Added request cmpl-8144f6accb854edc9330fff7d8e623a9-0.
INFO 02-05 12:26:31 logger.py:36] Received request cmpl-ac8c90e2b73f4f5ba178999c5df43875-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:31 async_llm_engine.py:173] Added request cmpl-ac8c90e2b73f4f5ba178999c5df43875-0.
INFO 02-05 12:26:31 logger.py:36] Received request cmpl-f23e4e794877476daecd82eded8d6f1f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:31 async_llm_engine.py:173] Added request cmpl-f23e4e794877476daecd82eded8d6f1f-0.
INFO 02-05 12:26:31 logger.py:36] Received request cmpl-40ed18c5ddd045b599e3fa875ea38101-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:31 async_llm_engine.py:173] Added request cmpl-40ed18c5ddd045b599e3fa875ea38101-0.
INFO 02-05 12:26:31 logger.py:36] Received request cmpl-2534bc419d7e4637940825fa8db2facb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:31 async_llm_engine.py:173] Added request cmpl-2534bc419d7e4637940825fa8db2facb-0.
INFO 02-05 12:26:31 logger.py:36] Received request cmpl-e41015b51f3d42d8916e85610fcd8e7f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:31 logger.py:36] Received request cmpl-f7e969293dc84cf4811a4be1017c58aa-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:31 async_llm_engine.py:173] Added request cmpl-e41015b51f3d42d8916e85610fcd8e7f-0.
INFO 02-05 12:26:31 async_llm_engine.py:173] Added request cmpl-f7e969293dc84cf4811a4be1017c58aa-0.
INFO 02-05 12:26:31 logger.py:36] Received request cmpl-6863f0b846a54b438e3b075b1844577c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:31 async_llm_engine.py:173] Added request cmpl-6863f0b846a54b438e3b075b1844577c-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:32 async_llm_engine.py:140] Finished request cmpl-6e01c357fa5c435fb71f45310ce0811d-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:32 async_llm_engine.py:140] Finished request cmpl-b76b58122c60491da72af68a1efe1711-0.
INFO 02-05 12:26:32 async_llm_engine.py:140] Finished request cmpl-8144f6accb854edc9330fff7d8e623a9-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:32 async_llm_engine.py:140] Finished request cmpl-ac8c90e2b73f4f5ba178999c5df43875-0.
INFO 02-05 12:26:32 async_llm_engine.py:140] Finished request cmpl-f23e4e794877476daecd82eded8d6f1f-0.
INFO 02-05 12:26:32 async_llm_engine.py:140] Finished request cmpl-40ed18c5ddd045b599e3fa875ea38101-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:32 async_llm_engine.py:140] Finished request cmpl-2534bc419d7e4637940825fa8db2facb-0.
INFO 02-05 12:26:32 async_llm_engine.py:140] Finished request cmpl-e41015b51f3d42d8916e85610fcd8e7f-0.
INFO 02-05 12:26:32 async_llm_engine.py:140] Finished request cmpl-f7e969293dc84cf4811a4be1017c58aa-0.
INFO 02-05 12:26:32 async_llm_engine.py:140] Finished request cmpl-6863f0b846a54b438e3b075b1844577c-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:32 logger.py:36] Received request cmpl-54922309527c47d4b28305f1d2be127a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:32 async_llm_engine.py:173] Added request cmpl-54922309527c47d4b28305f1d2be127a-0.
INFO 02-05 12:26:32 logger.py:36] Received request cmpl-772687b5bc1743668d1f97a53928453c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:32 async_llm_engine.py:173] Added request cmpl-772687b5bc1743668d1f97a53928453c-0.
INFO 02-05 12:26:32 logger.py:36] Received request cmpl-f4883f2d7c034e21b144d9f8d8da820d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:32 async_llm_engine.py:173] Added request cmpl-f4883f2d7c034e21b144d9f8d8da820d-0.
INFO 02-05 12:26:32 logger.py:36] Received request cmpl-6a888afa37c4496c87f1d9bf6b4f259d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:32 async_llm_engine.py:173] Added request cmpl-6a888afa37c4496c87f1d9bf6b4f259d-0.
INFO 02-05 12:26:32 logger.py:36] Received request cmpl-1a0ee7144e2548cab3ffd6b561edf6b1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:32 async_llm_engine.py:173] Added request cmpl-1a0ee7144e2548cab3ffd6b561edf6b1-0.
INFO 02-05 12:26:32 logger.py:36] Received request cmpl-bf6ed35950de4491b871a430c0bd2334-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:32 async_llm_engine.py:173] Added request cmpl-bf6ed35950de4491b871a430c0bd2334-0.
INFO 02-05 12:26:32 logger.py:36] Received request cmpl-30ffef73357d41e29a54ee78952c4fc6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:32 async_llm_engine.py:173] Added request cmpl-30ffef73357d41e29a54ee78952c4fc6-0.
INFO 02-05 12:26:32 logger.py:36] Received request cmpl-3fccff10797b428e87d6af58b71bc8bb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:32 async_llm_engine.py:173] Added request cmpl-3fccff10797b428e87d6af58b71bc8bb-0.
INFO 02-05 12:26:32 logger.py:36] Received request cmpl-79fe0337cb2a48e9b96fb8d0218e4ac0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:32 async_llm_engine.py:173] Added request cmpl-79fe0337cb2a48e9b96fb8d0218e4ac0-0.
INFO 02-05 12:26:32 logger.py:36] Received request cmpl-6fba2849029943b88781989efe34385b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:32 async_llm_engine.py:173] Added request cmpl-6fba2849029943b88781989efe34385b-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:33 async_llm_engine.py:140] Finished request cmpl-54922309527c47d4b28305f1d2be127a-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:33 async_llm_engine.py:140] Finished request cmpl-772687b5bc1743668d1f97a53928453c-0.
INFO 02-05 12:26:33 async_llm_engine.py:140] Finished request cmpl-f4883f2d7c034e21b144d9f8d8da820d-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:33 async_llm_engine.py:140] Finished request cmpl-6a888afa37c4496c87f1d9bf6b4f259d-0.
INFO 02-05 12:26:33 async_llm_engine.py:140] Finished request cmpl-1a0ee7144e2548cab3ffd6b561edf6b1-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:33 async_llm_engine.py:140] Finished request cmpl-bf6ed35950de4491b871a430c0bd2334-0.
INFO 02-05 12:26:33 async_llm_engine.py:140] Finished request cmpl-30ffef73357d41e29a54ee78952c4fc6-0.
INFO 02-05 12:26:33 async_llm_engine.py:140] Finished request cmpl-3fccff10797b428e87d6af58b71bc8bb-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:33 async_llm_engine.py:140] Finished request cmpl-79fe0337cb2a48e9b96fb8d0218e4ac0-0.
INFO 02-05 12:26:33 async_llm_engine.py:140] Finished request cmpl-6fba2849029943b88781989efe34385b-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:33 logger.py:36] Received request cmpl-cb39b2064a264e0b9db08d4c99938de4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:33 async_llm_engine.py:173] Added request cmpl-cb39b2064a264e0b9db08d4c99938de4-0.
INFO 02-05 12:26:33 logger.py:36] Received request cmpl-bb5353b9b2b34e099ecb6a5392346596-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:33 async_llm_engine.py:173] Added request cmpl-bb5353b9b2b34e099ecb6a5392346596-0.
INFO 02-05 12:26:33 logger.py:36] Received request cmpl-fe3c12d16be54986871e3ba725dec04d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:33 async_llm_engine.py:173] Added request cmpl-fe3c12d16be54986871e3ba725dec04d-0.
INFO 02-05 12:26:33 logger.py:36] Received request cmpl-28fae272f958439e8a558fd45fb54b9d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:33 logger.py:36] Received request cmpl-2859ec814d8e41a99697d5a2c5f8ebf3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:33 async_llm_engine.py:173] Added request cmpl-28fae272f958439e8a558fd45fb54b9d-0.
INFO 02-05 12:26:33 async_llm_engine.py:173] Added request cmpl-2859ec814d8e41a99697d5a2c5f8ebf3-0.
INFO 02-05 12:26:33 logger.py:36] Received request cmpl-e522e7d43bc54119961508c25f0c39ef-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:33 async_llm_engine.py:173] Added request cmpl-e522e7d43bc54119961508c25f0c39ef-0.
INFO 02-05 12:26:33 logger.py:36] Received request cmpl-0ffb3c28a4b14a97a62cf478802f0fe1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:33 async_llm_engine.py:173] Added request cmpl-0ffb3c28a4b14a97a62cf478802f0fe1-0.
INFO 02-05 12:26:33 logger.py:36] Received request cmpl-f35149239906426cb0eeb8e34e9ffaa5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:33 async_llm_engine.py:173] Added request cmpl-f35149239906426cb0eeb8e34e9ffaa5-0.
INFO 02-05 12:26:33 logger.py:36] Received request cmpl-c4e4c8ea3aff4106bc37fc7f87eae967-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:33 logger.py:36] Received request cmpl-c7daa196f2114a07a21709a9159ecd8e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:33 async_llm_engine.py:173] Added request cmpl-c4e4c8ea3aff4106bc37fc7f87eae967-0.
INFO 02-05 12:26:33 async_llm_engine.py:173] Added request cmpl-c7daa196f2114a07a21709a9159ecd8e-0.
INFO 02-05 12:26:33 metrics.py:396] Avg prompt throughput: 79.1 tokens/s, Avg generation throughput: 518.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:34 async_llm_engine.py:140] Finished request cmpl-cb39b2064a264e0b9db08d4c99938de4-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:34 async_llm_engine.py:140] Finished request cmpl-bb5353b9b2b34e099ecb6a5392346596-0.
INFO 02-05 12:26:34 async_llm_engine.py:140] Finished request cmpl-fe3c12d16be54986871e3ba725dec04d-0.
INFO 02-05 12:26:34 async_llm_engine.py:140] Finished request cmpl-28fae272f958439e8a558fd45fb54b9d-0.
INFO 02-05 12:26:34 async_llm_engine.py:140] Finished request cmpl-2859ec814d8e41a99697d5a2c5f8ebf3-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:34 async_llm_engine.py:140] Finished request cmpl-e522e7d43bc54119961508c25f0c39ef-0.
INFO 02-05 12:26:34 async_llm_engine.py:140] Finished request cmpl-0ffb3c28a4b14a97a62cf478802f0fe1-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:34 logger.py:36] Received request cmpl-8812ef4ffd964000b9fdf438ad4e9b05-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:34 async_llm_engine.py:173] Added request cmpl-8812ef4ffd964000b9fdf438ad4e9b05-0.
INFO 02-05 12:26:34 async_llm_engine.py:140] Finished request cmpl-f35149239906426cb0eeb8e34e9ffaa5-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:34 async_llm_engine.py:140] Finished request cmpl-c4e4c8ea3aff4106bc37fc7f87eae967-0.
INFO 02-05 12:26:34 async_llm_engine.py:140] Finished request cmpl-c7daa196f2114a07a21709a9159ecd8e-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:34 logger.py:36] Received request cmpl-c56923ec07634bd3ae287b5ef0d3121b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:34 async_llm_engine.py:173] Added request cmpl-c56923ec07634bd3ae287b5ef0d3121b-0.
INFO 02-05 12:26:34 logger.py:36] Received request cmpl-9450b2987e1f4b50bee8786b2d2caac3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:34 async_llm_engine.py:173] Added request cmpl-9450b2987e1f4b50bee8786b2d2caac3-0.
INFO 02-05 12:26:34 logger.py:36] Received request cmpl-e9a46987c51446c099e6cbcab6d0674d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:34 logger.py:36] Received request cmpl-3d1f43d9057a4cc796357dd99fc8a6b5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:34 async_llm_engine.py:173] Added request cmpl-e9a46987c51446c099e6cbcab6d0674d-0.
INFO 02-05 12:26:34 async_llm_engine.py:173] Added request cmpl-3d1f43d9057a4cc796357dd99fc8a6b5-0.
INFO 02-05 12:26:34 logger.py:36] Received request cmpl-e2d6e7e0dc3d4026b106e2e748c758bc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:34 async_llm_engine.py:173] Added request cmpl-e2d6e7e0dc3d4026b106e2e748c758bc-0.
INFO 02-05 12:26:34 logger.py:36] Received request cmpl-78133ad00b7c406e8d3e97f9dad3ccfd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:34 async_llm_engine.py:173] Added request cmpl-78133ad00b7c406e8d3e97f9dad3ccfd-0.
INFO 02-05 12:26:35 logger.py:36] Received request cmpl-fc78310c96d048bbb5df10f71b3067f7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:35 async_llm_engine.py:173] Added request cmpl-fc78310c96d048bbb5df10f71b3067f7-0.
INFO 02-05 12:26:35 logger.py:36] Received request cmpl-723ce4ce9c914fe68d7885c162ee39d8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:35 async_llm_engine.py:173] Added request cmpl-723ce4ce9c914fe68d7885c162ee39d8-0.
INFO 02-05 12:26:35 logger.py:36] Received request cmpl-e43fb07b67f7451cbfa82231dcd1debb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:35 async_llm_engine.py:173] Added request cmpl-e43fb07b67f7451cbfa82231dcd1debb-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:35 async_llm_engine.py:140] Finished request cmpl-8812ef4ffd964000b9fdf438ad4e9b05-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     89.105.200.105:51898 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:51906 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:26:36 async_llm_engine.py:140] Finished request cmpl-c56923ec07634bd3ae287b5ef0d3121b-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:36 async_llm_engine.py:140] Finished request cmpl-9450b2987e1f4b50bee8786b2d2caac3-0.
INFO 02-05 12:26:36 async_llm_engine.py:140] Finished request cmpl-e9a46987c51446c099e6cbcab6d0674d-0.
INFO 02-05 12:26:36 async_llm_engine.py:140] Finished request cmpl-3d1f43d9057a4cc796357dd99fc8a6b5-0.
INFO 02-05 12:26:36 async_llm_engine.py:140] Finished request cmpl-e2d6e7e0dc3d4026b106e2e748c758bc-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:36 logger.py:36] Received request cmpl-aea5cfb3eacd48e892ea991d176f0c74-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:36 async_llm_engine.py:140] Finished request cmpl-78133ad00b7c406e8d3e97f9dad3ccfd-0.
INFO 02-05 12:26:36 async_llm_engine.py:173] Added request cmpl-aea5cfb3eacd48e892ea991d176f0c74-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:36 async_llm_engine.py:140] Finished request cmpl-fc78310c96d048bbb5df10f71b3067f7-0.
INFO 02-05 12:26:36 async_llm_engine.py:140] Finished request cmpl-723ce4ce9c914fe68d7885c162ee39d8-0.
INFO 02-05 12:26:36 async_llm_engine.py:140] Finished request cmpl-e43fb07b67f7451cbfa82231dcd1debb-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:36 logger.py:36] Received request cmpl-456236c069d24972a93dd0487da7126a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:36 async_llm_engine.py:173] Added request cmpl-456236c069d24972a93dd0487da7126a-0.
INFO 02-05 12:26:36 logger.py:36] Received request cmpl-65522062dfc849019683d11e0a49f361-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:36 async_llm_engine.py:173] Added request cmpl-65522062dfc849019683d11e0a49f361-0.
INFO 02-05 12:26:36 logger.py:36] Received request cmpl-ff229e6032eb4b6a823e7ee10845fa91-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:36 logger.py:36] Received request cmpl-5e03c719b39b449c98497106a789d41a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:36 async_llm_engine.py:173] Added request cmpl-ff229e6032eb4b6a823e7ee10845fa91-0.
INFO 02-05 12:26:36 async_llm_engine.py:173] Added request cmpl-5e03c719b39b449c98497106a789d41a-0.
INFO 02-05 12:26:36 logger.py:36] Received request cmpl-fbea3954c60f4475b2c64157d8af9d75-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:36 async_llm_engine.py:173] Added request cmpl-fbea3954c60f4475b2c64157d8af9d75-0.
INFO 02-05 12:26:36 logger.py:36] Received request cmpl-961d4e242b51451b82761719cd384e82-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:36 async_llm_engine.py:173] Added request cmpl-961d4e242b51451b82761719cd384e82-0.
INFO 02-05 12:26:36 logger.py:36] Received request cmpl-07aa6e7b285b4c5fa8dc5f0df7aa7e74-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:36 async_llm_engine.py:173] Added request cmpl-07aa6e7b285b4c5fa8dc5f0df7aa7e74-0.
INFO 02-05 12:26:36 logger.py:36] Received request cmpl-faeae8e767cd467cbc7b4a2960db68de-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:36 async_llm_engine.py:173] Added request cmpl-faeae8e767cd467cbc7b4a2960db68de-0.
INFO 02-05 12:26:36 logger.py:36] Received request cmpl-89770515f45c4374b3d12806b509682d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:36 async_llm_engine.py:173] Added request cmpl-89770515f45c4374b3d12806b509682d-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:37 async_llm_engine.py:140] Finished request cmpl-aea5cfb3eacd48e892ea991d176f0c74-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:37 async_llm_engine.py:140] Finished request cmpl-456236c069d24972a93dd0487da7126a-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:37 async_llm_engine.py:140] Finished request cmpl-65522062dfc849019683d11e0a49f361-0.
INFO 02-05 12:26:37 async_llm_engine.py:140] Finished request cmpl-ff229e6032eb4b6a823e7ee10845fa91-0.
INFO 02-05 12:26:37 async_llm_engine.py:140] Finished request cmpl-5e03c719b39b449c98497106a789d41a-0.
INFO 02-05 12:26:37 async_llm_engine.py:140] Finished request cmpl-fbea3954c60f4475b2c64157d8af9d75-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:37 async_llm_engine.py:140] Finished request cmpl-961d4e242b51451b82761719cd384e82-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:37 async_llm_engine.py:140] Finished request cmpl-07aa6e7b285b4c5fa8dc5f0df7aa7e74-0.
INFO 02-05 12:26:37 async_llm_engine.py:140] Finished request cmpl-faeae8e767cd467cbc7b4a2960db68de-0.
INFO 02-05 12:26:37 async_llm_engine.py:140] Finished request cmpl-89770515f45c4374b3d12806b509682d-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:37 logger.py:36] Received request cmpl-496d2184fa114244b8adbb4e75a84a8d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:37 async_llm_engine.py:173] Added request cmpl-496d2184fa114244b8adbb4e75a84a8d-0.
INFO 02-05 12:26:37 logger.py:36] Received request cmpl-8e0e88de6a2a4842af88fc304b7021b0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:37 async_llm_engine.py:173] Added request cmpl-8e0e88de6a2a4842af88fc304b7021b0-0.
INFO 02-05 12:26:37 logger.py:36] Received request cmpl-6e5cae008b714fb3a036585efbbaabcd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:37 logger.py:36] Received request cmpl-cc79c166cae24f93929ed4cf1856292d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:37 async_llm_engine.py:173] Added request cmpl-6e5cae008b714fb3a036585efbbaabcd-0.
INFO 02-05 12:26:37 async_llm_engine.py:173] Added request cmpl-cc79c166cae24f93929ed4cf1856292d-0.
INFO 02-05 12:26:37 logger.py:36] Received request cmpl-e04f4059ff1747d288310a3dc039468d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:37 async_llm_engine.py:173] Added request cmpl-e04f4059ff1747d288310a3dc039468d-0.
INFO 02-05 12:26:37 logger.py:36] Received request cmpl-6559273db111448d89d1ef0e67c11724-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:37 async_llm_engine.py:173] Added request cmpl-6559273db111448d89d1ef0e67c11724-0.
INFO 02-05 12:26:37 logger.py:36] Received request cmpl-e2bbfb1c327d4e70b3cf52821d8c2f43-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:37 async_llm_engine.py:173] Added request cmpl-e2bbfb1c327d4e70b3cf52821d8c2f43-0.
INFO 02-05 12:26:37 logger.py:36] Received request cmpl-e7959c7af53e4e8f99b3a40cb394aa1a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:37 async_llm_engine.py:173] Added request cmpl-e7959c7af53e4e8f99b3a40cb394aa1a-0.
INFO 02-05 12:26:37 logger.py:36] Received request cmpl-80c474e26a624474afaf19e6a5c524e9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:37 async_llm_engine.py:173] Added request cmpl-80c474e26a624474afaf19e6a5c524e9-0.
INFO 02-05 12:26:37 logger.py:36] Received request cmpl-4254d56a87254afab3428b295cd37ff1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:37 async_llm_engine.py:173] Added request cmpl-4254d56a87254afab3428b295cd37ff1-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:38 async_llm_engine.py:140] Finished request cmpl-496d2184fa114244b8adbb4e75a84a8d-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:38 async_llm_engine.py:140] Finished request cmpl-8e0e88de6a2a4842af88fc304b7021b0-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:38 async_llm_engine.py:140] Finished request cmpl-6e5cae008b714fb3a036585efbbaabcd-0.
INFO 02-05 12:26:38 async_llm_engine.py:140] Finished request cmpl-cc79c166cae24f93929ed4cf1856292d-0.
INFO 02-05 12:26:38 async_llm_engine.py:140] Finished request cmpl-e04f4059ff1747d288310a3dc039468d-0.
INFO 02-05 12:26:38 async_llm_engine.py:140] Finished request cmpl-6559273db111448d89d1ef0e67c11724-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:38 async_llm_engine.py:140] Finished request cmpl-e2bbfb1c327d4e70b3cf52821d8c2f43-0.
INFO 02-05 12:26:38 async_llm_engine.py:140] Finished request cmpl-e7959c7af53e4e8f99b3a40cb394aa1a-0.
INFO 02-05 12:26:38 async_llm_engine.py:140] Finished request cmpl-80c474e26a624474afaf19e6a5c524e9-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:38 async_llm_engine.py:140] Finished request cmpl-4254d56a87254afab3428b295cd37ff1-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:38 logger.py:36] Received request cmpl-c56f0f81c181430cb7ef2b2807f7ae65-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:38 async_llm_engine.py:173] Added request cmpl-c56f0f81c181430cb7ef2b2807f7ae65-0.
INFO 02-05 12:26:38 logger.py:36] Received request cmpl-c4f2f0619fbf4dcb9237396df2bd9b0f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:38 async_llm_engine.py:173] Added request cmpl-c4f2f0619fbf4dcb9237396df2bd9b0f-0.
INFO 02-05 12:26:38 logger.py:36] Received request cmpl-94c398fcf7744ac8a0693aedb92545c4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:38 async_llm_engine.py:173] Added request cmpl-94c398fcf7744ac8a0693aedb92545c4-0.
INFO 02-05 12:26:38 logger.py:36] Received request cmpl-2d58faf6d97641299b4a295813e30264-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:38 async_llm_engine.py:173] Added request cmpl-2d58faf6d97641299b4a295813e30264-0.
INFO 02-05 12:26:38 logger.py:36] Received request cmpl-d888500bcb1748da80493ffe37496b0e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:38 async_llm_engine.py:173] Added request cmpl-d888500bcb1748da80493ffe37496b0e-0.
INFO 02-05 12:26:38 logger.py:36] Received request cmpl-13aa4d5d043b474bad8315eac482162c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:38 async_llm_engine.py:173] Added request cmpl-13aa4d5d043b474bad8315eac482162c-0.
INFO 02-05 12:26:38 logger.py:36] Received request cmpl-906bce16d60046e8b76a4e98016bb6c9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:38 async_llm_engine.py:173] Added request cmpl-906bce16d60046e8b76a4e98016bb6c9-0.
INFO 02-05 12:26:38 logger.py:36] Received request cmpl-eb73b939bf70445494cf1371af48ee68-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:38 async_llm_engine.py:173] Added request cmpl-eb73b939bf70445494cf1371af48ee68-0.
INFO 02-05 12:26:38 logger.py:36] Received request cmpl-0332ccd1c49c481a80b5d874b3bc0965-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:38 async_llm_engine.py:173] Added request cmpl-0332ccd1c49c481a80b5d874b3bc0965-0.
INFO 02-05 12:26:38 logger.py:36] Received request cmpl-db9564d3f33443ffa4f313aca852215f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:38 async_llm_engine.py:173] Added request cmpl-db9564d3f33443ffa4f313aca852215f-0.
INFO 02-05 12:26:38 metrics.py:396] Avg prompt throughput: 72.0 tokens/s, Avg generation throughput: 519.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:39 async_llm_engine.py:140] Finished request cmpl-c56f0f81c181430cb7ef2b2807f7ae65-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:39 async_llm_engine.py:140] Finished request cmpl-c4f2f0619fbf4dcb9237396df2bd9b0f-0.
INFO 02-05 12:26:39 async_llm_engine.py:140] Finished request cmpl-94c398fcf7744ac8a0693aedb92545c4-0.
INFO 02-05 12:26:39 async_llm_engine.py:140] Finished request cmpl-2d58faf6d97641299b4a295813e30264-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:39 async_llm_engine.py:140] Finished request cmpl-d888500bcb1748da80493ffe37496b0e-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:39 async_llm_engine.py:140] Finished request cmpl-13aa4d5d043b474bad8315eac482162c-0.
INFO 02-05 12:26:39 async_llm_engine.py:140] Finished request cmpl-906bce16d60046e8b76a4e98016bb6c9-0.
INFO 02-05 12:26:39 async_llm_engine.py:140] Finished request cmpl-eb73b939bf70445494cf1371af48ee68-0.
INFO 02-05 12:26:39 async_llm_engine.py:140] Finished request cmpl-0332ccd1c49c481a80b5d874b3bc0965-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:39 async_llm_engine.py:140] Finished request cmpl-db9564d3f33443ffa4f313aca852215f-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:39 logger.py:36] Received request cmpl-824d23db31c74a64b4839589a4fe02c5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:39 async_llm_engine.py:173] Added request cmpl-824d23db31c74a64b4839589a4fe02c5-0.
INFO 02-05 12:26:39 logger.py:36] Received request cmpl-08ceb7fb4aa24c23a6a8d44294e17ee3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:39 async_llm_engine.py:173] Added request cmpl-08ceb7fb4aa24c23a6a8d44294e17ee3-0.
INFO 02-05 12:26:39 logger.py:36] Received request cmpl-8707e5796b0347f0a452feeb4a69a1be-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:39 async_llm_engine.py:173] Added request cmpl-8707e5796b0347f0a452feeb4a69a1be-0.
INFO 02-05 12:26:39 logger.py:36] Received request cmpl-592b1396c30640ba8cde8dc4ca35700b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:39 async_llm_engine.py:173] Added request cmpl-592b1396c30640ba8cde8dc4ca35700b-0.
INFO 02-05 12:26:39 logger.py:36] Received request cmpl-ab67cd080cd84f4480ae9f105191f167-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:39 async_llm_engine.py:173] Added request cmpl-ab67cd080cd84f4480ae9f105191f167-0.
INFO 02-05 12:26:39 logger.py:36] Received request cmpl-9f1acbfc42c34c80ac67afe595985c79-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:39 async_llm_engine.py:173] Added request cmpl-9f1acbfc42c34c80ac67afe595985c79-0.
INFO 02-05 12:26:39 logger.py:36] Received request cmpl-8a401d7e7e32490ebc6c7b286c926358-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:39 async_llm_engine.py:173] Added request cmpl-8a401d7e7e32490ebc6c7b286c926358-0.
INFO 02-05 12:26:39 logger.py:36] Received request cmpl-cf978cb90bc34501b3a49c3c1aadeca3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:39 async_llm_engine.py:173] Added request cmpl-cf978cb90bc34501b3a49c3c1aadeca3-0.
INFO 02-05 12:26:39 logger.py:36] Received request cmpl-b73a9e9c5aeb4a5e93eb8e95ad5628ca-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:39 async_llm_engine.py:173] Added request cmpl-b73a9e9c5aeb4a5e93eb8e95ad5628ca-0.
INFO 02-05 12:26:39 logger.py:36] Received request cmpl-909b62f084194dbf9f926f53b3edf5bd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:39 async_llm_engine.py:173] Added request cmpl-909b62f084194dbf9f926f53b3edf5bd-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:40 async_llm_engine.py:140] Finished request cmpl-824d23db31c74a64b4839589a4fe02c5-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:40 async_llm_engine.py:140] Finished request cmpl-08ceb7fb4aa24c23a6a8d44294e17ee3-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:40 async_llm_engine.py:140] Finished request cmpl-8707e5796b0347f0a452feeb4a69a1be-0.
INFO 02-05 12:26:40 async_llm_engine.py:140] Finished request cmpl-592b1396c30640ba8cde8dc4ca35700b-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:41 async_llm_engine.py:140] Finished request cmpl-ab67cd080cd84f4480ae9f105191f167-0.
INFO 02-05 12:26:41 async_llm_engine.py:140] Finished request cmpl-9f1acbfc42c34c80ac67afe595985c79-0.
INFO 02-05 12:26:41 async_llm_engine.py:140] Finished request cmpl-8a401d7e7e32490ebc6c7b286c926358-0.
INFO 02-05 12:26:41 async_llm_engine.py:140] Finished request cmpl-cf978cb90bc34501b3a49c3c1aadeca3-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:41 async_llm_engine.py:140] Finished request cmpl-b73a9e9c5aeb4a5e93eb8e95ad5628ca-0.
INFO 02-05 12:26:41 async_llm_engine.py:140] Finished request cmpl-909b62f084194dbf9f926f53b3edf5bd-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:41 logger.py:36] Received request cmpl-9e223adcc8e8412cad37a9843e3830bd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:41 async_llm_engine.py:173] Added request cmpl-9e223adcc8e8412cad37a9843e3830bd-0.
INFO 02-05 12:26:41 logger.py:36] Received request cmpl-e38833a036e6453b92fbfbcf02806f7f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:41 async_llm_engine.py:173] Added request cmpl-e38833a036e6453b92fbfbcf02806f7f-0.
INFO 02-05 12:26:41 logger.py:36] Received request cmpl-34fd8015b4a544ce8895bd5797d12147-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:41 async_llm_engine.py:173] Added request cmpl-34fd8015b4a544ce8895bd5797d12147-0.
INFO 02-05 12:26:41 logger.py:36] Received request cmpl-47a8dd876f42423391536b5c46b940da-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:41 async_llm_engine.py:173] Added request cmpl-47a8dd876f42423391536b5c46b940da-0.
INFO 02-05 12:26:41 logger.py:36] Received request cmpl-6736815fc362418284f5486e1fce4ce1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:41 logger.py:36] Received request cmpl-1447137055cd4ea3980c73fd34aafef3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:41 async_llm_engine.py:173] Added request cmpl-6736815fc362418284f5486e1fce4ce1-0.
INFO 02-05 12:26:41 async_llm_engine.py:173] Added request cmpl-1447137055cd4ea3980c73fd34aafef3-0.
INFO 02-05 12:26:41 logger.py:36] Received request cmpl-418f4e5f151346b192bd0f7eccce8107-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:41 logger.py:36] Received request cmpl-4c819ae8e4ca452997cac13447511536-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:41 async_llm_engine.py:173] Added request cmpl-418f4e5f151346b192bd0f7eccce8107-0.
INFO 02-05 12:26:41 async_llm_engine.py:173] Added request cmpl-4c819ae8e4ca452997cac13447511536-0.
INFO 02-05 12:26:41 logger.py:36] Received request cmpl-b98134550fb24566ac00cca18315a9d9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:41 async_llm_engine.py:173] Added request cmpl-b98134550fb24566ac00cca18315a9d9-0.
INFO 02-05 12:26:41 logger.py:36] Received request cmpl-fe573f7d057246f3970c3b5254695da8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:41 async_llm_engine.py:173] Added request cmpl-fe573f7d057246f3970c3b5254695da8-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:42 async_llm_engine.py:140] Finished request cmpl-9e223adcc8e8412cad37a9843e3830bd-0.
INFO 02-05 12:26:42 async_llm_engine.py:140] Finished request cmpl-e38833a036e6453b92fbfbcf02806f7f-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:42 async_llm_engine.py:140] Finished request cmpl-34fd8015b4a544ce8895bd5797d12147-0.
INFO 02-05 12:26:42 async_llm_engine.py:140] Finished request cmpl-47a8dd876f42423391536b5c46b940da-0.
INFO 02-05 12:26:42 async_llm_engine.py:140] Finished request cmpl-6736815fc362418284f5486e1fce4ce1-0.
INFO 02-05 12:26:42 async_llm_engine.py:140] Finished request cmpl-1447137055cd4ea3980c73fd34aafef3-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:42 async_llm_engine.py:140] Finished request cmpl-418f4e5f151346b192bd0f7eccce8107-0.
INFO 02-05 12:26:42 async_llm_engine.py:140] Finished request cmpl-4c819ae8e4ca452997cac13447511536-0.
INFO 02-05 12:26:42 async_llm_engine.py:140] Finished request cmpl-b98134550fb24566ac00cca18315a9d9-0.
INFO 02-05 12:26:42 async_llm_engine.py:140] Finished request cmpl-fe573f7d057246f3970c3b5254695da8-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:42 logger.py:36] Received request cmpl-36730cc13a874b8aba4e49ee1cae9fe7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:42 async_llm_engine.py:173] Added request cmpl-36730cc13a874b8aba4e49ee1cae9fe7-0.
INFO 02-05 12:26:42 logger.py:36] Received request cmpl-5333891b6b564ce8b1cadb9d2003a00a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:42 async_llm_engine.py:173] Added request cmpl-5333891b6b564ce8b1cadb9d2003a00a-0.
INFO 02-05 12:26:42 logger.py:36] Received request cmpl-6b97a3e0808a42d2a2ba1630e7cfe84e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:42 logger.py:36] Received request cmpl-f79f8500df1f45838db68c0af497e371-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:42 async_llm_engine.py:173] Added request cmpl-6b97a3e0808a42d2a2ba1630e7cfe84e-0.
INFO 02-05 12:26:42 async_llm_engine.py:173] Added request cmpl-f79f8500df1f45838db68c0af497e371-0.
INFO 02-05 12:26:42 logger.py:36] Received request cmpl-eb822176abc54822b9462286a476b542-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:42 async_llm_engine.py:173] Added request cmpl-eb822176abc54822b9462286a476b542-0.
INFO 02-05 12:26:42 logger.py:36] Received request cmpl-ed004514ae9b49b3b5bfe4a8c0609240-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:42 async_llm_engine.py:173] Added request cmpl-ed004514ae9b49b3b5bfe4a8c0609240-0.
INFO 02-05 12:26:42 logger.py:36] Received request cmpl-9bddc52485144f7b8b8d0ce92f70fcf1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:42 async_llm_engine.py:173] Added request cmpl-9bddc52485144f7b8b8d0ce92f70fcf1-0.
INFO 02-05 12:26:42 logger.py:36] Received request cmpl-0d75e01941644194a8744f2b816592e3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:42 async_llm_engine.py:173] Added request cmpl-0d75e01941644194a8744f2b816592e3-0.
INFO 02-05 12:26:42 logger.py:36] Received request cmpl-826e6e210573404da7641966098b0c82-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:42 async_llm_engine.py:173] Added request cmpl-826e6e210573404da7641966098b0c82-0.
INFO 02-05 12:26:42 logger.py:36] Received request cmpl-6ee509624576415092eed60eba3ac737-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:42 async_llm_engine.py:173] Added request cmpl-6ee509624576415092eed60eba3ac737-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:43 async_llm_engine.py:140] Finished request cmpl-36730cc13a874b8aba4e49ee1cae9fe7-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:43 metrics.py:396] Avg prompt throughput: 54.0 tokens/s, Avg generation throughput: 494.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 02-05 12:26:43 async_llm_engine.py:140] Finished request cmpl-5333891b6b564ce8b1cadb9d2003a00a-0.
INFO 02-05 12:26:43 async_llm_engine.py:140] Finished request cmpl-6b97a3e0808a42d2a2ba1630e7cfe84e-0.
INFO 02-05 12:26:43 async_llm_engine.py:140] Finished request cmpl-f79f8500df1f45838db68c0af497e371-0.
INFO 02-05 12:26:43 async_llm_engine.py:140] Finished request cmpl-eb822176abc54822b9462286a476b542-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:43 async_llm_engine.py:140] Finished request cmpl-ed004514ae9b49b3b5bfe4a8c0609240-0.
INFO 02-05 12:26:43 async_llm_engine.py:140] Finished request cmpl-9bddc52485144f7b8b8d0ce92f70fcf1-0.
INFO 02-05 12:26:43 async_llm_engine.py:140] Finished request cmpl-0d75e01941644194a8744f2b816592e3-0.
INFO 02-05 12:26:43 async_llm_engine.py:140] Finished request cmpl-826e6e210573404da7641966098b0c82-0.
INFO 02-05 12:26:43 async_llm_engine.py:140] Finished request cmpl-6ee509624576415092eed60eba3ac737-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:43 logger.py:36] Received request cmpl-56aaeb5045794ee28c8e3a637b6f32ad-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:43 async_llm_engine.py:173] Added request cmpl-56aaeb5045794ee28c8e3a637b6f32ad-0.
INFO 02-05 12:26:43 logger.py:36] Received request cmpl-fffed72232f84e67b68cba778f086f53-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:43 async_llm_engine.py:173] Added request cmpl-fffed72232f84e67b68cba778f086f53-0.
INFO 02-05 12:26:43 logger.py:36] Received request cmpl-23cfec6d958b40989f8e4d2cef028273-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:43 logger.py:36] Received request cmpl-d1ce4cfa1ad9425298f45100a2f799c8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:43 async_llm_engine.py:173] Added request cmpl-23cfec6d958b40989f8e4d2cef028273-0.
INFO 02-05 12:26:43 async_llm_engine.py:173] Added request cmpl-d1ce4cfa1ad9425298f45100a2f799c8-0.
INFO 02-05 12:26:43 logger.py:36] Received request cmpl-7210c769586742ca984eab18d3607567-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:43 async_llm_engine.py:173] Added request cmpl-7210c769586742ca984eab18d3607567-0.
INFO 02-05 12:26:43 logger.py:36] Received request cmpl-ec592cd2ac5d4ab49e2e7d4f42a3634d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:43 async_llm_engine.py:173] Added request cmpl-ec592cd2ac5d4ab49e2e7d4f42a3634d-0.
INFO 02-05 12:26:43 logger.py:36] Received request cmpl-34c3d79ab5694678a0ed32add9d3da35-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:43 async_llm_engine.py:173] Added request cmpl-34c3d79ab5694678a0ed32add9d3da35-0.
INFO 02-05 12:26:43 logger.py:36] Received request cmpl-f39ba4f0a04e4f6d845c797484d979ff-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:43 async_llm_engine.py:173] Added request cmpl-f39ba4f0a04e4f6d845c797484d979ff-0.
INFO 02-05 12:26:44 logger.py:36] Received request cmpl-9d781f6774ae4077b3187f2f5cdeebed-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:44 async_llm_engine.py:173] Added request cmpl-9d781f6774ae4077b3187f2f5cdeebed-0.
INFO 02-05 12:26:44 logger.py:36] Received request cmpl-9cd639fd2d264bc2b40d72f34e893310-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:44 async_llm_engine.py:173] Added request cmpl-9cd639fd2d264bc2b40d72f34e893310-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:45 async_llm_engine.py:140] Finished request cmpl-56aaeb5045794ee28c8e3a637b6f32ad-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:45 async_llm_engine.py:140] Finished request cmpl-fffed72232f84e67b68cba778f086f53-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:45 async_llm_engine.py:140] Finished request cmpl-23cfec6d958b40989f8e4d2cef028273-0.
INFO 02-05 12:26:45 async_llm_engine.py:140] Finished request cmpl-d1ce4cfa1ad9425298f45100a2f799c8-0.
INFO 02-05 12:26:45 async_llm_engine.py:140] Finished request cmpl-7210c769586742ca984eab18d3607567-0.
INFO 02-05 12:26:45 async_llm_engine.py:140] Finished request cmpl-ec592cd2ac5d4ab49e2e7d4f42a3634d-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:45 async_llm_engine.py:140] Finished request cmpl-34c3d79ab5694678a0ed32add9d3da35-0.
INFO 02-05 12:26:45 async_llm_engine.py:140] Finished request cmpl-f39ba4f0a04e4f6d845c797484d979ff-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:45 async_llm_engine.py:140] Finished request cmpl-9d781f6774ae4077b3187f2f5cdeebed-0.
INFO 02-05 12:26:45 async_llm_engine.py:140] Finished request cmpl-9cd639fd2d264bc2b40d72f34e893310-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:45 logger.py:36] Received request cmpl-83f0649ebb434ba493cef7c9e40ef418-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:45 async_llm_engine.py:173] Added request cmpl-83f0649ebb434ba493cef7c9e40ef418-0.
INFO 02-05 12:26:45 logger.py:36] Received request cmpl-9a91c7ef88524136b139d800acb428ef-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:45 async_llm_engine.py:173] Added request cmpl-9a91c7ef88524136b139d800acb428ef-0.
INFO 02-05 12:26:45 logger.py:36] Received request cmpl-2a458095cd2d48bfbd57aa3ecebe453a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:45 logger.py:36] Received request cmpl-c6e0f43c94fa4f18817864477eaea8a1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:45 async_llm_engine.py:173] Added request cmpl-2a458095cd2d48bfbd57aa3ecebe453a-0.
INFO 02-05 12:26:45 async_llm_engine.py:173] Added request cmpl-c6e0f43c94fa4f18817864477eaea8a1-0.
INFO 02-05 12:26:45 logger.py:36] Received request cmpl-ebf14148d0904ab8bce341c678eb9ea5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:45 async_llm_engine.py:173] Added request cmpl-ebf14148d0904ab8bce341c678eb9ea5-0.
INFO 02-05 12:26:45 logger.py:36] Received request cmpl-60543d873562482186f57f64fe089b71-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:45 async_llm_engine.py:173] Added request cmpl-60543d873562482186f57f64fe089b71-0.
INFO 02-05 12:26:45 logger.py:36] Received request cmpl-5af47d0083624af9a38b379fad97b4af-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:45 async_llm_engine.py:173] Added request cmpl-5af47d0083624af9a38b379fad97b4af-0.
INFO 02-05 12:26:45 logger.py:36] Received request cmpl-31a23c037dac4970848e697d9b068d92-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:45 async_llm_engine.py:173] Added request cmpl-31a23c037dac4970848e697d9b068d92-0.
INFO 02-05 12:26:45 logger.py:36] Received request cmpl-6b84a1ed972d47e59f3f60d3fe8784de-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:45 async_llm_engine.py:173] Added request cmpl-6b84a1ed972d47e59f3f60d3fe8784de-0.
INFO 02-05 12:26:45 logger.py:36] Received request cmpl-b74d6d1f54a44632ae0fd93f549b44ca-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:45 async_llm_engine.py:173] Added request cmpl-b74d6d1f54a44632ae0fd93f549b44ca-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:49396 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:49410 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:26:46 async_llm_engine.py:140] Finished request cmpl-83f0649ebb434ba493cef7c9e40ef418-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:46 async_llm_engine.py:140] Finished request cmpl-9a91c7ef88524136b139d800acb428ef-0.
INFO 02-05 12:26:46 async_llm_engine.py:140] Finished request cmpl-2a458095cd2d48bfbd57aa3ecebe453a-0.
INFO 02-05 12:26:46 async_llm_engine.py:140] Finished request cmpl-c6e0f43c94fa4f18817864477eaea8a1-0.
INFO 02-05 12:26:46 async_llm_engine.py:140] Finished request cmpl-ebf14148d0904ab8bce341c678eb9ea5-0.
INFO 02-05 12:26:46 async_llm_engine.py:140] Finished request cmpl-60543d873562482186f57f64fe089b71-0.
INFO 02-05 12:26:46 async_llm_engine.py:140] Finished request cmpl-5af47d0083624af9a38b379fad97b4af-0.
INFO 02-05 12:26:46 async_llm_engine.py:140] Finished request cmpl-31a23c037dac4970848e697d9b068d92-0.
INFO 02-05 12:26:46 async_llm_engine.py:140] Finished request cmpl-6b84a1ed972d47e59f3f60d3fe8784de-0.
INFO 02-05 12:26:46 async_llm_engine.py:140] Finished request cmpl-b74d6d1f54a44632ae0fd93f549b44ca-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:46 logger.py:36] Received request cmpl-fe1d52f3610443228845f6bb682cedb0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:46 logger.py:36] Received request cmpl-6ab7ad5fc1bf44338363662b3c9839fb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:46 async_llm_engine.py:173] Added request cmpl-fe1d52f3610443228845f6bb682cedb0-0.
INFO 02-05 12:26:46 async_llm_engine.py:173] Added request cmpl-6ab7ad5fc1bf44338363662b3c9839fb-0.
INFO 02-05 12:26:46 logger.py:36] Received request cmpl-95694c5236214ef69b10e7ce69c856cc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:46 async_llm_engine.py:173] Added request cmpl-95694c5236214ef69b10e7ce69c856cc-0.
INFO 02-05 12:26:46 logger.py:36] Received request cmpl-982a13fdc0d740afb550c20805cb3c7a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:46 async_llm_engine.py:173] Added request cmpl-982a13fdc0d740afb550c20805cb3c7a-0.
INFO 02-05 12:26:46 logger.py:36] Received request cmpl-e506af385b274ea8ad9be967e3d3af72-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:46 logger.py:36] Received request cmpl-60245e72d41840deabfddee5c813a21a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:46 async_llm_engine.py:173] Added request cmpl-e506af385b274ea8ad9be967e3d3af72-0.
INFO 02-05 12:26:46 async_llm_engine.py:173] Added request cmpl-60245e72d41840deabfddee5c813a21a-0.
INFO 02-05 12:26:46 logger.py:36] Received request cmpl-7ed1ecb0208c400f931a4418f582c032-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:46 async_llm_engine.py:173] Added request cmpl-7ed1ecb0208c400f931a4418f582c032-0.
INFO 02-05 12:26:46 logger.py:36] Received request cmpl-6d59f769f6204396995b6687e03342c9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:46 async_llm_engine.py:173] Added request cmpl-6d59f769f6204396995b6687e03342c9-0.
INFO 02-05 12:26:46 logger.py:36] Received request cmpl-f7d3036746334de09c2ba4caff9e71da-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:46 logger.py:36] Received request cmpl-d3cfeef535b04a85a2a22f4c7da1d8c9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:46 async_llm_engine.py:173] Added request cmpl-f7d3036746334de09c2ba4caff9e71da-0.
INFO 02-05 12:26:46 async_llm_engine.py:173] Added request cmpl-d3cfeef535b04a85a2a22f4c7da1d8c9-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:47 async_llm_engine.py:140] Finished request cmpl-fe1d52f3610443228845f6bb682cedb0-0.
INFO 02-05 12:26:47 async_llm_engine.py:140] Finished request cmpl-6ab7ad5fc1bf44338363662b3c9839fb-0.
INFO 02-05 12:26:47 async_llm_engine.py:140] Finished request cmpl-95694c5236214ef69b10e7ce69c856cc-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:47 async_llm_engine.py:140] Finished request cmpl-982a13fdc0d740afb550c20805cb3c7a-0.
INFO 02-05 12:26:47 async_llm_engine.py:140] Finished request cmpl-e506af385b274ea8ad9be967e3d3af72-0.
INFO 02-05 12:26:47 async_llm_engine.py:140] Finished request cmpl-60245e72d41840deabfddee5c813a21a-0.
INFO 02-05 12:26:47 async_llm_engine.py:140] Finished request cmpl-7ed1ecb0208c400f931a4418f582c032-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:47 async_llm_engine.py:140] Finished request cmpl-6d59f769f6204396995b6687e03342c9-0.
INFO 02-05 12:26:47 async_llm_engine.py:140] Finished request cmpl-f7d3036746334de09c2ba4caff9e71da-0.
INFO 02-05 12:26:47 async_llm_engine.py:140] Finished request cmpl-d3cfeef535b04a85a2a22f4c7da1d8c9-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:47 logger.py:36] Received request cmpl-77b5fbe452144a8badfc9024dbf5cf9d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:47 async_llm_engine.py:173] Added request cmpl-77b5fbe452144a8badfc9024dbf5cf9d-0.
INFO 02-05 12:26:47 logger.py:36] Received request cmpl-74ed33d955ce4245a1506e5487bf9237-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:47 async_llm_engine.py:173] Added request cmpl-74ed33d955ce4245a1506e5487bf9237-0.
INFO 02-05 12:26:47 logger.py:36] Received request cmpl-b0c690f17cee41d8b6a0df3fee65ad6f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:47 async_llm_engine.py:173] Added request cmpl-b0c690f17cee41d8b6a0df3fee65ad6f-0.
INFO 02-05 12:26:47 logger.py:36] Received request cmpl-18a534306f244616a6e454b60482cc37-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:47 async_llm_engine.py:173] Added request cmpl-18a534306f244616a6e454b60482cc37-0.
INFO 02-05 12:26:47 logger.py:36] Received request cmpl-23ba9b1915d543eaa7953c804900c199-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:47 async_llm_engine.py:173] Added request cmpl-23ba9b1915d543eaa7953c804900c199-0.
INFO 02-05 12:26:47 logger.py:36] Received request cmpl-a65f5da023be48ab9bd4bc32e9ee193d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:47 async_llm_engine.py:173] Added request cmpl-a65f5da023be48ab9bd4bc32e9ee193d-0.
INFO 02-05 12:26:47 logger.py:36] Received request cmpl-708de24ba4004a11980400d569a55b7e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:47 async_llm_engine.py:173] Added request cmpl-708de24ba4004a11980400d569a55b7e-0.
INFO 02-05 12:26:47 logger.py:36] Received request cmpl-8b19e8a9bab2417d86989bd6aa6b5b05-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:47 async_llm_engine.py:173] Added request cmpl-8b19e8a9bab2417d86989bd6aa6b5b05-0.
INFO 02-05 12:26:47 logger.py:36] Received request cmpl-34fd55f582f94541a91c12c19bf64609-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:47 async_llm_engine.py:173] Added request cmpl-34fd55f582f94541a91c12c19bf64609-0.
INFO 02-05 12:26:47 logger.py:36] Received request cmpl-9f17bdf874324696890757c9e4e4e4fd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:47 async_llm_engine.py:173] Added request cmpl-9f17bdf874324696890757c9e4e4e4fd-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:48 metrics.py:396] Avg prompt throughput: 72.0 tokens/s, Avg generation throughput: 508.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 02-05 12:26:48 async_llm_engine.py:140] Finished request cmpl-77b5fbe452144a8badfc9024dbf5cf9d-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:48 async_llm_engine.py:140] Finished request cmpl-74ed33d955ce4245a1506e5487bf9237-0.
INFO 02-05 12:26:48 async_llm_engine.py:140] Finished request cmpl-b0c690f17cee41d8b6a0df3fee65ad6f-0.
INFO 02-05 12:26:48 async_llm_engine.py:140] Finished request cmpl-18a534306f244616a6e454b60482cc37-0.
INFO 02-05 12:26:48 async_llm_engine.py:140] Finished request cmpl-23ba9b1915d543eaa7953c804900c199-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:48 async_llm_engine.py:140] Finished request cmpl-a65f5da023be48ab9bd4bc32e9ee193d-0.
INFO 02-05 12:26:48 async_llm_engine.py:140] Finished request cmpl-708de24ba4004a11980400d569a55b7e-0.
INFO 02-05 12:26:48 async_llm_engine.py:140] Finished request cmpl-8b19e8a9bab2417d86989bd6aa6b5b05-0.
INFO 02-05 12:26:48 async_llm_engine.py:140] Finished request cmpl-34fd55f582f94541a91c12c19bf64609-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:48 async_llm_engine.py:140] Finished request cmpl-9f17bdf874324696890757c9e4e4e4fd-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:48 logger.py:36] Received request cmpl-09b52b4ddd2f4fd9977bdaf74af72a16-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:48 async_llm_engine.py:173] Added request cmpl-09b52b4ddd2f4fd9977bdaf74af72a16-0.
INFO 02-05 12:26:48 logger.py:36] Received request cmpl-c185f2c400a24e569e4c811e4d508a1f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:48 async_llm_engine.py:173] Added request cmpl-c185f2c400a24e569e4c811e4d508a1f-0.
INFO 02-05 12:26:48 logger.py:36] Received request cmpl-ef5fb0974d434f73abe26e2c9c299d0e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:48 async_llm_engine.py:173] Added request cmpl-ef5fb0974d434f73abe26e2c9c299d0e-0.
INFO 02-05 12:26:49 logger.py:36] Received request cmpl-c0f0e92ee6da496fa40d4efb406ac977-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:49 async_llm_engine.py:173] Added request cmpl-c0f0e92ee6da496fa40d4efb406ac977-0.
INFO 02-05 12:26:49 logger.py:36] Received request cmpl-bf48d8911dae4ab4b0b9a21ad87db536-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:49 async_llm_engine.py:173] Added request cmpl-bf48d8911dae4ab4b0b9a21ad87db536-0.
INFO 02-05 12:26:49 logger.py:36] Received request cmpl-b0e19621ac7b4ac0b76273c7d08af910-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:49 logger.py:36] Received request cmpl-ad46c24c407d484fa27b4f6f0b86ee14-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:49 async_llm_engine.py:173] Added request cmpl-b0e19621ac7b4ac0b76273c7d08af910-0.
INFO 02-05 12:26:49 async_llm_engine.py:173] Added request cmpl-ad46c24c407d484fa27b4f6f0b86ee14-0.
INFO 02-05 12:26:49 logger.py:36] Received request cmpl-4c8bdd6bdff74ae2ab6b17d7d6b551d2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:49 async_llm_engine.py:173] Added request cmpl-4c8bdd6bdff74ae2ab6b17d7d6b551d2-0.
INFO 02-05 12:26:49 logger.py:36] Received request cmpl-4e281499d11849398359c66af35f491e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:49 async_llm_engine.py:173] Added request cmpl-4e281499d11849398359c66af35f491e-0.
INFO 02-05 12:26:49 logger.py:36] Received request cmpl-a16cadbe24d44f13ad8fc4b681423afb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:49 async_llm_engine.py:173] Added request cmpl-a16cadbe24d44f13ad8fc4b681423afb-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:50 async_llm_engine.py:140] Finished request cmpl-09b52b4ddd2f4fd9977bdaf74af72a16-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:50 async_llm_engine.py:140] Finished request cmpl-c185f2c400a24e569e4c811e4d508a1f-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:50 async_llm_engine.py:140] Finished request cmpl-ef5fb0974d434f73abe26e2c9c299d0e-0.
INFO 02-05 12:26:50 async_llm_engine.py:140] Finished request cmpl-c0f0e92ee6da496fa40d4efb406ac977-0.
INFO 02-05 12:26:50 async_llm_engine.py:140] Finished request cmpl-bf48d8911dae4ab4b0b9a21ad87db536-0.
INFO 02-05 12:26:50 async_llm_engine.py:140] Finished request cmpl-b0e19621ac7b4ac0b76273c7d08af910-0.
INFO 02-05 12:26:50 async_llm_engine.py:140] Finished request cmpl-ad46c24c407d484fa27b4f6f0b86ee14-0.
INFO 02-05 12:26:50 async_llm_engine.py:140] Finished request cmpl-4c8bdd6bdff74ae2ab6b17d7d6b551d2-0.
INFO 02-05 12:26:50 async_llm_engine.py:140] Finished request cmpl-4e281499d11849398359c66af35f491e-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:50 async_llm_engine.py:140] Finished request cmpl-a16cadbe24d44f13ad8fc4b681423afb-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:50 logger.py:36] Received request cmpl-044071a0b00b47e8ab60df86c3167ae0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:50 async_llm_engine.py:173] Added request cmpl-044071a0b00b47e8ab60df86c3167ae0-0.
INFO 02-05 12:26:50 logger.py:36] Received request cmpl-5a7631f5a265474f8af6aec0cd50fb3d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:50 async_llm_engine.py:173] Added request cmpl-5a7631f5a265474f8af6aec0cd50fb3d-0.
INFO 02-05 12:26:50 logger.py:36] Received request cmpl-b7b87bd0f9604218abb48625e78ae654-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:50 logger.py:36] Received request cmpl-46bd34f8ae82418885ce1198e5d6aed1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:50 async_llm_engine.py:173] Added request cmpl-b7b87bd0f9604218abb48625e78ae654-0.
INFO 02-05 12:26:50 async_llm_engine.py:173] Added request cmpl-46bd34f8ae82418885ce1198e5d6aed1-0.
INFO 02-05 12:26:50 logger.py:36] Received request cmpl-53e9582a1c5b498ba24c40e01eb6c7f7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:50 async_llm_engine.py:173] Added request cmpl-53e9582a1c5b498ba24c40e01eb6c7f7-0.
INFO 02-05 12:26:50 logger.py:36] Received request cmpl-5e0f80a595dc4246bc16dfe00e089a8c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:50 async_llm_engine.py:173] Added request cmpl-5e0f80a595dc4246bc16dfe00e089a8c-0.
INFO 02-05 12:26:50 logger.py:36] Received request cmpl-ac6f101f8d154ea69ee813b471b7781e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:50 logger.py:36] Received request cmpl-7121e74950d141c1b11d2914c25d5bab-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:50 async_llm_engine.py:173] Added request cmpl-ac6f101f8d154ea69ee813b471b7781e-0.
INFO 02-05 12:26:50 async_llm_engine.py:173] Added request cmpl-7121e74950d141c1b11d2914c25d5bab-0.
INFO 02-05 12:26:50 logger.py:36] Received request cmpl-9067fcb5eeb247ce926007936c1a570e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:50 async_llm_engine.py:173] Added request cmpl-9067fcb5eeb247ce926007936c1a570e-0.
INFO 02-05 12:26:50 logger.py:36] Received request cmpl-432737fc35084ec5ab4517c950861304-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:50 async_llm_engine.py:173] Added request cmpl-432737fc35084ec5ab4517c950861304-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:51 async_llm_engine.py:140] Finished request cmpl-044071a0b00b47e8ab60df86c3167ae0-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:51 async_llm_engine.py:140] Finished request cmpl-5a7631f5a265474f8af6aec0cd50fb3d-0.
INFO 02-05 12:26:51 async_llm_engine.py:140] Finished request cmpl-b7b87bd0f9604218abb48625e78ae654-0.
INFO 02-05 12:26:51 async_llm_engine.py:140] Finished request cmpl-46bd34f8ae82418885ce1198e5d6aed1-0.
INFO 02-05 12:26:51 async_llm_engine.py:140] Finished request cmpl-53e9582a1c5b498ba24c40e01eb6c7f7-0.
INFO 02-05 12:26:51 async_llm_engine.py:140] Finished request cmpl-5e0f80a595dc4246bc16dfe00e089a8c-0.
INFO 02-05 12:26:51 async_llm_engine.py:140] Finished request cmpl-ac6f101f8d154ea69ee813b471b7781e-0.
INFO 02-05 12:26:51 async_llm_engine.py:140] Finished request cmpl-7121e74950d141c1b11d2914c25d5bab-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:51 async_llm_engine.py:140] Finished request cmpl-9067fcb5eeb247ce926007936c1a570e-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:51 async_llm_engine.py:140] Finished request cmpl-432737fc35084ec5ab4517c950861304-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:51 logger.py:36] Received request cmpl-af81875582f14bfb833fb1aa1c9c15a2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:51 async_llm_engine.py:173] Added request cmpl-af81875582f14bfb833fb1aa1c9c15a2-0.
INFO 02-05 12:26:51 logger.py:36] Received request cmpl-a7745b5111024bc7972b68881ad5eb62-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:51 async_llm_engine.py:173] Added request cmpl-a7745b5111024bc7972b68881ad5eb62-0.
INFO 02-05 12:26:51 logger.py:36] Received request cmpl-06680e8e230f46f787fbb47c4d058356-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:51 async_llm_engine.py:173] Added request cmpl-06680e8e230f46f787fbb47c4d058356-0.
INFO 02-05 12:26:51 logger.py:36] Received request cmpl-879e8d32f1e44454ab1a1b7efb052e4c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:51 async_llm_engine.py:173] Added request cmpl-879e8d32f1e44454ab1a1b7efb052e4c-0.
INFO 02-05 12:26:51 logger.py:36] Received request cmpl-b3f71a1b5833409995f1c3a77b1791c7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:51 logger.py:36] Received request cmpl-19ec520d511f402fb46d7b5ca9266779-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:51 async_llm_engine.py:173] Added request cmpl-b3f71a1b5833409995f1c3a77b1791c7-0.
INFO 02-05 12:26:51 async_llm_engine.py:173] Added request cmpl-19ec520d511f402fb46d7b5ca9266779-0.
INFO 02-05 12:26:51 logger.py:36] Received request cmpl-cc3382e3aba4469b9901f4b27d6a80ac-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:51 async_llm_engine.py:173] Added request cmpl-cc3382e3aba4469b9901f4b27d6a80ac-0.
INFO 02-05 12:26:51 logger.py:36] Received request cmpl-197f78d562c04e8fbc22addf0cbc3835-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:51 async_llm_engine.py:173] Added request cmpl-197f78d562c04e8fbc22addf0cbc3835-0.
INFO 02-05 12:26:51 logger.py:36] Received request cmpl-a7652ee393804bd6a05b94351d1a5da8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:51 async_llm_engine.py:173] Added request cmpl-a7652ee393804bd6a05b94351d1a5da8-0.
INFO 02-05 12:26:51 logger.py:36] Received request cmpl-d008b8fd648b4a8eb258054a4cf6a46f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:51 async_llm_engine.py:173] Added request cmpl-d008b8fd648b4a8eb258054a4cf6a46f-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:52 async_llm_engine.py:140] Finished request cmpl-af81875582f14bfb833fb1aa1c9c15a2-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:52 async_llm_engine.py:140] Finished request cmpl-a7745b5111024bc7972b68881ad5eb62-0.
INFO 02-05 12:26:52 async_llm_engine.py:140] Finished request cmpl-06680e8e230f46f787fbb47c4d058356-0.
INFO 02-05 12:26:52 async_llm_engine.py:140] Finished request cmpl-879e8d32f1e44454ab1a1b7efb052e4c-0.
INFO 02-05 12:26:52 async_llm_engine.py:140] Finished request cmpl-b3f71a1b5833409995f1c3a77b1791c7-0.
INFO 02-05 12:26:52 async_llm_engine.py:140] Finished request cmpl-19ec520d511f402fb46d7b5ca9266779-0.
INFO 02-05 12:26:52 async_llm_engine.py:140] Finished request cmpl-cc3382e3aba4469b9901f4b27d6a80ac-0.
INFO 02-05 12:26:52 async_llm_engine.py:140] Finished request cmpl-197f78d562c04e8fbc22addf0cbc3835-0.
INFO 02-05 12:26:52 async_llm_engine.py:140] Finished request cmpl-a7652ee393804bd6a05b94351d1a5da8-0.
INFO 02-05 12:26:52 async_llm_engine.py:140] Finished request cmpl-d008b8fd648b4a8eb258054a4cf6a46f-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:52 logger.py:36] Received request cmpl-edef61f2d2e74f8a87736cd1bbdeca8d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:52 async_llm_engine.py:173] Added request cmpl-edef61f2d2e74f8a87736cd1bbdeca8d-0.
INFO 02-05 12:26:52 logger.py:36] Received request cmpl-7357e7d0ec3b42d9839fbd60838dae4b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:52 async_llm_engine.py:173] Added request cmpl-7357e7d0ec3b42d9839fbd60838dae4b-0.
INFO 02-05 12:26:52 logger.py:36] Received request cmpl-d91fa6f3e7d04c2f8c24533a5beee702-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:52 logger.py:36] Received request cmpl-5ff43f5deaed44729240b50fc1aa31e9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:52 logger.py:36] Received request cmpl-a8affb71204646ae9f051ec479bcefb8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:52 logger.py:36] Received request cmpl-e29846dbb9e843568448762e9bb64e9c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:52 async_llm_engine.py:173] Added request cmpl-d91fa6f3e7d04c2f8c24533a5beee702-0.
INFO 02-05 12:26:52 async_llm_engine.py:173] Added request cmpl-5ff43f5deaed44729240b50fc1aa31e9-0.
INFO 02-05 12:26:52 async_llm_engine.py:173] Added request cmpl-a8affb71204646ae9f051ec479bcefb8-0.
INFO 02-05 12:26:52 async_llm_engine.py:173] Added request cmpl-e29846dbb9e843568448762e9bb64e9c-0.
INFO 02-05 12:26:52 logger.py:36] Received request cmpl-4dd53d8bfe9f4c20a2108cf52247d052-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:52 async_llm_engine.py:173] Added request cmpl-4dd53d8bfe9f4c20a2108cf52247d052-0.
INFO:     192.168.200.241:45894 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:52 logger.py:36] Received request cmpl-2a09861717744da7b9e29fc8510fe1b3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:52 async_llm_engine.py:173] Added request cmpl-2a09861717744da7b9e29fc8510fe1b3-0.
INFO 02-05 12:26:52 logger.py:36] Received request cmpl-a27eab9e7c6742d8b6af3ae65d60a912-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:52 logger.py:36] Received request cmpl-89e4bcf631a841ae8b0b9e11cb92abca-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:52 async_llm_engine.py:173] Added request cmpl-a27eab9e7c6742d8b6af3ae65d60a912-0.
INFO 02-05 12:26:52 async_llm_engine.py:173] Added request cmpl-89e4bcf631a841ae8b0b9e11cb92abca-0.
INFO 02-05 12:26:53 async_llm_engine.py:140] Finished request cmpl-edef61f2d2e74f8a87736cd1bbdeca8d-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:53 async_llm_engine.py:140] Finished request cmpl-7357e7d0ec3b42d9839fbd60838dae4b-0.
INFO 02-05 12:26:53 async_llm_engine.py:140] Finished request cmpl-d91fa6f3e7d04c2f8c24533a5beee702-0.
INFO 02-05 12:26:53 async_llm_engine.py:140] Finished request cmpl-5ff43f5deaed44729240b50fc1aa31e9-0.
INFO 02-05 12:26:53 async_llm_engine.py:140] Finished request cmpl-a8affb71204646ae9f051ec479bcefb8-0.
INFO 02-05 12:26:53 async_llm_engine.py:140] Finished request cmpl-e29846dbb9e843568448762e9bb64e9c-0.
INFO 02-05 12:26:53 async_llm_engine.py:140] Finished request cmpl-4dd53d8bfe9f4c20a2108cf52247d052-0.
INFO 02-05 12:26:53 async_llm_engine.py:140] Finished request cmpl-2a09861717744da7b9e29fc8510fe1b3-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:53 metrics.py:396] Avg prompt throughput: 72.0 tokens/s, Avg generation throughput: 516.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:26:53 async_llm_engine.py:140] Finished request cmpl-a27eab9e7c6742d8b6af3ae65d60a912-0.
INFO 02-05 12:26:53 async_llm_engine.py:140] Finished request cmpl-89e4bcf631a841ae8b0b9e11cb92abca-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:53 logger.py:36] Received request cmpl-17f3e79c459e43c58c38d5740f59db0e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:53 async_llm_engine.py:173] Added request cmpl-17f3e79c459e43c58c38d5740f59db0e-0.
INFO 02-05 12:26:53 logger.py:36] Received request cmpl-cfef158c25bc4046814c1458abc32313-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:53 async_llm_engine.py:173] Added request cmpl-cfef158c25bc4046814c1458abc32313-0.
INFO 02-05 12:26:53 logger.py:36] Received request cmpl-09481ad1f8af467bad995a0ec1da3f01-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:53 async_llm_engine.py:173] Added request cmpl-09481ad1f8af467bad995a0ec1da3f01-0.
INFO 02-05 12:26:53 logger.py:36] Received request cmpl-65818b4383ff4bdeb1549b91173ef145-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:53 async_llm_engine.py:173] Added request cmpl-65818b4383ff4bdeb1549b91173ef145-0.
INFO 02-05 12:26:53 logger.py:36] Received request cmpl-69058570c15d49edbf6861b732dd5980-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:53 async_llm_engine.py:173] Added request cmpl-69058570c15d49edbf6861b732dd5980-0.
INFO 02-05 12:26:53 logger.py:36] Received request cmpl-d9115355899a4aa7bd08248549ba0c4d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:53 async_llm_engine.py:173] Added request cmpl-d9115355899a4aa7bd08248549ba0c4d-0.
INFO 02-05 12:26:53 logger.py:36] Received request cmpl-6a0966ed8faa4fc6b085fdefce923fce-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:53 logger.py:36] Received request cmpl-7c3e9cc4d1d64899903910dc22ac1ed6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:53 async_llm_engine.py:173] Added request cmpl-6a0966ed8faa4fc6b085fdefce923fce-0.
INFO 02-05 12:26:53 async_llm_engine.py:173] Added request cmpl-7c3e9cc4d1d64899903910dc22ac1ed6-0.
INFO 02-05 12:26:53 logger.py:36] Received request cmpl-cf71fc832aa8498eb2a353b2e51acf0b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:53 logger.py:36] Received request cmpl-bbdb80b52ad3427fb6797ad876196cad-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:53 async_llm_engine.py:173] Added request cmpl-cf71fc832aa8498eb2a353b2e51acf0b-0.
INFO 02-05 12:26:53 async_llm_engine.py:173] Added request cmpl-bbdb80b52ad3427fb6797ad876196cad-0.
INFO 02-05 12:26:54 async_llm_engine.py:140] Finished request cmpl-17f3e79c459e43c58c38d5740f59db0e-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:55 async_llm_engine.py:140] Finished request cmpl-cfef158c25bc4046814c1458abc32313-0.
INFO 02-05 12:26:55 async_llm_engine.py:140] Finished request cmpl-09481ad1f8af467bad995a0ec1da3f01-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:55 async_llm_engine.py:140] Finished request cmpl-65818b4383ff4bdeb1549b91173ef145-0.
INFO 02-05 12:26:55 async_llm_engine.py:140] Finished request cmpl-69058570c15d49edbf6861b732dd5980-0.
INFO 02-05 12:26:55 async_llm_engine.py:140] Finished request cmpl-d9115355899a4aa7bd08248549ba0c4d-0.
INFO 02-05 12:26:55 async_llm_engine.py:140] Finished request cmpl-6a0966ed8faa4fc6b085fdefce923fce-0.
INFO 02-05 12:26:55 async_llm_engine.py:140] Finished request cmpl-7c3e9cc4d1d64899903910dc22ac1ed6-0.
INFO 02-05 12:26:55 async_llm_engine.py:140] Finished request cmpl-cf71fc832aa8498eb2a353b2e51acf0b-0.
INFO 02-05 12:26:55 async_llm_engine.py:140] Finished request cmpl-bbdb80b52ad3427fb6797ad876196cad-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:55 logger.py:36] Received request cmpl-5016b983cbc14ff2a5b2c6d29a4d9cb0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:55 async_llm_engine.py:173] Added request cmpl-5016b983cbc14ff2a5b2c6d29a4d9cb0-0.
INFO 02-05 12:26:55 logger.py:36] Received request cmpl-ac06d2253af5441c90b84513e7c79a8c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:55 async_llm_engine.py:173] Added request cmpl-ac06d2253af5441c90b84513e7c79a8c-0.
INFO 02-05 12:26:55 logger.py:36] Received request cmpl-9cb29c7f31a545a7ab6f8d0091cc0ca9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:55 async_llm_engine.py:173] Added request cmpl-9cb29c7f31a545a7ab6f8d0091cc0ca9-0.
INFO 02-05 12:26:55 logger.py:36] Received request cmpl-4039f834b88e4c27aa37f01588f13994-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:55 logger.py:36] Received request cmpl-3d10efaed7b54a3696c24e03224d813e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:55 async_llm_engine.py:173] Added request cmpl-4039f834b88e4c27aa37f01588f13994-0.
INFO 02-05 12:26:55 async_llm_engine.py:173] Added request cmpl-3d10efaed7b54a3696c24e03224d813e-0.
INFO 02-05 12:26:55 logger.py:36] Received request cmpl-e831a944fc1f4b53b9c5541a2ebe4a64-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:55 async_llm_engine.py:173] Added request cmpl-e831a944fc1f4b53b9c5541a2ebe4a64-0.
INFO 02-05 12:26:55 logger.py:36] Received request cmpl-3d7919ca388c40bfb7ff080df0f834ad-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:55 async_llm_engine.py:173] Added request cmpl-3d7919ca388c40bfb7ff080df0f834ad-0.
INFO 02-05 12:26:55 logger.py:36] Received request cmpl-38933a593d4c44e292cbf77634fe26b1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:55 async_llm_engine.py:173] Added request cmpl-38933a593d4c44e292cbf77634fe26b1-0.
INFO 02-05 12:26:55 logger.py:36] Received request cmpl-d895fbf8f1a24f3cb0fc0604d8c0398e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:55 async_llm_engine.py:173] Added request cmpl-d895fbf8f1a24f3cb0fc0604d8c0398e-0.
INFO 02-05 12:26:55 logger.py:36] Received request cmpl-4c7c0bc3eee1473eaf60cd710ccda3aa-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:55 async_llm_engine.py:173] Added request cmpl-4c7c0bc3eee1473eaf60cd710ccda3aa-0.
INFO:     89.105.200.105:46892 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:46898 - "GET /health HTTP/1.1" 200 OK
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:56 async_llm_engine.py:140] Finished request cmpl-5016b983cbc14ff2a5b2c6d29a4d9cb0-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:56 async_llm_engine.py:140] Finished request cmpl-ac06d2253af5441c90b84513e7c79a8c-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:56 async_llm_engine.py:140] Finished request cmpl-9cb29c7f31a545a7ab6f8d0091cc0ca9-0.
INFO 02-05 12:26:56 async_llm_engine.py:140] Finished request cmpl-4039f834b88e4c27aa37f01588f13994-0.
INFO 02-05 12:26:56 async_llm_engine.py:140] Finished request cmpl-3d10efaed7b54a3696c24e03224d813e-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:56 async_llm_engine.py:140] Finished request cmpl-e831a944fc1f4b53b9c5541a2ebe4a64-0.
INFO 02-05 12:26:56 async_llm_engine.py:140] Finished request cmpl-3d7919ca388c40bfb7ff080df0f834ad-0.
INFO 02-05 12:26:56 async_llm_engine.py:140] Finished request cmpl-38933a593d4c44e292cbf77634fe26b1-0.
INFO 02-05 12:26:56 async_llm_engine.py:140] Finished request cmpl-d895fbf8f1a24f3cb0fc0604d8c0398e-0.
INFO 02-05 12:26:56 async_llm_engine.py:140] Finished request cmpl-4c7c0bc3eee1473eaf60cd710ccda3aa-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:56 logger.py:36] Received request cmpl-6051a436ab7c41ecab5ff2a8da59c79d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:56 async_llm_engine.py:173] Added request cmpl-6051a436ab7c41ecab5ff2a8da59c79d-0.
INFO 02-05 12:26:56 logger.py:36] Received request cmpl-c85861b94b1b49b59862b4707385a723-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:56 async_llm_engine.py:173] Added request cmpl-c85861b94b1b49b59862b4707385a723-0.
INFO 02-05 12:26:56 logger.py:36] Received request cmpl-2b49055b58f7448b8651c7fed177fec9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:56 async_llm_engine.py:173] Added request cmpl-2b49055b58f7448b8651c7fed177fec9-0.
INFO 02-05 12:26:56 logger.py:36] Received request cmpl-ac0da66ae71549bd948106a83d9470b8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:56 async_llm_engine.py:173] Added request cmpl-ac0da66ae71549bd948106a83d9470b8-0.
INFO 02-05 12:26:56 logger.py:36] Received request cmpl-fb7d432276a4483f93976a6e30d31905-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:56 logger.py:36] Received request cmpl-71da28230c524e62ad61636b19d5fe5c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:56 async_llm_engine.py:173] Added request cmpl-fb7d432276a4483f93976a6e30d31905-0.
INFO 02-05 12:26:56 async_llm_engine.py:173] Added request cmpl-71da28230c524e62ad61636b19d5fe5c-0.
INFO 02-05 12:26:56 logger.py:36] Received request cmpl-b8a91aeb5c1149a49bef1596c3243a55-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:56 async_llm_engine.py:173] Added request cmpl-b8a91aeb5c1149a49bef1596c3243a55-0.
INFO 02-05 12:26:56 logger.py:36] Received request cmpl-b3e50db49a1941f891345aa744a8478f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:56 async_llm_engine.py:173] Added request cmpl-b3e50db49a1941f891345aa744a8478f-0.
INFO 02-05 12:26:56 logger.py:36] Received request cmpl-41d46b8602bf44239b7e570111c61942-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:56 async_llm_engine.py:173] Added request cmpl-41d46b8602bf44239b7e570111c61942-0.
INFO 02-05 12:26:56 logger.py:36] Received request cmpl-fb4c631a44a34aaf928dac6b13a78ac6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:56 async_llm_engine.py:173] Added request cmpl-fb4c631a44a34aaf928dac6b13a78ac6-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:57 async_llm_engine.py:140] Finished request cmpl-6051a436ab7c41ecab5ff2a8da59c79d-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:57 async_llm_engine.py:140] Finished request cmpl-c85861b94b1b49b59862b4707385a723-0.
INFO 02-05 12:26:57 async_llm_engine.py:140] Finished request cmpl-2b49055b58f7448b8651c7fed177fec9-0.
INFO 02-05 12:26:57 async_llm_engine.py:140] Finished request cmpl-ac0da66ae71549bd948106a83d9470b8-0.
INFO 02-05 12:26:57 async_llm_engine.py:140] Finished request cmpl-fb7d432276a4483f93976a6e30d31905-0.
INFO 02-05 12:26:57 async_llm_engine.py:140] Finished request cmpl-71da28230c524e62ad61636b19d5fe5c-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:57 async_llm_engine.py:140] Finished request cmpl-b8a91aeb5c1149a49bef1596c3243a55-0.
INFO 02-05 12:26:57 async_llm_engine.py:140] Finished request cmpl-b3e50db49a1941f891345aa744a8478f-0.
INFO 02-05 12:26:57 async_llm_engine.py:140] Finished request cmpl-41d46b8602bf44239b7e570111c61942-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:57 async_llm_engine.py:140] Finished request cmpl-fb4c631a44a34aaf928dac6b13a78ac6-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:57 logger.py:36] Received request cmpl-3cf70c621d2043309c3f73c6b403d244-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:57 async_llm_engine.py:173] Added request cmpl-3cf70c621d2043309c3f73c6b403d244-0.
INFO 02-05 12:26:57 logger.py:36] Received request cmpl-faf8c5151f554a3ea27b427a497acb5d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:57 async_llm_engine.py:173] Added request cmpl-faf8c5151f554a3ea27b427a497acb5d-0.
INFO 02-05 12:26:57 logger.py:36] Received request cmpl-f0d44158c5204654b425106542d85495-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:57 async_llm_engine.py:173] Added request cmpl-f0d44158c5204654b425106542d85495-0.
INFO 02-05 12:26:57 logger.py:36] Received request cmpl-2c4dc51e240f4e88bde86ed1a2f47226-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:57 async_llm_engine.py:173] Added request cmpl-2c4dc51e240f4e88bde86ed1a2f47226-0.
INFO 02-05 12:26:57 logger.py:36] Received request cmpl-4ec24d8dd097493fa7b6cffa103aeb2f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:57 logger.py:36] Received request cmpl-ec8aa5e338964bd99a5e3705ad9abb4e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:57 async_llm_engine.py:173] Added request cmpl-4ec24d8dd097493fa7b6cffa103aeb2f-0.
INFO 02-05 12:26:57 async_llm_engine.py:173] Added request cmpl-ec8aa5e338964bd99a5e3705ad9abb4e-0.
INFO 02-05 12:26:57 logger.py:36] Received request cmpl-e6728cac4f9840a7999a197da39ecb11-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:57 async_llm_engine.py:173] Added request cmpl-e6728cac4f9840a7999a197da39ecb11-0.
INFO 02-05 12:26:57 logger.py:36] Received request cmpl-e24ca6bd701a426d9f64e4b229966c84-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:57 async_llm_engine.py:173] Added request cmpl-e24ca6bd701a426d9f64e4b229966c84-0.
INFO 02-05 12:26:57 logger.py:36] Received request cmpl-813be88f944e4a5288f6d6d2c77ea8ed-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:57 async_llm_engine.py:173] Added request cmpl-813be88f944e4a5288f6d6d2c77ea8ed-0.
INFO 02-05 12:26:57 logger.py:36] Received request cmpl-cc363308fcae49b88c3be41ea025d938-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:57 async_llm_engine.py:173] Added request cmpl-cc363308fcae49b88c3be41ea025d938-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:58 async_llm_engine.py:140] Finished request cmpl-3cf70c621d2043309c3f73c6b403d244-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:58 async_llm_engine.py:140] Finished request cmpl-faf8c5151f554a3ea27b427a497acb5d-0.
INFO 02-05 12:26:58 async_llm_engine.py:140] Finished request cmpl-f0d44158c5204654b425106542d85495-0.
INFO 02-05 12:26:58 async_llm_engine.py:140] Finished request cmpl-2c4dc51e240f4e88bde86ed1a2f47226-0.
INFO 02-05 12:26:58 async_llm_engine.py:140] Finished request cmpl-4ec24d8dd097493fa7b6cffa103aeb2f-0.
INFO 02-05 12:26:58 async_llm_engine.py:140] Finished request cmpl-ec8aa5e338964bd99a5e3705ad9abb4e-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:58 async_llm_engine.py:140] Finished request cmpl-e6728cac4f9840a7999a197da39ecb11-0.
INFO 02-05 12:26:58 async_llm_engine.py:140] Finished request cmpl-e24ca6bd701a426d9f64e4b229966c84-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:58 async_llm_engine.py:140] Finished request cmpl-813be88f944e4a5288f6d6d2c77ea8ed-0.
INFO 02-05 12:26:58 async_llm_engine.py:140] Finished request cmpl-cc363308fcae49b88c3be41ea025d938-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:58 logger.py:36] Received request cmpl-1e42fca0249144d9aced19e948053dbe-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:58 async_llm_engine.py:173] Added request cmpl-1e42fca0249144d9aced19e948053dbe-0.
INFO 02-05 12:26:58 metrics.py:396] Avg prompt throughput: 73.7 tokens/s, Avg generation throughput: 511.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:26:58 logger.py:36] Received request cmpl-6484b8b4f6e34f4ca4aa199d5ba74d12-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:58 async_llm_engine.py:173] Added request cmpl-6484b8b4f6e34f4ca4aa199d5ba74d12-0.
INFO 02-05 12:26:58 logger.py:36] Received request cmpl-e052e43fcc06489680b4a88154eec1b3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:58 async_llm_engine.py:173] Added request cmpl-e052e43fcc06489680b4a88154eec1b3-0.
INFO 02-05 12:26:58 logger.py:36] Received request cmpl-da460a4f9b514c1f99d376dd3e99be79-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:58 async_llm_engine.py:173] Added request cmpl-da460a4f9b514c1f99d376dd3e99be79-0.
INFO 02-05 12:26:58 logger.py:36] Received request cmpl-cf0e0ac830c344678c567a75a265d3a7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:58 async_llm_engine.py:173] Added request cmpl-cf0e0ac830c344678c567a75a265d3a7-0.
INFO 02-05 12:26:58 logger.py:36] Received request cmpl-679456ff5050401c91d48c57224f79e1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:58 async_llm_engine.py:173] Added request cmpl-679456ff5050401c91d48c57224f79e1-0.
INFO 02-05 12:26:58 logger.py:36] Received request cmpl-427780c71e3443f49bdff86e67617e01-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:58 logger.py:36] Received request cmpl-ad5f91c9fcf04a4ea9daa96f4c603e51-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:58 async_llm_engine.py:173] Added request cmpl-427780c71e3443f49bdff86e67617e01-0.
INFO 02-05 12:26:58 async_llm_engine.py:173] Added request cmpl-ad5f91c9fcf04a4ea9daa96f4c603e51-0.
INFO 02-05 12:26:58 logger.py:36] Received request cmpl-de3dd6e4eb9b4b60b94282bc22b05308-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:58 async_llm_engine.py:173] Added request cmpl-de3dd6e4eb9b4b60b94282bc22b05308-0.
INFO 02-05 12:26:58 logger.py:36] Received request cmpl-6b2380b5c8944e11a1efc1db11062e79-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:26:58 async_llm_engine.py:173] Added request cmpl-6b2380b5c8944e11a1efc1db11062e79-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:26:59 async_llm_engine.py:140] Finished request cmpl-1e42fca0249144d9aced19e948053dbe-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:59 async_llm_engine.py:140] Finished request cmpl-6484b8b4f6e34f4ca4aa199d5ba74d12-0.
INFO 02-05 12:26:59 async_llm_engine.py:140] Finished request cmpl-e052e43fcc06489680b4a88154eec1b3-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:59 async_llm_engine.py:140] Finished request cmpl-da460a4f9b514c1f99d376dd3e99be79-0.
INFO 02-05 12:26:59 async_llm_engine.py:140] Finished request cmpl-cf0e0ac830c344678c567a75a265d3a7-0.
INFO 02-05 12:26:59 async_llm_engine.py:140] Finished request cmpl-679456ff5050401c91d48c57224f79e1-0.
INFO 02-05 12:26:59 async_llm_engine.py:140] Finished request cmpl-427780c71e3443f49bdff86e67617e01-0.
INFO 02-05 12:26:59 async_llm_engine.py:140] Finished request cmpl-ad5f91c9fcf04a4ea9daa96f4c603e51-0.
INFO 02-05 12:26:59 async_llm_engine.py:140] Finished request cmpl-de3dd6e4eb9b4b60b94282bc22b05308-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:26:59 async_llm_engine.py:140] Finished request cmpl-6b2380b5c8944e11a1efc1db11062e79-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:00 logger.py:36] Received request cmpl-e7d7260f6a1f496ebbe95ac5cd6c01b2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:00 async_llm_engine.py:173] Added request cmpl-e7d7260f6a1f496ebbe95ac5cd6c01b2-0.
INFO 02-05 12:27:00 logger.py:36] Received request cmpl-7542ca8b85e349d29d4a15730f0ed3b3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:00 async_llm_engine.py:173] Added request cmpl-7542ca8b85e349d29d4a15730f0ed3b3-0.
INFO 02-05 12:27:00 logger.py:36] Received request cmpl-f942cc7059234e299fcd4e4b8725f743-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:00 async_llm_engine.py:173] Added request cmpl-f942cc7059234e299fcd4e4b8725f743-0.
INFO 02-05 12:27:00 logger.py:36] Received request cmpl-d6d7af47ef6143ccab595bdfb5eeb9d2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:00 async_llm_engine.py:173] Added request cmpl-d6d7af47ef6143ccab595bdfb5eeb9d2-0.
INFO 02-05 12:27:00 logger.py:36] Received request cmpl-1f035c1f27e74bcd86fbbdfd4cf6cdbf-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:00 async_llm_engine.py:173] Added request cmpl-1f035c1f27e74bcd86fbbdfd4cf6cdbf-0.
INFO 02-05 12:27:00 logger.py:36] Received request cmpl-1d99856abc2b4f57b31a84e320f40073-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:00 async_llm_engine.py:173] Added request cmpl-1d99856abc2b4f57b31a84e320f40073-0.
INFO 02-05 12:27:00 logger.py:36] Received request cmpl-d54f9526fdf94468bc502d8d94bc7f38-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:00 async_llm_engine.py:173] Added request cmpl-d54f9526fdf94468bc502d8d94bc7f38-0.
INFO 02-05 12:27:00 logger.py:36] Received request cmpl-fc942119de22494baf7cc50227aa5cf0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:00 async_llm_engine.py:173] Added request cmpl-fc942119de22494baf7cc50227aa5cf0-0.
INFO 02-05 12:27:00 logger.py:36] Received request cmpl-137fa86fed974050a610728a3788203b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:00 async_llm_engine.py:173] Added request cmpl-137fa86fed974050a610728a3788203b-0.
INFO 02-05 12:27:00 logger.py:36] Received request cmpl-526087e35ea7496aafa59970c9e75d78-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:00 async_llm_engine.py:173] Added request cmpl-526087e35ea7496aafa59970c9e75d78-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:01 async_llm_engine.py:140] Finished request cmpl-e7d7260f6a1f496ebbe95ac5cd6c01b2-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:01 async_llm_engine.py:140] Finished request cmpl-7542ca8b85e349d29d4a15730f0ed3b3-0.
INFO 02-05 12:27:01 async_llm_engine.py:140] Finished request cmpl-f942cc7059234e299fcd4e4b8725f743-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:01 async_llm_engine.py:140] Finished request cmpl-d6d7af47ef6143ccab595bdfb5eeb9d2-0.
INFO 02-05 12:27:01 async_llm_engine.py:140] Finished request cmpl-1f035c1f27e74bcd86fbbdfd4cf6cdbf-0.
INFO 02-05 12:27:01 async_llm_engine.py:140] Finished request cmpl-1d99856abc2b4f57b31a84e320f40073-0.
INFO 02-05 12:27:01 async_llm_engine.py:140] Finished request cmpl-d54f9526fdf94468bc502d8d94bc7f38-0.
INFO 02-05 12:27:01 async_llm_engine.py:140] Finished request cmpl-fc942119de22494baf7cc50227aa5cf0-0.
INFO 02-05 12:27:01 async_llm_engine.py:140] Finished request cmpl-137fa86fed974050a610728a3788203b-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:01 async_llm_engine.py:140] Finished request cmpl-526087e35ea7496aafa59970c9e75d78-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:01 logger.py:36] Received request cmpl-0db2f222e4d748c5afc63a05c6b82053-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:01 async_llm_engine.py:173] Added request cmpl-0db2f222e4d748c5afc63a05c6b82053-0.
INFO 02-05 12:27:01 logger.py:36] Received request cmpl-ce758961f7584a8abddaaf15b922fffb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:01 async_llm_engine.py:173] Added request cmpl-ce758961f7584a8abddaaf15b922fffb-0.
INFO 02-05 12:27:01 logger.py:36] Received request cmpl-52117feb20404ca3bdb770fe9f2291ee-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:01 async_llm_engine.py:173] Added request cmpl-52117feb20404ca3bdb770fe9f2291ee-0.
INFO 02-05 12:27:01 logger.py:36] Received request cmpl-2fac0518ca6e40818559a29fd400ad7d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:01 logger.py:36] Received request cmpl-ffb93a60d8954103aa77df268f98585e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:01 async_llm_engine.py:173] Added request cmpl-2fac0518ca6e40818559a29fd400ad7d-0.
INFO 02-05 12:27:01 async_llm_engine.py:173] Added request cmpl-ffb93a60d8954103aa77df268f98585e-0.
INFO 02-05 12:27:01 logger.py:36] Received request cmpl-dbed6cc424cc4ef6938d710b370dc54f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:01 async_llm_engine.py:173] Added request cmpl-dbed6cc424cc4ef6938d710b370dc54f-0.
INFO 02-05 12:27:01 logger.py:36] Received request cmpl-04575462eec64dd68592dc3a8c2962bd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:01 async_llm_engine.py:173] Added request cmpl-04575462eec64dd68592dc3a8c2962bd-0.
INFO 02-05 12:27:01 logger.py:36] Received request cmpl-f7381e814c794e6ea8bcb8a006cb8eb3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:01 logger.py:36] Received request cmpl-3d6f90423c6549669db8e04cadbb4e5f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:01 async_llm_engine.py:173] Added request cmpl-f7381e814c794e6ea8bcb8a006cb8eb3-0.
INFO 02-05 12:27:01 async_llm_engine.py:173] Added request cmpl-3d6f90423c6549669db8e04cadbb4e5f-0.
INFO 02-05 12:27:01 logger.py:36] Received request cmpl-c90b9b75fb3d4389b9c19ac213045544-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:01 async_llm_engine.py:173] Added request cmpl-c90b9b75fb3d4389b9c19ac213045544-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:02 async_llm_engine.py:140] Finished request cmpl-0db2f222e4d748c5afc63a05c6b82053-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:02 async_llm_engine.py:140] Finished request cmpl-ce758961f7584a8abddaaf15b922fffb-0.
INFO 02-05 12:27:02 async_llm_engine.py:140] Finished request cmpl-52117feb20404ca3bdb770fe9f2291ee-0.
INFO 02-05 12:27:02 async_llm_engine.py:140] Finished request cmpl-2fac0518ca6e40818559a29fd400ad7d-0.
INFO 02-05 12:27:02 async_llm_engine.py:140] Finished request cmpl-ffb93a60d8954103aa77df268f98585e-0.
INFO 02-05 12:27:02 async_llm_engine.py:140] Finished request cmpl-dbed6cc424cc4ef6938d710b370dc54f-0.
INFO 02-05 12:27:02 async_llm_engine.py:140] Finished request cmpl-04575462eec64dd68592dc3a8c2962bd-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:02 async_llm_engine.py:140] Finished request cmpl-f7381e814c794e6ea8bcb8a006cb8eb3-0.
INFO 02-05 12:27:02 async_llm_engine.py:140] Finished request cmpl-3d6f90423c6549669db8e04cadbb4e5f-0.
INFO 02-05 12:27:02 async_llm_engine.py:140] Finished request cmpl-c90b9b75fb3d4389b9c19ac213045544-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:02 logger.py:36] Received request cmpl-3c43cef65ea946a3ac4d8b91358a792d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:02 async_llm_engine.py:173] Added request cmpl-3c43cef65ea946a3ac4d8b91358a792d-0.
INFO 02-05 12:27:02 logger.py:36] Received request cmpl-3b3d11f1f6874420be79c684dd96d00a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:02 async_llm_engine.py:173] Added request cmpl-3b3d11f1f6874420be79c684dd96d00a-0.
INFO 02-05 12:27:02 logger.py:36] Received request cmpl-3e1c59a593234bdb9fbdd3d5e097be1e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:02 async_llm_engine.py:173] Added request cmpl-3e1c59a593234bdb9fbdd3d5e097be1e-0.
INFO 02-05 12:27:02 logger.py:36] Received request cmpl-6a0626d1c2084115b8ff394591d2db23-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:02 logger.py:36] Received request cmpl-d95cbabd07d440f383a9e0cb3adcfca8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:02 async_llm_engine.py:173] Added request cmpl-6a0626d1c2084115b8ff394591d2db23-0.
INFO 02-05 12:27:02 async_llm_engine.py:173] Added request cmpl-d95cbabd07d440f383a9e0cb3adcfca8-0.
INFO 02-05 12:27:02 logger.py:36] Received request cmpl-f4100a8c0707402493eb6e5f1edb44d2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:02 async_llm_engine.py:173] Added request cmpl-f4100a8c0707402493eb6e5f1edb44d2-0.
INFO 02-05 12:27:02 logger.py:36] Received request cmpl-128aa9d6bc1749e2bbc759fa17764df5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:02 async_llm_engine.py:173] Added request cmpl-128aa9d6bc1749e2bbc759fa17764df5-0.
INFO 02-05 12:27:02 logger.py:36] Received request cmpl-feabaa68da244c0abfdc9ff9208120de-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:02 logger.py:36] Received request cmpl-03e0b46a3e9e44fdba287e2a6284a2b8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:02 async_llm_engine.py:173] Added request cmpl-feabaa68da244c0abfdc9ff9208120de-0.
INFO 02-05 12:27:02 async_llm_engine.py:173] Added request cmpl-03e0b46a3e9e44fdba287e2a6284a2b8-0.
INFO 02-05 12:27:02 logger.py:36] Received request cmpl-3433b0b33d5d41239f2f764d717ca809-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:02 async_llm_engine.py:173] Added request cmpl-3433b0b33d5d41239f2f764d717ca809-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:03 async_llm_engine.py:140] Finished request cmpl-3c43cef65ea946a3ac4d8b91358a792d-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:03 async_llm_engine.py:140] Finished request cmpl-3b3d11f1f6874420be79c684dd96d00a-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:03 async_llm_engine.py:140] Finished request cmpl-3e1c59a593234bdb9fbdd3d5e097be1e-0.
INFO 02-05 12:27:03 async_llm_engine.py:140] Finished request cmpl-6a0626d1c2084115b8ff394591d2db23-0.
INFO 02-05 12:27:03 async_llm_engine.py:140] Finished request cmpl-d95cbabd07d440f383a9e0cb3adcfca8-0.
INFO 02-05 12:27:03 async_llm_engine.py:140] Finished request cmpl-f4100a8c0707402493eb6e5f1edb44d2-0.
INFO 02-05 12:27:03 async_llm_engine.py:140] Finished request cmpl-128aa9d6bc1749e2bbc759fa17764df5-0.
INFO 02-05 12:27:03 async_llm_engine.py:140] Finished request cmpl-feabaa68da244c0abfdc9ff9208120de-0.
INFO 02-05 12:27:03 async_llm_engine.py:140] Finished request cmpl-03e0b46a3e9e44fdba287e2a6284a2b8-0.
INFO 02-05 12:27:03 async_llm_engine.py:140] Finished request cmpl-3433b0b33d5d41239f2f764d717ca809-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:03 logger.py:36] Received request cmpl-a8f6c55d245d429c993008d47ca36cb0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:03 async_llm_engine.py:173] Added request cmpl-a8f6c55d245d429c993008d47ca36cb0-0.
INFO 02-05 12:27:03 metrics.py:396] Avg prompt throughput: 72.0 tokens/s, Avg generation throughput: 512.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:27:03 logger.py:36] Received request cmpl-24d7d7bf5514402d9cac33bdab7f34a3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:03 async_llm_engine.py:173] Added request cmpl-24d7d7bf5514402d9cac33bdab7f34a3-0.
INFO 02-05 12:27:03 logger.py:36] Received request cmpl-43af66c7af134805aa1128cf6e46596d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:03 async_llm_engine.py:173] Added request cmpl-43af66c7af134805aa1128cf6e46596d-0.
INFO 02-05 12:27:03 logger.py:36] Received request cmpl-188a9f2d8b6542e9ad7b603d4115f957-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:03 async_llm_engine.py:173] Added request cmpl-188a9f2d8b6542e9ad7b603d4115f957-0.
INFO 02-05 12:27:03 logger.py:36] Received request cmpl-063ab846a443429f9a96acf51f6d2fe7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:03 async_llm_engine.py:173] Added request cmpl-063ab846a443429f9a96acf51f6d2fe7-0.
INFO 02-05 12:27:03 logger.py:36] Received request cmpl-a6a11393b7a047109811e36c98bd9e85-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:03 async_llm_engine.py:173] Added request cmpl-a6a11393b7a047109811e36c98bd9e85-0.
INFO 02-05 12:27:03 logger.py:36] Received request cmpl-335a7b5879e847bab2857023c8748d4c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:03 async_llm_engine.py:173] Added request cmpl-335a7b5879e847bab2857023c8748d4c-0.
INFO 02-05 12:27:03 logger.py:36] Received request cmpl-a81a91959e014db88b7df60ea32c6508-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:03 async_llm_engine.py:173] Added request cmpl-a81a91959e014db88b7df60ea32c6508-0.
INFO 02-05 12:27:03 logger.py:36] Received request cmpl-de6a8afb222242f69f0822007f7ca96c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:03 async_llm_engine.py:173] Added request cmpl-de6a8afb222242f69f0822007f7ca96c-0.
INFO 02-05 12:27:03 logger.py:36] Received request cmpl-5d1c2d0d3fca44afafb1e1b31ac9d2ba-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:03 async_llm_engine.py:173] Added request cmpl-5d1c2d0d3fca44afafb1e1b31ac9d2ba-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:04 async_llm_engine.py:140] Finished request cmpl-a8f6c55d245d429c993008d47ca36cb0-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:04 async_llm_engine.py:140] Finished request cmpl-24d7d7bf5514402d9cac33bdab7f34a3-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:04 async_llm_engine.py:140] Finished request cmpl-43af66c7af134805aa1128cf6e46596d-0.
INFO 02-05 12:27:04 async_llm_engine.py:140] Finished request cmpl-188a9f2d8b6542e9ad7b603d4115f957-0.
INFO 02-05 12:27:04 async_llm_engine.py:140] Finished request cmpl-063ab846a443429f9a96acf51f6d2fe7-0.
INFO 02-05 12:27:04 async_llm_engine.py:140] Finished request cmpl-a6a11393b7a047109811e36c98bd9e85-0.
INFO 02-05 12:27:04 async_llm_engine.py:140] Finished request cmpl-335a7b5879e847bab2857023c8748d4c-0.
INFO 02-05 12:27:04 async_llm_engine.py:140] Finished request cmpl-a81a91959e014db88b7df60ea32c6508-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:04 async_llm_engine.py:140] Finished request cmpl-de6a8afb222242f69f0822007f7ca96c-0.
INFO 02-05 12:27:04 async_llm_engine.py:140] Finished request cmpl-5d1c2d0d3fca44afafb1e1b31ac9d2ba-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:05 logger.py:36] Received request cmpl-a94d16451e134679a60596902f9faf12-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:05 async_llm_engine.py:173] Added request cmpl-a94d16451e134679a60596902f9faf12-0.
INFO 02-05 12:27:05 logger.py:36] Received request cmpl-0621e09271624cb89b390367b2c5c7cf-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:05 async_llm_engine.py:173] Added request cmpl-0621e09271624cb89b390367b2c5c7cf-0.
INFO 02-05 12:27:05 logger.py:36] Received request cmpl-9e34c1fb082140019340e5ac5a18005c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:05 async_llm_engine.py:173] Added request cmpl-9e34c1fb082140019340e5ac5a18005c-0.
INFO 02-05 12:27:05 logger.py:36] Received request cmpl-8649c6479f844ed8a656d01fac7830d6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:05 async_llm_engine.py:173] Added request cmpl-8649c6479f844ed8a656d01fac7830d6-0.
INFO 02-05 12:27:05 logger.py:36] Received request cmpl-8fc39af0bc8f46db88983962a4865feb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:05 async_llm_engine.py:173] Added request cmpl-8fc39af0bc8f46db88983962a4865feb-0.
INFO 02-05 12:27:05 logger.py:36] Received request cmpl-4379c42c279f4d73873298b76f55068e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:05 logger.py:36] Received request cmpl-3b5fdd68689e47feb6b2e484596bdbdf-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:05 async_llm_engine.py:173] Added request cmpl-4379c42c279f4d73873298b76f55068e-0.
INFO 02-05 12:27:05 async_llm_engine.py:173] Added request cmpl-3b5fdd68689e47feb6b2e484596bdbdf-0.
INFO 02-05 12:27:05 logger.py:36] Received request cmpl-2aae470126114a51bba38da43658750a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:05 async_llm_engine.py:173] Added request cmpl-2aae470126114a51bba38da43658750a-0.
INFO 02-05 12:27:05 logger.py:36] Received request cmpl-8c245f6105db48af99b9ff2eceadb7eb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:05 async_llm_engine.py:173] Added request cmpl-8c245f6105db48af99b9ff2eceadb7eb-0.
INFO 02-05 12:27:05 logger.py:36] Received request cmpl-a2e1d66d7eda48d2997285c2cb0a084a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:05 async_llm_engine.py:173] Added request cmpl-a2e1d66d7eda48d2997285c2cb0a084a-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:33264 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:33276 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:27:06 async_llm_engine.py:140] Finished request cmpl-a94d16451e134679a60596902f9faf12-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:06 async_llm_engine.py:140] Finished request cmpl-0621e09271624cb89b390367b2c5c7cf-0.
INFO 02-05 12:27:06 async_llm_engine.py:140] Finished request cmpl-9e34c1fb082140019340e5ac5a18005c-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:06 async_llm_engine.py:140] Finished request cmpl-8649c6479f844ed8a656d01fac7830d6-0.
INFO 02-05 12:27:06 async_llm_engine.py:140] Finished request cmpl-8fc39af0bc8f46db88983962a4865feb-0.
INFO 02-05 12:27:06 async_llm_engine.py:140] Finished request cmpl-4379c42c279f4d73873298b76f55068e-0.
INFO 02-05 12:27:06 async_llm_engine.py:140] Finished request cmpl-3b5fdd68689e47feb6b2e484596bdbdf-0.
INFO 02-05 12:27:06 async_llm_engine.py:140] Finished request cmpl-2aae470126114a51bba38da43658750a-0.
INFO 02-05 12:27:06 async_llm_engine.py:140] Finished request cmpl-8c245f6105db48af99b9ff2eceadb7eb-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:06 async_llm_engine.py:140] Finished request cmpl-a2e1d66d7eda48d2997285c2cb0a084a-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:06 logger.py:36] Received request cmpl-b234a5cb475e4c6aaa60a1b53fc01cd3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:06 async_llm_engine.py:173] Added request cmpl-b234a5cb475e4c6aaa60a1b53fc01cd3-0.
INFO 02-05 12:27:06 logger.py:36] Received request cmpl-d3f63ef26cfa4455a2905f0d259af1f4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:06 async_llm_engine.py:173] Added request cmpl-d3f63ef26cfa4455a2905f0d259af1f4-0.
INFO 02-05 12:27:06 logger.py:36] Received request cmpl-f7c9c82a85e4489891093b4826b402a0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:06 async_llm_engine.py:173] Added request cmpl-f7c9c82a85e4489891093b4826b402a0-0.
INFO 02-05 12:27:06 logger.py:36] Received request cmpl-6fc073e6ec6c46be84f3280bd537caa5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:06 async_llm_engine.py:173] Added request cmpl-6fc073e6ec6c46be84f3280bd537caa5-0.
INFO 02-05 12:27:06 logger.py:36] Received request cmpl-39c8a70c8b464e5ebbaa8cb00d34d90f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:06 async_llm_engine.py:173] Added request cmpl-39c8a70c8b464e5ebbaa8cb00d34d90f-0.
INFO 02-05 12:27:06 logger.py:36] Received request cmpl-e890d938b9214dacb0246f0650e70d48-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:06 async_llm_engine.py:173] Added request cmpl-e890d938b9214dacb0246f0650e70d48-0.
INFO 02-05 12:27:06 logger.py:36] Received request cmpl-ff9d714db69245878d31c22c7fc1c108-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:06 async_llm_engine.py:173] Added request cmpl-ff9d714db69245878d31c22c7fc1c108-0.
INFO 02-05 12:27:06 logger.py:36] Received request cmpl-fe9174d21146402d83728684897807bb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:06 async_llm_engine.py:173] Added request cmpl-fe9174d21146402d83728684897807bb-0.
INFO 02-05 12:27:06 logger.py:36] Received request cmpl-da7834e80b9c422390adeb9e868b4dc8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:06 logger.py:36] Received request cmpl-b44ce461577447c29b9c04985f883a21-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:06 async_llm_engine.py:173] Added request cmpl-da7834e80b9c422390adeb9e868b4dc8-0.
INFO 02-05 12:27:06 async_llm_engine.py:173] Added request cmpl-b44ce461577447c29b9c04985f883a21-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:07 async_llm_engine.py:140] Finished request cmpl-b234a5cb475e4c6aaa60a1b53fc01cd3-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:07 async_llm_engine.py:140] Finished request cmpl-d3f63ef26cfa4455a2905f0d259af1f4-0.
INFO 02-05 12:27:07 async_llm_engine.py:140] Finished request cmpl-f7c9c82a85e4489891093b4826b402a0-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:07 async_llm_engine.py:140] Finished request cmpl-6fc073e6ec6c46be84f3280bd537caa5-0.
INFO 02-05 12:27:07 async_llm_engine.py:140] Finished request cmpl-39c8a70c8b464e5ebbaa8cb00d34d90f-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:07 async_llm_engine.py:140] Finished request cmpl-e890d938b9214dacb0246f0650e70d48-0.
INFO 02-05 12:27:07 async_llm_engine.py:140] Finished request cmpl-ff9d714db69245878d31c22c7fc1c108-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:07 async_llm_engine.py:140] Finished request cmpl-fe9174d21146402d83728684897807bb-0.
INFO 02-05 12:27:07 async_llm_engine.py:140] Finished request cmpl-da7834e80b9c422390adeb9e868b4dc8-0.
INFO 02-05 12:27:07 async_llm_engine.py:140] Finished request cmpl-b44ce461577447c29b9c04985f883a21-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:07 logger.py:36] Received request cmpl-990f356f75394d71bc351fb7e20d3d80-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:07 async_llm_engine.py:173] Added request cmpl-990f356f75394d71bc351fb7e20d3d80-0.
INFO 02-05 12:27:07 logger.py:36] Received request cmpl-cc32f9afe4824853912facc5825eb47e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:07 async_llm_engine.py:173] Added request cmpl-cc32f9afe4824853912facc5825eb47e-0.
INFO 02-05 12:27:07 logger.py:36] Received request cmpl-8ddaa87d36204bd3aa16498d8d840839-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:07 async_llm_engine.py:173] Added request cmpl-8ddaa87d36204bd3aa16498d8d840839-0.
INFO 02-05 12:27:07 logger.py:36] Received request cmpl-501c868cf0334b55817a717efeb63c31-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:07 async_llm_engine.py:173] Added request cmpl-501c868cf0334b55817a717efeb63c31-0.
INFO 02-05 12:27:07 logger.py:36] Received request cmpl-82817441e16f44b78affc6d55213e93a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:07 async_llm_engine.py:173] Added request cmpl-82817441e16f44b78affc6d55213e93a-0.
INFO 02-05 12:27:07 logger.py:36] Received request cmpl-7c6a951a9ebd4268abc7ea906aade2ff-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:07 async_llm_engine.py:173] Added request cmpl-7c6a951a9ebd4268abc7ea906aade2ff-0.
INFO:     192.168.200.241:45938 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:07 logger.py:36] Received request cmpl-d70852fe7e0a426388c1fddd3d365cfc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:07 async_llm_engine.py:173] Added request cmpl-d70852fe7e0a426388c1fddd3d365cfc-0.
INFO 02-05 12:27:07 logger.py:36] Received request cmpl-e63a4ef571e7417d8020554061339e09-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:07 async_llm_engine.py:173] Added request cmpl-e63a4ef571e7417d8020554061339e09-0.
INFO 02-05 12:27:07 logger.py:36] Received request cmpl-06a683ce49654ce0838b30c33ef35977-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:07 async_llm_engine.py:173] Added request cmpl-06a683ce49654ce0838b30c33ef35977-0.
INFO 02-05 12:27:07 logger.py:36] Received request cmpl-2b4cddbccdb243e1b80f3512b492fcfe-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:07 async_llm_engine.py:173] Added request cmpl-2b4cddbccdb243e1b80f3512b492fcfe-0.
INFO 02-05 12:27:08 async_llm_engine.py:140] Finished request cmpl-990f356f75394d71bc351fb7e20d3d80-0.
INFO 02-05 12:27:08 async_llm_engine.py:140] Finished request cmpl-cc32f9afe4824853912facc5825eb47e-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:08 async_llm_engine.py:140] Finished request cmpl-8ddaa87d36204bd3aa16498d8d840839-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:08 async_llm_engine.py:140] Finished request cmpl-501c868cf0334b55817a717efeb63c31-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:08 async_llm_engine.py:140] Finished request cmpl-82817441e16f44b78affc6d55213e93a-0.
INFO 02-05 12:27:08 async_llm_engine.py:140] Finished request cmpl-7c6a951a9ebd4268abc7ea906aade2ff-0.
INFO 02-05 12:27:08 async_llm_engine.py:140] Finished request cmpl-d70852fe7e0a426388c1fddd3d365cfc-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:08 async_llm_engine.py:140] Finished request cmpl-e63a4ef571e7417d8020554061339e09-0.
INFO 02-05 12:27:08 async_llm_engine.py:140] Finished request cmpl-06a683ce49654ce0838b30c33ef35977-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:08 async_llm_engine.py:140] Finished request cmpl-2b4cddbccdb243e1b80f3512b492fcfe-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:08 logger.py:36] Received request cmpl-7cedcf170891463b91cc6e864618166a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:08 async_llm_engine.py:173] Added request cmpl-7cedcf170891463b91cc6e864618166a-0.
INFO 02-05 12:27:08 logger.py:36] Received request cmpl-1117099d6b5f4b09803520894a3822e3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:08 async_llm_engine.py:173] Added request cmpl-1117099d6b5f4b09803520894a3822e3-0.
INFO 02-05 12:27:08 logger.py:36] Received request cmpl-706d57f3d9484fc59ac08e0c4dd653a6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:08 async_llm_engine.py:173] Added request cmpl-706d57f3d9484fc59ac08e0c4dd653a6-0.
INFO 02-05 12:27:08 logger.py:36] Received request cmpl-74e21f52aca54dc5b78b79804e7c8521-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:08 async_llm_engine.py:173] Added request cmpl-74e21f52aca54dc5b78b79804e7c8521-0.
INFO 02-05 12:27:08 logger.py:36] Received request cmpl-f0de93ca6c0d4fc8a29e71e47c206a8f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:08 logger.py:36] Received request cmpl-f98436a39b6b4905938781419869b146-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:08 logger.py:36] Received request cmpl-6582657cb4ea48acbf6af5f807961776-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:08 async_llm_engine.py:173] Added request cmpl-f0de93ca6c0d4fc8a29e71e47c206a8f-0.
INFO 02-05 12:27:08 async_llm_engine.py:173] Added request cmpl-f98436a39b6b4905938781419869b146-0.
INFO 02-05 12:27:08 async_llm_engine.py:173] Added request cmpl-6582657cb4ea48acbf6af5f807961776-0.
INFO 02-05 12:27:08 logger.py:36] Received request cmpl-1a2ce76d5c444494bb9b259049229b67-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:08 async_llm_engine.py:173] Added request cmpl-1a2ce76d5c444494bb9b259049229b67-0.
INFO 02-05 12:27:08 logger.py:36] Received request cmpl-a7d5ca7ad8d74c529b5838b14cf664c6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:08 async_llm_engine.py:173] Added request cmpl-a7d5ca7ad8d74c529b5838b14cf664c6-0.
INFO 02-05 12:27:08 metrics.py:396] Avg prompt throughput: 75.5 tokens/s, Avg generation throughput: 512.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:27:08 logger.py:36] Received request cmpl-e7ba18c1534a41a99a403ae287bc5958-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:08 async_llm_engine.py:173] Added request cmpl-e7ba18c1534a41a99a403ae287bc5958-0.
INFO 02-05 12:27:09 async_llm_engine.py:140] Finished request cmpl-7cedcf170891463b91cc6e864618166a-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:09 async_llm_engine.py:140] Finished request cmpl-1117099d6b5f4b09803520894a3822e3-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:09 async_llm_engine.py:140] Finished request cmpl-706d57f3d9484fc59ac08e0c4dd653a6-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:09 async_llm_engine.py:140] Finished request cmpl-74e21f52aca54dc5b78b79804e7c8521-0.
INFO 02-05 12:27:09 async_llm_engine.py:140] Finished request cmpl-f0de93ca6c0d4fc8a29e71e47c206a8f-0.
INFO 02-05 12:27:09 async_llm_engine.py:140] Finished request cmpl-f98436a39b6b4905938781419869b146-0.
INFO 02-05 12:27:09 async_llm_engine.py:140] Finished request cmpl-6582657cb4ea48acbf6af5f807961776-0.
INFO 02-05 12:27:09 async_llm_engine.py:140] Finished request cmpl-1a2ce76d5c444494bb9b259049229b67-0.
INFO 02-05 12:27:09 async_llm_engine.py:140] Finished request cmpl-a7d5ca7ad8d74c529b5838b14cf664c6-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:09 async_llm_engine.py:140] Finished request cmpl-e7ba18c1534a41a99a403ae287bc5958-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:09 logger.py:36] Received request cmpl-568b23ae195c40b1a1f8678e980ad657-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:10 async_llm_engine.py:173] Added request cmpl-568b23ae195c40b1a1f8678e980ad657-0.
INFO 02-05 12:27:10 logger.py:36] Received request cmpl-3bdeb8688db648c3bd73a3fe0420a5e3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:10 logger.py:36] Received request cmpl-725253d9a7e148daa0092fb1435110c1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:10 async_llm_engine.py:173] Added request cmpl-3bdeb8688db648c3bd73a3fe0420a5e3-0.
INFO 02-05 12:27:10 async_llm_engine.py:173] Added request cmpl-725253d9a7e148daa0092fb1435110c1-0.
INFO 02-05 12:27:10 logger.py:36] Received request cmpl-8fbeba82a0344bd8b6ed45110d1b344d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:10 async_llm_engine.py:173] Added request cmpl-8fbeba82a0344bd8b6ed45110d1b344d-0.
INFO 02-05 12:27:10 logger.py:36] Received request cmpl-fab6229536fe4206a3867b853f7b482c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:10 async_llm_engine.py:173] Added request cmpl-fab6229536fe4206a3867b853f7b482c-0.
INFO 02-05 12:27:10 logger.py:36] Received request cmpl-be51b8eafa0c4061815d2f0a35cdec37-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:10 logger.py:36] Received request cmpl-5a29cd7be9f44a828d0c4740f3040632-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:10 async_llm_engine.py:173] Added request cmpl-be51b8eafa0c4061815d2f0a35cdec37-0.
INFO 02-05 12:27:10 async_llm_engine.py:173] Added request cmpl-5a29cd7be9f44a828d0c4740f3040632-0.
INFO 02-05 12:27:10 logger.py:36] Received request cmpl-8e5a6af0791d4ee496f8cd7944e0d6b2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:10 async_llm_engine.py:173] Added request cmpl-8e5a6af0791d4ee496f8cd7944e0d6b2-0.
INFO 02-05 12:27:10 logger.py:36] Received request cmpl-f86253560cac497690e16be338000e10-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:10 async_llm_engine.py:173] Added request cmpl-f86253560cac497690e16be338000e10-0.
INFO 02-05 12:27:10 logger.py:36] Received request cmpl-dd160bb3ae6144ef89fb1f339eadf991-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:10 async_llm_engine.py:173] Added request cmpl-dd160bb3ae6144ef89fb1f339eadf991-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:11 async_llm_engine.py:140] Finished request cmpl-568b23ae195c40b1a1f8678e980ad657-0.
INFO 02-05 12:27:11 async_llm_engine.py:140] Finished request cmpl-3bdeb8688db648c3bd73a3fe0420a5e3-0.
INFO 02-05 12:27:11 async_llm_engine.py:140] Finished request cmpl-725253d9a7e148daa0092fb1435110c1-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:11 async_llm_engine.py:140] Finished request cmpl-8fbeba82a0344bd8b6ed45110d1b344d-0.
INFO 02-05 12:27:11 async_llm_engine.py:140] Finished request cmpl-fab6229536fe4206a3867b853f7b482c-0.
INFO 02-05 12:27:11 async_llm_engine.py:140] Finished request cmpl-be51b8eafa0c4061815d2f0a35cdec37-0.
INFO 02-05 12:27:11 async_llm_engine.py:140] Finished request cmpl-5a29cd7be9f44a828d0c4740f3040632-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:11 async_llm_engine.py:140] Finished request cmpl-8e5a6af0791d4ee496f8cd7944e0d6b2-0.
INFO 02-05 12:27:11 async_llm_engine.py:140] Finished request cmpl-f86253560cac497690e16be338000e10-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:11 async_llm_engine.py:140] Finished request cmpl-dd160bb3ae6144ef89fb1f339eadf991-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:11 logger.py:36] Received request cmpl-2a1235d52ab040babc64e50161daafd7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:11 async_llm_engine.py:173] Added request cmpl-2a1235d52ab040babc64e50161daafd7-0.
INFO 02-05 12:27:11 logger.py:36] Received request cmpl-3567cd0432aa48e1b81fbd0f21298a0e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:11 async_llm_engine.py:173] Added request cmpl-3567cd0432aa48e1b81fbd0f21298a0e-0.
INFO 02-05 12:27:11 logger.py:36] Received request cmpl-9c2270eadc6c4559974de4646e455121-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:11 async_llm_engine.py:173] Added request cmpl-9c2270eadc6c4559974de4646e455121-0.
INFO 02-05 12:27:11 logger.py:36] Received request cmpl-5e0443da60a141b3ad98c82be323e317-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:11 async_llm_engine.py:173] Added request cmpl-5e0443da60a141b3ad98c82be323e317-0.
INFO 02-05 12:27:11 logger.py:36] Received request cmpl-3decafa7d57f47ea8c404481f8a21013-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:11 async_llm_engine.py:173] Added request cmpl-3decafa7d57f47ea8c404481f8a21013-0.
INFO 02-05 12:27:11 logger.py:36] Received request cmpl-bbbfe747320f4fac801bd146d927390d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:11 async_llm_engine.py:173] Added request cmpl-bbbfe747320f4fac801bd146d927390d-0.
INFO 02-05 12:27:11 logger.py:36] Received request cmpl-c1974df6885d4071a434c737f08bfcd1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:11 async_llm_engine.py:173] Added request cmpl-c1974df6885d4071a434c737f08bfcd1-0.
INFO 02-05 12:27:11 logger.py:36] Received request cmpl-8dbb9c9b584348fb8e2701096d63e204-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:11 async_llm_engine.py:173] Added request cmpl-8dbb9c9b584348fb8e2701096d63e204-0.
INFO 02-05 12:27:11 logger.py:36] Received request cmpl-689afce074fe4d7cb60687e5bff31cae-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:11 async_llm_engine.py:173] Added request cmpl-689afce074fe4d7cb60687e5bff31cae-0.
INFO 02-05 12:27:11 logger.py:36] Received request cmpl-1e7a454f90614f81a06734d5f3f34f9e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:11 async_llm_engine.py:173] Added request cmpl-1e7a454f90614f81a06734d5f3f34f9e-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:12 async_llm_engine.py:140] Finished request cmpl-2a1235d52ab040babc64e50161daafd7-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:12 async_llm_engine.py:140] Finished request cmpl-3567cd0432aa48e1b81fbd0f21298a0e-0.
INFO 02-05 12:27:12 async_llm_engine.py:140] Finished request cmpl-9c2270eadc6c4559974de4646e455121-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:12 async_llm_engine.py:140] Finished request cmpl-5e0443da60a141b3ad98c82be323e317-0.
INFO 02-05 12:27:12 async_llm_engine.py:140] Finished request cmpl-3decafa7d57f47ea8c404481f8a21013-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:12 async_llm_engine.py:140] Finished request cmpl-bbbfe747320f4fac801bd146d927390d-0.
INFO 02-05 12:27:12 async_llm_engine.py:140] Finished request cmpl-c1974df6885d4071a434c737f08bfcd1-0.
INFO 02-05 12:27:12 async_llm_engine.py:140] Finished request cmpl-8dbb9c9b584348fb8e2701096d63e204-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:12 async_llm_engine.py:140] Finished request cmpl-689afce074fe4d7cb60687e5bff31cae-0.
INFO 02-05 12:27:12 async_llm_engine.py:140] Finished request cmpl-1e7a454f90614f81a06734d5f3f34f9e-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:12 logger.py:36] Received request cmpl-a74ce332b9874c1a9d27c6bef83ab9bc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:12 async_llm_engine.py:173] Added request cmpl-a74ce332b9874c1a9d27c6bef83ab9bc-0.
INFO 02-05 12:27:12 logger.py:36] Received request cmpl-91d29e3f90ed4c339749e2d1c82105d6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:12 async_llm_engine.py:173] Added request cmpl-91d29e3f90ed4c339749e2d1c82105d6-0.
INFO 02-05 12:27:12 logger.py:36] Received request cmpl-3f04052bf46e486db45588664673674b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:12 async_llm_engine.py:173] Added request cmpl-3f04052bf46e486db45588664673674b-0.
INFO 02-05 12:27:12 logger.py:36] Received request cmpl-a244b2a06e574dd2b3788ceed01c901b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:12 async_llm_engine.py:173] Added request cmpl-a244b2a06e574dd2b3788ceed01c901b-0.
INFO 02-05 12:27:12 logger.py:36] Received request cmpl-6178db88ef6c448b95f933938de30782-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:12 logger.py:36] Received request cmpl-010f4e79d2914d09912aab57d48e166b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:12 async_llm_engine.py:173] Added request cmpl-6178db88ef6c448b95f933938de30782-0.
INFO 02-05 12:27:12 async_llm_engine.py:173] Added request cmpl-010f4e79d2914d09912aab57d48e166b-0.
INFO 02-05 12:27:12 logger.py:36] Received request cmpl-0a148e301ae841b1a3a8b6b62981db2b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:12 async_llm_engine.py:173] Added request cmpl-0a148e301ae841b1a3a8b6b62981db2b-0.
INFO 02-05 12:27:12 logger.py:36] Received request cmpl-5d17739513f14a72a4bf3c8fd59c741f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:12 async_llm_engine.py:173] Added request cmpl-5d17739513f14a72a4bf3c8fd59c741f-0.
INFO 02-05 12:27:12 logger.py:36] Received request cmpl-159fb945f3a148c5b49e55a9a94d4a9c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:12 async_llm_engine.py:173] Added request cmpl-159fb945f3a148c5b49e55a9a94d4a9c-0.
INFO 02-05 12:27:12 logger.py:36] Received request cmpl-775d1863f64643e891621b05e7dee885-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:12 async_llm_engine.py:173] Added request cmpl-775d1863f64643e891621b05e7dee885-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:13 async_llm_engine.py:140] Finished request cmpl-a74ce332b9874c1a9d27c6bef83ab9bc-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:13 async_llm_engine.py:140] Finished request cmpl-91d29e3f90ed4c339749e2d1c82105d6-0.
INFO 02-05 12:27:13 async_llm_engine.py:140] Finished request cmpl-3f04052bf46e486db45588664673674b-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:13 async_llm_engine.py:140] Finished request cmpl-a244b2a06e574dd2b3788ceed01c901b-0.
INFO 02-05 12:27:13 async_llm_engine.py:140] Finished request cmpl-6178db88ef6c448b95f933938de30782-0.
INFO 02-05 12:27:13 async_llm_engine.py:140] Finished request cmpl-010f4e79d2914d09912aab57d48e166b-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:13 async_llm_engine.py:140] Finished request cmpl-0a148e301ae841b1a3a8b6b62981db2b-0.
INFO 02-05 12:27:13 async_llm_engine.py:140] Finished request cmpl-5d17739513f14a72a4bf3c8fd59c741f-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:13 logger.py:36] Received request cmpl-332b80d4c218495aa8724b2a9d3f3772-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:13 async_llm_engine.py:173] Added request cmpl-332b80d4c218495aa8724b2a9d3f3772-0.
INFO 02-05 12:27:13 async_llm_engine.py:140] Finished request cmpl-159fb945f3a148c5b49e55a9a94d4a9c-0.
INFO 02-05 12:27:13 async_llm_engine.py:140] Finished request cmpl-775d1863f64643e891621b05e7dee885-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:13 logger.py:36] Received request cmpl-4dc97e3d02414999979d208ac6254d29-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:13 async_llm_engine.py:173] Added request cmpl-4dc97e3d02414999979d208ac6254d29-0.
INFO 02-05 12:27:13 logger.py:36] Received request cmpl-bb305078a367473ebe1e76c2fc257573-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:13 async_llm_engine.py:173] Added request cmpl-bb305078a367473ebe1e76c2fc257573-0.
INFO 02-05 12:27:13 logger.py:36] Received request cmpl-20f7f997143a46b5a549d04d52a07b87-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:13 async_llm_engine.py:173] Added request cmpl-20f7f997143a46b5a549d04d52a07b87-0.
INFO 02-05 12:27:13 logger.py:36] Received request cmpl-76b77f9088c94bfd968789e28add20f3-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:13 async_llm_engine.py:173] Added request cmpl-76b77f9088c94bfd968789e28add20f3-0.
INFO 02-05 12:27:13 logger.py:36] Received request cmpl-f5f8425f84674c7889b7e0456e23fca2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:13 async_llm_engine.py:173] Added request cmpl-f5f8425f84674c7889b7e0456e23fca2-0.
INFO 02-05 12:27:13 logger.py:36] Received request cmpl-c250abcd9d0c427fa346a01effbab873-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:13 async_llm_engine.py:173] Added request cmpl-c250abcd9d0c427fa346a01effbab873-0.
INFO 02-05 12:27:13 logger.py:36] Received request cmpl-a6ba8487691b46dab7d3e7439376123f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:13 async_llm_engine.py:173] Added request cmpl-a6ba8487691b46dab7d3e7439376123f-0.
INFO 02-05 12:27:13 logger.py:36] Received request cmpl-19db0023b23f450c9aaea55b1d5f37f7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:13 async_llm_engine.py:173] Added request cmpl-19db0023b23f450c9aaea55b1d5f37f7-0.
INFO 02-05 12:27:13 logger.py:36] Received request cmpl-422373c24ef549deba9ac2938cd064e4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:13 async_llm_engine.py:173] Added request cmpl-422373c24ef549deba9ac2938cd064e4-0.
INFO 02-05 12:27:13 metrics.py:396] Avg prompt throughput: 84.4 tokens/s, Avg generation throughput: 521.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:14 async_llm_engine.py:140] Finished request cmpl-332b80d4c218495aa8724b2a9d3f3772-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:14 async_llm_engine.py:140] Finished request cmpl-4dc97e3d02414999979d208ac6254d29-0.
INFO 02-05 12:27:14 async_llm_engine.py:140] Finished request cmpl-bb305078a367473ebe1e76c2fc257573-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:14 async_llm_engine.py:140] Finished request cmpl-20f7f997143a46b5a549d04d52a07b87-0.
INFO 02-05 12:27:14 async_llm_engine.py:140] Finished request cmpl-76b77f9088c94bfd968789e28add20f3-0.
INFO 02-05 12:27:14 async_llm_engine.py:140] Finished request cmpl-f5f8425f84674c7889b7e0456e23fca2-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:14 async_llm_engine.py:140] Finished request cmpl-c250abcd9d0c427fa346a01effbab873-0.
INFO 02-05 12:27:14 async_llm_engine.py:140] Finished request cmpl-a6ba8487691b46dab7d3e7439376123f-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:14 async_llm_engine.py:140] Finished request cmpl-19db0023b23f450c9aaea55b1d5f37f7-0.
INFO 02-05 12:27:14 async_llm_engine.py:140] Finished request cmpl-422373c24ef549deba9ac2938cd064e4-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:14 logger.py:36] Received request cmpl-72ad545165b94566a667f70e6f5c57c0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:14 async_llm_engine.py:173] Added request cmpl-72ad545165b94566a667f70e6f5c57c0-0.
INFO 02-05 12:27:14 logger.py:36] Received request cmpl-6170a16247c041eb90342fe6bcf2078f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:14 async_llm_engine.py:173] Added request cmpl-6170a16247c041eb90342fe6bcf2078f-0.
INFO 02-05 12:27:14 logger.py:36] Received request cmpl-c7cfcea95fbe4155b00f5d1600b1fbc0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:14 async_llm_engine.py:173] Added request cmpl-c7cfcea95fbe4155b00f5d1600b1fbc0-0.
INFO 02-05 12:27:14 logger.py:36] Received request cmpl-70b18206ef6746068f326ca230489dff-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:14 async_llm_engine.py:173] Added request cmpl-70b18206ef6746068f326ca230489dff-0.
INFO 02-05 12:27:14 logger.py:36] Received request cmpl-23178ef7148343afba57d3b08f057bc0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:14 async_llm_engine.py:173] Added request cmpl-23178ef7148343afba57d3b08f057bc0-0.
INFO 02-05 12:27:14 logger.py:36] Received request cmpl-0c13205dc0714c12bd4570ec58b7d169-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:14 async_llm_engine.py:173] Added request cmpl-0c13205dc0714c12bd4570ec58b7d169-0.
INFO 02-05 12:27:14 logger.py:36] Received request cmpl-e86946c61e48464b9402eeddd224bf7b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:14 async_llm_engine.py:173] Added request cmpl-e86946c61e48464b9402eeddd224bf7b-0.
INFO 02-05 12:27:14 logger.py:36] Received request cmpl-de6e8693f60c4910ab4309a8fb5fb7c4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:14 async_llm_engine.py:173] Added request cmpl-de6e8693f60c4910ab4309a8fb5fb7c4-0.
INFO 02-05 12:27:14 logger.py:36] Received request cmpl-e5463ef0191c4f9b9ac9c51a67d7fd19-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:14 async_llm_engine.py:173] Added request cmpl-e5463ef0191c4f9b9ac9c51a67d7fd19-0.
INFO 02-05 12:27:14 logger.py:36] Received request cmpl-4003898696234a1682cd2aa613d66ce1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:14 async_llm_engine.py:173] Added request cmpl-4003898696234a1682cd2aa613d66ce1-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:53504 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:53510 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:27:16 async_llm_engine.py:140] Finished request cmpl-72ad545165b94566a667f70e6f5c57c0-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:16 async_llm_engine.py:140] Finished request cmpl-6170a16247c041eb90342fe6bcf2078f-0.
INFO 02-05 12:27:16 async_llm_engine.py:140] Finished request cmpl-c7cfcea95fbe4155b00f5d1600b1fbc0-0.
INFO 02-05 12:27:16 async_llm_engine.py:140] Finished request cmpl-70b18206ef6746068f326ca230489dff-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:16 async_llm_engine.py:140] Finished request cmpl-23178ef7148343afba57d3b08f057bc0-0.
INFO 02-05 12:27:16 async_llm_engine.py:140] Finished request cmpl-0c13205dc0714c12bd4570ec58b7d169-0.
INFO 02-05 12:27:16 async_llm_engine.py:140] Finished request cmpl-e86946c61e48464b9402eeddd224bf7b-0.
INFO 02-05 12:27:16 async_llm_engine.py:140] Finished request cmpl-de6e8693f60c4910ab4309a8fb5fb7c4-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:16 async_llm_engine.py:140] Finished request cmpl-e5463ef0191c4f9b9ac9c51a67d7fd19-0.
INFO 02-05 12:27:16 async_llm_engine.py:140] Finished request cmpl-4003898696234a1682cd2aa613d66ce1-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:16 logger.py:36] Received request cmpl-c06ddd01b59a43da884eb131da8ea7f4-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:16 async_llm_engine.py:173] Added request cmpl-c06ddd01b59a43da884eb131da8ea7f4-0.
INFO 02-05 12:27:16 logger.py:36] Received request cmpl-1612b39da1f04fbda12c7a52eecf71c6-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:16 async_llm_engine.py:173] Added request cmpl-1612b39da1f04fbda12c7a52eecf71c6-0.
INFO 02-05 12:27:16 logger.py:36] Received request cmpl-7a49d92cb14545dc8f229d1ec8d5515e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:16 async_llm_engine.py:173] Added request cmpl-7a49d92cb14545dc8f229d1ec8d5515e-0.
INFO 02-05 12:27:16 logger.py:36] Received request cmpl-2f28edcbfa1f4786b96fa59d492e45c7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:16 async_llm_engine.py:173] Added request cmpl-2f28edcbfa1f4786b96fa59d492e45c7-0.
INFO 02-05 12:27:16 logger.py:36] Received request cmpl-426ecf150f714f05a8a131ac954964c5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:16 async_llm_engine.py:173] Added request cmpl-426ecf150f714f05a8a131ac954964c5-0.
INFO 02-05 12:27:16 logger.py:36] Received request cmpl-23e2217459864e519455cc1c49a33079-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:16 async_llm_engine.py:173] Added request cmpl-23e2217459864e519455cc1c49a33079-0.
INFO 02-05 12:27:16 logger.py:36] Received request cmpl-fcca9a5cc37d486ead0584c4998f8c75-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:16 logger.py:36] Received request cmpl-5a85cde6029c4e948c2e4f2f7ab9b696-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:16 async_llm_engine.py:173] Added request cmpl-fcca9a5cc37d486ead0584c4998f8c75-0.
INFO 02-05 12:27:16 async_llm_engine.py:173] Added request cmpl-5a85cde6029c4e948c2e4f2f7ab9b696-0.
INFO 02-05 12:27:16 logger.py:36] Received request cmpl-a7c4e756387448fc87bd487f11f0f302-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:16 async_llm_engine.py:173] Added request cmpl-a7c4e756387448fc87bd487f11f0f302-0.
INFO 02-05 12:27:16 logger.py:36] Received request cmpl-12b354a3f75345b6add954b8f250a327-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:16 async_llm_engine.py:173] Added request cmpl-12b354a3f75345b6add954b8f250a327-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:17 async_llm_engine.py:140] Finished request cmpl-c06ddd01b59a43da884eb131da8ea7f4-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:17 async_llm_engine.py:140] Finished request cmpl-1612b39da1f04fbda12c7a52eecf71c6-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:17 async_llm_engine.py:140] Finished request cmpl-7a49d92cb14545dc8f229d1ec8d5515e-0.
INFO 02-05 12:27:17 async_llm_engine.py:140] Finished request cmpl-2f28edcbfa1f4786b96fa59d492e45c7-0.
INFO 02-05 12:27:17 async_llm_engine.py:140] Finished request cmpl-426ecf150f714f05a8a131ac954964c5-0.
INFO 02-05 12:27:17 async_llm_engine.py:140] Finished request cmpl-23e2217459864e519455cc1c49a33079-0.
INFO 02-05 12:27:17 async_llm_engine.py:140] Finished request cmpl-fcca9a5cc37d486ead0584c4998f8c75-0.
INFO 02-05 12:27:17 async_llm_engine.py:140] Finished request cmpl-5a85cde6029c4e948c2e4f2f7ab9b696-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:17 async_llm_engine.py:140] Finished request cmpl-a7c4e756387448fc87bd487f11f0f302-0.
INFO 02-05 12:27:17 async_llm_engine.py:140] Finished request cmpl-12b354a3f75345b6add954b8f250a327-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:17 logger.py:36] Received request cmpl-a28a8ecc279d42b8ba92b982f8755014-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:17 async_llm_engine.py:173] Added request cmpl-a28a8ecc279d42b8ba92b982f8755014-0.
INFO 02-05 12:27:17 logger.py:36] Received request cmpl-5a6d176d5fb740d9bc4256329f1d2cae-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:17 async_llm_engine.py:173] Added request cmpl-5a6d176d5fb740d9bc4256329f1d2cae-0.
INFO 02-05 12:27:17 logger.py:36] Received request cmpl-543754698cb84d26b2387a3b005064be-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:17 async_llm_engine.py:173] Added request cmpl-543754698cb84d26b2387a3b005064be-0.
INFO 02-05 12:27:17 logger.py:36] Received request cmpl-7dac11566890472a90362d9b14adbf15-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:17 async_llm_engine.py:173] Added request cmpl-7dac11566890472a90362d9b14adbf15-0.
INFO 02-05 12:27:17 logger.py:36] Received request cmpl-3e5c126758a04e1387d48906b223de72-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:17 async_llm_engine.py:173] Added request cmpl-3e5c126758a04e1387d48906b223de72-0.
INFO 02-05 12:27:17 logger.py:36] Received request cmpl-96975024b3124ea78ec71d0449b7b5f1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:17 async_llm_engine.py:173] Added request cmpl-96975024b3124ea78ec71d0449b7b5f1-0.
INFO 02-05 12:27:17 logger.py:36] Received request cmpl-20a3a36be0df4a4a8df5fad00e0d7f7f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:17 async_llm_engine.py:173] Added request cmpl-20a3a36be0df4a4a8df5fad00e0d7f7f-0.
INFO 02-05 12:27:17 logger.py:36] Received request cmpl-905b55e3f5b74268b5250b3b764c5081-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:17 async_llm_engine.py:173] Added request cmpl-905b55e3f5b74268b5250b3b764c5081-0.
INFO 02-05 12:27:17 logger.py:36] Received request cmpl-c088cbc46cef47e2bc1c71d31b21cfa0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:17 async_llm_engine.py:173] Added request cmpl-c088cbc46cef47e2bc1c71d31b21cfa0-0.
INFO 02-05 12:27:17 logger.py:36] Received request cmpl-e2011fe8c08d49618508e77298c72591-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:17 async_llm_engine.py:173] Added request cmpl-e2011fe8c08d49618508e77298c72591-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:18 async_llm_engine.py:140] Finished request cmpl-a28a8ecc279d42b8ba92b982f8755014-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:18 async_llm_engine.py:140] Finished request cmpl-5a6d176d5fb740d9bc4256329f1d2cae-0.
INFO 02-05 12:27:18 async_llm_engine.py:140] Finished request cmpl-543754698cb84d26b2387a3b005064be-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:18 async_llm_engine.py:140] Finished request cmpl-7dac11566890472a90362d9b14adbf15-0.
INFO 02-05 12:27:18 async_llm_engine.py:140] Finished request cmpl-3e5c126758a04e1387d48906b223de72-0.
INFO 02-05 12:27:18 async_llm_engine.py:140] Finished request cmpl-96975024b3124ea78ec71d0449b7b5f1-0.
INFO 02-05 12:27:18 async_llm_engine.py:140] Finished request cmpl-20a3a36be0df4a4a8df5fad00e0d7f7f-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:18 async_llm_engine.py:140] Finished request cmpl-905b55e3f5b74268b5250b3b764c5081-0.
INFO 02-05 12:27:18 async_llm_engine.py:140] Finished request cmpl-c088cbc46cef47e2bc1c71d31b21cfa0-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:18 async_llm_engine.py:140] Finished request cmpl-e2011fe8c08d49618508e77298c72591-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:18 logger.py:36] Received request cmpl-b756b7925a61457b8b02035b1d51dede-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:18 async_llm_engine.py:173] Added request cmpl-b756b7925a61457b8b02035b1d51dede-0.
INFO 02-05 12:27:18 logger.py:36] Received request cmpl-982731d529b6420dba3b59a50e4a6759-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:18 async_llm_engine.py:173] Added request cmpl-982731d529b6420dba3b59a50e4a6759-0.
INFO 02-05 12:27:18 metrics.py:396] Avg prompt throughput: 55.4 tokens/s, Avg generation throughput: 496.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-05 12:27:18 logger.py:36] Received request cmpl-4a8dcc09c2fa4cae9264b6698229bb6e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:18 async_llm_engine.py:173] Added request cmpl-4a8dcc09c2fa4cae9264b6698229bb6e-0.
INFO 02-05 12:27:18 logger.py:36] Received request cmpl-c8d5b851b3104c2abef658d0f50e5fef-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:18 async_llm_engine.py:173] Added request cmpl-c8d5b851b3104c2abef658d0f50e5fef-0.
INFO 02-05 12:27:18 logger.py:36] Received request cmpl-6301d8b921314c11b41ef6d881cdd706-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:18 logger.py:36] Received request cmpl-866609a2577a4da7b0b582328842f669-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:18 logger.py:36] Received request cmpl-7d296eccd7b240598156d1e558779ff2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:18 async_llm_engine.py:173] Added request cmpl-6301d8b921314c11b41ef6d881cdd706-0.
INFO 02-05 12:27:18 async_llm_engine.py:173] Added request cmpl-866609a2577a4da7b0b582328842f669-0.
INFO 02-05 12:27:18 async_llm_engine.py:173] Added request cmpl-7d296eccd7b240598156d1e558779ff2-0.
INFO 02-05 12:27:18 logger.py:36] Received request cmpl-b5ceed2f09634ea5921bff875836baca-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:18 async_llm_engine.py:173] Added request cmpl-b5ceed2f09634ea5921bff875836baca-0.
INFO 02-05 12:27:18 logger.py:36] Received request cmpl-9d040583dd174176b04df7008bfd7466-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:18 async_llm_engine.py:173] Added request cmpl-9d040583dd174176b04df7008bfd7466-0.
INFO 02-05 12:27:18 logger.py:36] Received request cmpl-128eb48c641e4c16b1a6f0c750514231-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:18 async_llm_engine.py:173] Added request cmpl-128eb48c641e4c16b1a6f0c750514231-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:19 async_llm_engine.py:140] Finished request cmpl-b756b7925a61457b8b02035b1d51dede-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:20 async_llm_engine.py:140] Finished request cmpl-982731d529b6420dba3b59a50e4a6759-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:20 async_llm_engine.py:140] Finished request cmpl-4a8dcc09c2fa4cae9264b6698229bb6e-0.
INFO 02-05 12:27:20 async_llm_engine.py:140] Finished request cmpl-c8d5b851b3104c2abef658d0f50e5fef-0.
INFO 02-05 12:27:20 async_llm_engine.py:140] Finished request cmpl-6301d8b921314c11b41ef6d881cdd706-0.
INFO 02-05 12:27:20 async_llm_engine.py:140] Finished request cmpl-866609a2577a4da7b0b582328842f669-0.
INFO 02-05 12:27:20 async_llm_engine.py:140] Finished request cmpl-7d296eccd7b240598156d1e558779ff2-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:20 async_llm_engine.py:140] Finished request cmpl-b5ceed2f09634ea5921bff875836baca-0.
INFO 02-05 12:27:20 async_llm_engine.py:140] Finished request cmpl-9d040583dd174176b04df7008bfd7466-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:20 async_llm_engine.py:140] Finished request cmpl-128eb48c641e4c16b1a6f0c750514231-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:20 logger.py:36] Received request cmpl-db62238a55ef4a3993f79ffdf44ef27d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:20 async_llm_engine.py:173] Added request cmpl-db62238a55ef4a3993f79ffdf44ef27d-0.
INFO 02-05 12:27:20 logger.py:36] Received request cmpl-38ac178ae56e4c4da4f00a9afdd14e71-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:20 async_llm_engine.py:173] Added request cmpl-38ac178ae56e4c4da4f00a9afdd14e71-0.
INFO 02-05 12:27:20 logger.py:36] Received request cmpl-c094a3a2b44545e38810f84a842434bf-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:20 async_llm_engine.py:173] Added request cmpl-c094a3a2b44545e38810f84a842434bf-0.
INFO:     192.168.200.241:45896 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:20 logger.py:36] Received request cmpl-3c4ab5796e4f457b877c2cc9fdb01c03-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:20 async_llm_engine.py:173] Added request cmpl-3c4ab5796e4f457b877c2cc9fdb01c03-0.
INFO 02-05 12:27:20 logger.py:36] Received request cmpl-c67fd81387d84063b72878320b434087-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:20 logger.py:36] Received request cmpl-8b5fa538feff4c3ebc6070363ee608ea-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:20 async_llm_engine.py:173] Added request cmpl-c67fd81387d84063b72878320b434087-0.
INFO 02-05 12:27:20 async_llm_engine.py:173] Added request cmpl-8b5fa538feff4c3ebc6070363ee608ea-0.
INFO 02-05 12:27:20 logger.py:36] Received request cmpl-7bbb1ded65f94f6595ced10e0c379c0f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:20 async_llm_engine.py:173] Added request cmpl-7bbb1ded65f94f6595ced10e0c379c0f-0.
INFO 02-05 12:27:20 logger.py:36] Received request cmpl-d61dab0483be4d1c9af1a9af4f74b25d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:20 async_llm_engine.py:173] Added request cmpl-d61dab0483be4d1c9af1a9af4f74b25d-0.
INFO 02-05 12:27:20 logger.py:36] Received request cmpl-72cfdd5026a04897a5c266d9ccdd9b06-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:20 async_llm_engine.py:173] Added request cmpl-72cfdd5026a04897a5c266d9ccdd9b06-0.
INFO 02-05 12:27:20 logger.py:36] Received request cmpl-bf31b2a3d12b43979241a0b98a2b38fc-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:20 async_llm_engine.py:173] Added request cmpl-bf31b2a3d12b43979241a0b98a2b38fc-0.
INFO 02-05 12:27:21 async_llm_engine.py:140] Finished request cmpl-db62238a55ef4a3993f79ffdf44ef27d-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:21 async_llm_engine.py:140] Finished request cmpl-38ac178ae56e4c4da4f00a9afdd14e71-0.
INFO 02-05 12:27:21 async_llm_engine.py:140] Finished request cmpl-c094a3a2b44545e38810f84a842434bf-0.
INFO 02-05 12:27:21 async_llm_engine.py:140] Finished request cmpl-3c4ab5796e4f457b877c2cc9fdb01c03-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:21 async_llm_engine.py:140] Finished request cmpl-c67fd81387d84063b72878320b434087-0.
INFO 02-05 12:27:21 async_llm_engine.py:140] Finished request cmpl-8b5fa538feff4c3ebc6070363ee608ea-0.
INFO 02-05 12:27:21 async_llm_engine.py:140] Finished request cmpl-7bbb1ded65f94f6595ced10e0c379c0f-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:21 async_llm_engine.py:140] Finished request cmpl-d61dab0483be4d1c9af1a9af4f74b25d-0.
INFO 02-05 12:27:21 async_llm_engine.py:140] Finished request cmpl-72cfdd5026a04897a5c266d9ccdd9b06-0.
INFO 02-05 12:27:21 async_llm_engine.py:140] Finished request cmpl-bf31b2a3d12b43979241a0b98a2b38fc-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:21 logger.py:36] Received request cmpl-3a25f3a71d19429096aa7583990b3f4d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:21 async_llm_engine.py:173] Added request cmpl-3a25f3a71d19429096aa7583990b3f4d-0.
INFO 02-05 12:27:21 logger.py:36] Received request cmpl-a6f53f05b0d94676a142420bd9aa5abb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:21 async_llm_engine.py:173] Added request cmpl-a6f53f05b0d94676a142420bd9aa5abb-0.
INFO 02-05 12:27:21 logger.py:36] Received request cmpl-241f0ec2d7d44d14b23e795dc37d4b2f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:21 async_llm_engine.py:173] Added request cmpl-241f0ec2d7d44d14b23e795dc37d4b2f-0.
INFO 02-05 12:27:21 logger.py:36] Received request cmpl-f509e39a23f445c8a6bf4d4c67fb9190-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:21 async_llm_engine.py:173] Added request cmpl-f509e39a23f445c8a6bf4d4c67fb9190-0.
INFO 02-05 12:27:21 logger.py:36] Received request cmpl-3afdb23cbaa34a44ab452f645d676735-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:21 async_llm_engine.py:173] Added request cmpl-3afdb23cbaa34a44ab452f645d676735-0.
INFO 02-05 12:27:21 logger.py:36] Received request cmpl-161290065b364e8a8054f96f828ce753-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:21 async_llm_engine.py:173] Added request cmpl-161290065b364e8a8054f96f828ce753-0.
INFO 02-05 12:27:21 logger.py:36] Received request cmpl-8a57539de05b49508509d6c0c8d0a7b9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:21 async_llm_engine.py:173] Added request cmpl-8a57539de05b49508509d6c0c8d0a7b9-0.
INFO 02-05 12:27:21 logger.py:36] Received request cmpl-05e9d0db1fc04e829b181cc8da46688c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:21 logger.py:36] Received request cmpl-214bc9f994954f21a9ab11bc1b122b19-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:21 async_llm_engine.py:173] Added request cmpl-05e9d0db1fc04e829b181cc8da46688c-0.
INFO 02-05 12:27:21 async_llm_engine.py:173] Added request cmpl-214bc9f994954f21a9ab11bc1b122b19-0.
INFO 02-05 12:27:21 logger.py:36] Received request cmpl-c3be7f6221e54172a464d3f525178787-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:21 async_llm_engine.py:173] Added request cmpl-c3be7f6221e54172a464d3f525178787-0.
INFO 02-05 12:27:22 async_llm_engine.py:140] Finished request cmpl-3a25f3a71d19429096aa7583990b3f4d-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45938 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:22 async_llm_engine.py:140] Finished request cmpl-a6f53f05b0d94676a142420bd9aa5abb-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:22 async_llm_engine.py:140] Finished request cmpl-241f0ec2d7d44d14b23e795dc37d4b2f-0.
INFO 02-05 12:27:22 async_llm_engine.py:140] Finished request cmpl-f509e39a23f445c8a6bf4d4c67fb9190-0.
INFO 02-05 12:27:22 async_llm_engine.py:140] Finished request cmpl-3afdb23cbaa34a44ab452f645d676735-0.
INFO 02-05 12:27:22 async_llm_engine.py:140] Finished request cmpl-161290065b364e8a8054f96f828ce753-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:22 logger.py:36] Received request cmpl-185833d3dd064966bab2300f155447f7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:22 async_llm_engine.py:173] Added request cmpl-185833d3dd064966bab2300f155447f7-0.
INFO 02-05 12:27:22 async_llm_engine.py:140] Finished request cmpl-8a57539de05b49508509d6c0c8d0a7b9-0.
INFO 02-05 12:27:22 async_llm_engine.py:140] Finished request cmpl-05e9d0db1fc04e829b181cc8da46688c-0.
INFO 02-05 12:27:22 async_llm_engine.py:140] Finished request cmpl-214bc9f994954f21a9ab11bc1b122b19-0.
INFO 02-05 12:27:22 async_llm_engine.py:140] Finished request cmpl-c3be7f6221e54172a464d3f525178787-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:22 logger.py:36] Received request cmpl-9ec394e8900040459d354a3bb80fbe88-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:22 async_llm_engine.py:173] Added request cmpl-9ec394e8900040459d354a3bb80fbe88-0.
INFO 02-05 12:27:22 logger.py:36] Received request cmpl-b51c77c61c57495084a128d702a24eda-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:22 async_llm_engine.py:173] Added request cmpl-b51c77c61c57495084a128d702a24eda-0.
INFO 02-05 12:27:22 logger.py:36] Received request cmpl-d9c15d0041e6440c8901d9fe7e3cda9f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:22 async_llm_engine.py:173] Added request cmpl-d9c15d0041e6440c8901d9fe7e3cda9f-0.
INFO 02-05 12:27:22 logger.py:36] Received request cmpl-8f5b7a8c9b7f460c840430342516585c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:22 async_llm_engine.py:173] Added request cmpl-8f5b7a8c9b7f460c840430342516585c-0.
INFO 02-05 12:27:22 logger.py:36] Received request cmpl-c288e421b5574444b7b4623cbe71d55b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:22 async_llm_engine.py:173] Added request cmpl-c288e421b5574444b7b4623cbe71d55b-0.
INFO 02-05 12:27:22 logger.py:36] Received request cmpl-1e1e8f22992b4a048f499e026d90aa08-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:22 async_llm_engine.py:173] Added request cmpl-1e1e8f22992b4a048f499e026d90aa08-0.
INFO 02-05 12:27:22 logger.py:36] Received request cmpl-c0aeb13397554fae8973cf9b767bae51-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:22 async_llm_engine.py:173] Added request cmpl-c0aeb13397554fae8973cf9b767bae51-0.
INFO 02-05 12:27:22 logger.py:36] Received request cmpl-ab5dbb8775ee4e069d99c001868ed25c-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:22 async_llm_engine.py:173] Added request cmpl-ab5dbb8775ee4e069d99c001868ed25c-0.
INFO 02-05 12:27:22 logger.py:36] Received request cmpl-28223513eeea4c0bbd2e99f565fab70a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:22 async_llm_engine.py:173] Added request cmpl-28223513eeea4c0bbd2e99f565fab70a-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:23 async_llm_engine.py:140] Finished request cmpl-185833d3dd064966bab2300f155447f7-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:23 async_llm_engine.py:140] Finished request cmpl-9ec394e8900040459d354a3bb80fbe88-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:23 async_llm_engine.py:140] Finished request cmpl-b51c77c61c57495084a128d702a24eda-0.
INFO 02-05 12:27:23 async_llm_engine.py:140] Finished request cmpl-d9c15d0041e6440c8901d9fe7e3cda9f-0.
INFO 02-05 12:27:23 async_llm_engine.py:140] Finished request cmpl-8f5b7a8c9b7f460c840430342516585c-0.
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:23 logger.py:36] Received request cmpl-b61fb02696124bc38dff68adcf25b6b7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:23 async_llm_engine.py:173] Added request cmpl-b61fb02696124bc38dff68adcf25b6b7-0.
INFO 02-05 12:27:23 async_llm_engine.py:140] Finished request cmpl-c288e421b5574444b7b4623cbe71d55b-0.
INFO 02-05 12:27:23 async_llm_engine.py:140] Finished request cmpl-1e1e8f22992b4a048f499e026d90aa08-0.
INFO 02-05 12:27:23 async_llm_engine.py:140] Finished request cmpl-c0aeb13397554fae8973cf9b767bae51-0.
INFO 02-05 12:27:23 async_llm_engine.py:140] Finished request cmpl-ab5dbb8775ee4e069d99c001868ed25c-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:23 async_llm_engine.py:140] Finished request cmpl-28223513eeea4c0bbd2e99f565fab70a-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:23 logger.py:36] Received request cmpl-d09c53e85a9f48309264c21344f5cf4d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:23 async_llm_engine.py:173] Added request cmpl-d09c53e85a9f48309264c21344f5cf4d-0.
INFO 02-05 12:27:23 logger.py:36] Received request cmpl-dff16bf419c44e64b3999a3b6066d142-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:23 async_llm_engine.py:173] Added request cmpl-dff16bf419c44e64b3999a3b6066d142-0.
INFO 02-05 12:27:23 logger.py:36] Received request cmpl-fe8d5766aa074d5482d360f15bee35d7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:23 async_llm_engine.py:173] Added request cmpl-fe8d5766aa074d5482d360f15bee35d7-0.
INFO 02-05 12:27:23 logger.py:36] Received request cmpl-7f6343e4050b442abe6fdcead4fc385a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:23 async_llm_engine.py:173] Added request cmpl-7f6343e4050b442abe6fdcead4fc385a-0.
INFO 02-05 12:27:23 logger.py:36] Received request cmpl-be03e87160874a2585c195ef794f83b9-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:23 async_llm_engine.py:173] Added request cmpl-be03e87160874a2585c195ef794f83b9-0.
INFO 02-05 12:27:23 logger.py:36] Received request cmpl-e6f434c780974ecfaaea38856af1ba74-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:23 async_llm_engine.py:173] Added request cmpl-e6f434c780974ecfaaea38856af1ba74-0.
INFO 02-05 12:27:23 logger.py:36] Received request cmpl-6711b57af4fc4727bb1cc3c50808a93a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:23 async_llm_engine.py:173] Added request cmpl-6711b57af4fc4727bb1cc3c50808a93a-0.
INFO 02-05 12:27:23 logger.py:36] Received request cmpl-2618ca3d741947ca978ea3e32fa1f27e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:23 async_llm_engine.py:173] Added request cmpl-2618ca3d741947ca978ea3e32fa1f27e-0.
INFO 02-05 12:27:23 logger.py:36] Received request cmpl-b01e84a18e7e470a9e0d0b12d52671ba-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:23 async_llm_engine.py:173] Added request cmpl-b01e84a18e7e470a9e0d0b12d52671ba-0.
INFO 02-05 12:27:23 metrics.py:396] Avg prompt throughput: 88.0 tokens/s, Avg generation throughput: 517.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:24 async_llm_engine.py:140] Finished request cmpl-b61fb02696124bc38dff68adcf25b6b7-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:24 async_llm_engine.py:140] Finished request cmpl-d09c53e85a9f48309264c21344f5cf4d-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:24 async_llm_engine.py:140] Finished request cmpl-dff16bf419c44e64b3999a3b6066d142-0.
INFO 02-05 12:27:24 async_llm_engine.py:140] Finished request cmpl-fe8d5766aa074d5482d360f15bee35d7-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:24 async_llm_engine.py:140] Finished request cmpl-7f6343e4050b442abe6fdcead4fc385a-0.
INFO 02-05 12:27:24 logger.py:36] Received request cmpl-5e76755a13e8423bbf893eeaea55068d-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:24 async_llm_engine.py:173] Added request cmpl-5e76755a13e8423bbf893eeaea55068d-0.
INFO 02-05 12:27:24 async_llm_engine.py:140] Finished request cmpl-be03e87160874a2585c195ef794f83b9-0.
INFO 02-05 12:27:24 async_llm_engine.py:140] Finished request cmpl-e6f434c780974ecfaaea38856af1ba74-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:24 async_llm_engine.py:140] Finished request cmpl-6711b57af4fc4727bb1cc3c50808a93a-0.
INFO 02-05 12:27:24 async_llm_engine.py:140] Finished request cmpl-2618ca3d741947ca978ea3e32fa1f27e-0.
INFO 02-05 12:27:24 async_llm_engine.py:140] Finished request cmpl-b01e84a18e7e470a9e0d0b12d52671ba-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:24 logger.py:36] Received request cmpl-b0f513a6b77e46e79b8ea85ebfd34f74-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:24 async_llm_engine.py:173] Added request cmpl-b0f513a6b77e46e79b8ea85ebfd34f74-0.
INFO 02-05 12:27:25 logger.py:36] Received request cmpl-49c5c1d8d72548aab796a1aa561b717e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:25 async_llm_engine.py:173] Added request cmpl-49c5c1d8d72548aab796a1aa561b717e-0.
INFO 02-05 12:27:25 logger.py:36] Received request cmpl-65c22bfbc79149cdbb251db4892ee82e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:25 async_llm_engine.py:173] Added request cmpl-65c22bfbc79149cdbb251db4892ee82e-0.
INFO 02-05 12:27:25 logger.py:36] Received request cmpl-c306d21fc70c4bfabefe8a2f9e67face-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:25 async_llm_engine.py:173] Added request cmpl-c306d21fc70c4bfabefe8a2f9e67face-0.
INFO 02-05 12:27:25 logger.py:36] Received request cmpl-5505b27682e44b7ba3f9abf0d9ac92b5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:25 async_llm_engine.py:173] Added request cmpl-5505b27682e44b7ba3f9abf0d9ac92b5-0.
INFO 02-05 12:27:25 logger.py:36] Received request cmpl-47b19f7ca29347d5b75db55c5fff4b09-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:25 async_llm_engine.py:173] Added request cmpl-47b19f7ca29347d5b75db55c5fff4b09-0.
INFO 02-05 12:27:25 logger.py:36] Received request cmpl-c2ce38535e774779ba300cc624505e38-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:25 async_llm_engine.py:173] Added request cmpl-c2ce38535e774779ba300cc624505e38-0.
INFO 02-05 12:27:25 logger.py:36] Received request cmpl-8ae3cf58ab9d469c82c7423cb2c4fae1-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:25 async_llm_engine.py:173] Added request cmpl-8ae3cf58ab9d469c82c7423cb2c4fae1-0.
INFO 02-05 12:27:25 logger.py:36] Received request cmpl-46ac1734867f446d8556285fce8167e0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:25 async_llm_engine.py:173] Added request cmpl-46ac1734867f446d8556285fce8167e0-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:58996 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:59004 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:27:26 async_llm_engine.py:140] Finished request cmpl-5e76755a13e8423bbf893eeaea55068d-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:26 async_llm_engine.py:140] Finished request cmpl-b0f513a6b77e46e79b8ea85ebfd34f74-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:26 async_llm_engine.py:140] Finished request cmpl-49c5c1d8d72548aab796a1aa561b717e-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:26 logger.py:36] Received request cmpl-e4a635cb160b4c7787682e7fcdb04d40-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:26 async_llm_engine.py:173] Added request cmpl-e4a635cb160b4c7787682e7fcdb04d40-0.
INFO 02-05 12:27:26 async_llm_engine.py:140] Finished request cmpl-65c22bfbc79149cdbb251db4892ee82e-0.
INFO 02-05 12:27:26 async_llm_engine.py:140] Finished request cmpl-c306d21fc70c4bfabefe8a2f9e67face-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:26 async_llm_engine.py:140] Finished request cmpl-5505b27682e44b7ba3f9abf0d9ac92b5-0.
INFO 02-05 12:27:26 async_llm_engine.py:140] Finished request cmpl-47b19f7ca29347d5b75db55c5fff4b09-0.
INFO 02-05 12:27:26 async_llm_engine.py:140] Finished request cmpl-c2ce38535e774779ba300cc624505e38-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:26 async_llm_engine.py:140] Finished request cmpl-8ae3cf58ab9d469c82c7423cb2c4fae1-0.
INFO 02-05 12:27:26 async_llm_engine.py:140] Finished request cmpl-46ac1734867f446d8556285fce8167e0-0.
INFO 02-05 12:27:26 logger.py:36] Received request cmpl-e9468b071c314c8583f4fca2d00cd4a0-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:26 async_llm_engine.py:173] Added request cmpl-e9468b071c314c8583f4fca2d00cd4a0-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:26 logger.py:36] Received request cmpl-d25ed1cc2928401db17961c9b28e07fd-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:26 async_llm_engine.py:173] Added request cmpl-d25ed1cc2928401db17961c9b28e07fd-0.
INFO 02-05 12:27:26 logger.py:36] Received request cmpl-52e682b549b34ef5801b986eff9021a8-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:26 async_llm_engine.py:173] Added request cmpl-52e682b549b34ef5801b986eff9021a8-0.
INFO 02-05 12:27:26 logger.py:36] Received request cmpl-f97c978bc0384ffa8e983980698e91cb-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:26 async_llm_engine.py:173] Added request cmpl-f97c978bc0384ffa8e983980698e91cb-0.
INFO 02-05 12:27:26 logger.py:36] Received request cmpl-b0c74bf2274840f38cd61c9214ac229f-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:26 async_llm_engine.py:173] Added request cmpl-b0c74bf2274840f38cd61c9214ac229f-0.
INFO 02-05 12:27:26 logger.py:36] Received request cmpl-be03e607fb3a4e22a6fe6737783780a5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:26 async_llm_engine.py:173] Added request cmpl-be03e607fb3a4e22a6fe6737783780a5-0.
INFO 02-05 12:27:26 logger.py:36] Received request cmpl-54912c00215d4361b15da8c485916f5a-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:26 async_llm_engine.py:173] Added request cmpl-54912c00215d4361b15da8c485916f5a-0.
INFO 02-05 12:27:26 logger.py:36] Received request cmpl-ee9f91c760d14061b14f7ee9bfdbabd7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:26 async_llm_engine.py:173] Added request cmpl-ee9f91c760d14061b14f7ee9bfdbabd7-0.
INFO 02-05 12:27:26 logger.py:36] Received request cmpl-e41f06541bba4c699285e37551413768-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:26 async_llm_engine.py:173] Added request cmpl-e41f06541bba4c699285e37551413768-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:27 async_llm_engine.py:140] Finished request cmpl-e4a635cb160b4c7787682e7fcdb04d40-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:27 async_llm_engine.py:140] Finished request cmpl-e9468b071c314c8583f4fca2d00cd4a0-0.
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:27 async_llm_engine.py:140] Finished request cmpl-d25ed1cc2928401db17961c9b28e07fd-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:27 logger.py:36] Received request cmpl-2079e71c349f40b8a8b09facac36893b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:27 async_llm_engine.py:173] Added request cmpl-2079e71c349f40b8a8b09facac36893b-0.
INFO 02-05 12:27:27 async_llm_engine.py:140] Finished request cmpl-52e682b549b34ef5801b986eff9021a8-0.
INFO 02-05 12:27:27 async_llm_engine.py:140] Finished request cmpl-f97c978bc0384ffa8e983980698e91cb-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:27 async_llm_engine.py:140] Finished request cmpl-b0c74bf2274840f38cd61c9214ac229f-0.
INFO 02-05 12:27:27 async_llm_engine.py:140] Finished request cmpl-be03e607fb3a4e22a6fe6737783780a5-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:27 logger.py:36] Received request cmpl-13b4706a34024ebfb5ed4d99ffeaaa1e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:27 async_llm_engine.py:173] Added request cmpl-13b4706a34024ebfb5ed4d99ffeaaa1e-0.
INFO 02-05 12:27:27 async_llm_engine.py:140] Finished request cmpl-54912c00215d4361b15da8c485916f5a-0.
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:27 async_llm_engine.py:140] Finished request cmpl-ee9f91c760d14061b14f7ee9bfdbabd7-0.
INFO 02-05 12:27:27 async_llm_engine.py:140] Finished request cmpl-e41f06541bba4c699285e37551413768-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:27 logger.py:36] Received request cmpl-0dfa438d13ae45d498e78ed05ecd482b-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:27 async_llm_engine.py:173] Added request cmpl-0dfa438d13ae45d498e78ed05ecd482b-0.
INFO 02-05 12:27:27 logger.py:36] Received request cmpl-b8a3fabe48c44253b4ba6df070297851-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:27 async_llm_engine.py:173] Added request cmpl-b8a3fabe48c44253b4ba6df070297851-0.
INFO 02-05 12:27:27 logger.py:36] Received request cmpl-e5816973727148e69fa062aebbc1fcb2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:27 async_llm_engine.py:173] Added request cmpl-e5816973727148e69fa062aebbc1fcb2-0.
INFO 02-05 12:27:27 logger.py:36] Received request cmpl-d6ab30a4bd5347bcab73724523ef43f7-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:27 async_llm_engine.py:173] Added request cmpl-d6ab30a4bd5347bcab73724523ef43f7-0.
INFO 02-05 12:27:27 logger.py:36] Received request cmpl-7d5243fcca9b42f2b616ed3f3775c0a2-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:27 async_llm_engine.py:173] Added request cmpl-7d5243fcca9b42f2b616ed3f3775c0a2-0.
INFO 02-05 12:27:27 logger.py:36] Received request cmpl-2a04924bd1eb405a852cfb2b592f6fd5-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:27 async_llm_engine.py:173] Added request cmpl-2a04924bd1eb405a852cfb2b592f6fd5-0.
INFO 02-05 12:27:27 logger.py:36] Received request cmpl-2a17b753a0f74094bdf0ef939d6cbd0e-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:27 async_llm_engine.py:173] Added request cmpl-2a17b753a0f74094bdf0ef939d6cbd0e-0.
INFO 02-05 12:27:27 logger.py:36] Received request cmpl-c123ff541f474cb7ac513bbd7365a155-0: prompt: 'Lets explore some architecture patterns for microservices', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 94413, 13488, 1063, 18112, 12912, 369, 8162, 13069], lora_request: None, prompt_adapter_request: None.
INFO 02-05 12:27:27 async_llm_engine.py:173] Added request cmpl-c123ff541f474cb7ac513bbd7365a155-0.
INFO:     192.168.200.241:33980 - "GET /metrics HTTP/1.1" 200 OK
INFO 02-05 12:27:28 async_llm_engine.py:140] Finished request cmpl-2079e71c349f40b8a8b09facac36893b-0.
INFO:     192.168.200.241:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:28 async_llm_engine.py:140] Finished request cmpl-13b4706a34024ebfb5ed4d99ffeaaa1e-0.
INFO:     192.168.200.241:45896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:28 async_llm_engine.py:140] Finished request cmpl-0dfa438d13ae45d498e78ed05ecd482b-0.
INFO:     192.168.200.241:45920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:28 async_llm_engine.py:140] Finished request cmpl-b8a3fabe48c44253b4ba6df070297851-0.
INFO 02-05 12:27:28 async_llm_engine.py:140] Finished request cmpl-e5816973727148e69fa062aebbc1fcb2-0.
INFO 02-05 12:27:28 async_llm_engine.py:140] Finished request cmpl-d6ab30a4bd5347bcab73724523ef43f7-0.
INFO:     192.168.200.241:45932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:28 async_llm_engine.py:140] Finished request cmpl-7d5243fcca9b42f2b616ed3f3775c0a2-0.
INFO:     192.168.200.241:45944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:28 async_llm_engine.py:140] Finished request cmpl-2a04924bd1eb405a852cfb2b592f6fd5-0.
INFO 02-05 12:27:28 async_llm_engine.py:140] Finished request cmpl-2a17b753a0f74094bdf0ef939d6cbd0e-0.
INFO:     192.168.200.241:45938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 02-05 12:27:28 async_llm_engine.py:140] Finished request cmpl-c123ff541f474cb7ac513bbd7365a155-0.
INFO:     192.168.200.241:45936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     192.168.200.241:45936 - "GET /metrics HTTP/1.1" 200 OK
INFO:     89.105.200.105:39136 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:39148 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:27:38 metrics.py:396] Avg prompt throughput: 18.4 tokens/s, Avg generation throughput: 172.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:58740 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:58754 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:27:48 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:40836 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:40840 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:27:58 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:40100 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:40106 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:28:08 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:44558 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:44566 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:28:18 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:36742 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:36744 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:28:28 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:41790 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:41804 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:28:38 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:53544 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:53550 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:28:48 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:48358 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:48360 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:28:58 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:55818 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:55820 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:29:08 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:60430 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:60438 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:29:18 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:55132 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:55134 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:29:28 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:38380 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:38376 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:29:38 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:53156 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:53172 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:29:48 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:51464 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:51478 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:29:58 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:38842 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:38856 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:30:08 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:55856 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:55860 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:30:18 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:40002 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:39990 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:30:28 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:60506 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:60510 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:30:38 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:47900 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:47908 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:30:48 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:53130 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:53142 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:30:58 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:57622 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:57628 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:31:08 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:35266 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:35282 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:31:18 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:57444 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:57460 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:31:28 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:57652 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:57666 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:31:38 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:42888 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:42890 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:31:48 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:57446 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:57456 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:31:58 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:51166 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:51172 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:32:08 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:52498 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:52506 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:32:18 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     89.105.200.105:58192 - "GET /health HTTP/1.1" 200 OK
INFO:     89.105.200.105:58200 - "GET /health HTTP/1.1" 200 OK
INFO 02-05 12:32:28 metrics.py:396] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [1]
INFO 02-05 12:32:35 async_llm_engine.py:53] Engine is gracefully shutting down.
